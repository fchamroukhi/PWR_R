#Bon ou mauvais
resQuality=rep(1,nrow(table))
resQuality
resQuality[table$quality<=5]=0
resQuality
T=table[,1:11]
T
head(T)
table2=cbind(T,resQuality)
head(table2)
reg_log = glm(resQuality ~ fixed.acidity + volatile.acidity + citric.acid + residual.sugar+chlorides+free.sulfur.dioxide+total.sulfur.dioxide+density+pH+sulphates+alcohol,data=table2,family="binomial")
summary(reg_log)
install.packages("C:/Program Files/R/concordance_1.6.tar.gz", repos = NULL, type = "source")
concordance(T$resQuality, predict(reg_log, type = "response"))
library(concordance)
data(concord_data, package="concordance")
concordance(dat$vs, predict(glm.reg, type = "response"))
concordance(T$resQuality, predict(reg_log, type = "response"))
predict(reg_log, type = "response")
p=predict(reg_log, type = "response")
plot(p)
concordance(T, predict(reg_log, type = "response"))
T
diff=resQuality-p
diff
sum(abs(diff))
View(`T`)
View(`T`)
sum(p)
3258/4898
#TP noté analyse de données
library(readr)
table = read.csv("C:/Données/Aline/Master SAAD/R/Projet Analyse de données/winequality-white.csv",sep=",",header = T)
attach(table)
head(table)
T=table[,1:10]
T
#enlever 2 outliers :
T2=T[-2782,]
T3=T2[-4746,]
table2=table[-2782,]
table3=table2[-4746,]
quality
T3
head(T3)
quality
T3$quality
drop(table)
drop(table2)
#Bon ou mauvais
resQuality=rep("bon",nrow(table3))
resQuality
resQuality[table3$quality<=5]="mauvais"
resQuality
as.data.frame(resQuality)
head(resQuality)
resQuality2=rep(2,nrow(table3))
#resQuality2[="mauvais"]=1
resQuality2
#boites a moustache
par(mfrow = c(4,3))
p1=plot(table$fixed.acidity ~as.factor (table$quality ))
p2=plot(table$volatile.acidity ~as.factor (table$quality ))
p3=plot(table$citric.acid ~as.factor (table$quality ))
p4=plot(table$residual.sugar ~as.factor (table$quality ))
p5=plot(table$chlorides ~as.factor (table$quality ))
p6=plot(table$free.sulfur.dioxide ~as.factor (table$quality ))
p7=plot(table$total.sulfur.dioxide ~as.factor (table$quality ))
p8=plot(table$density ~as.factor (table$quality ))
p9=plot(table$pH ~as.factor (table$quality ))
p10=plot(table$sulphates ~as.factor (table$quality ))
p11=plot(table$alcohol ~as.factor (table$quality ))
#regression multi lineaire
regMult=lm(quality ~ fixed.acidity + volatile.acidity + citric.acid + residual.sugar+chlorides+free.sulfur.dioxide+total.sulfur.dioxide+density+pH+sulphates+alcohol)
regMult
summary(regMult)
plot(regMult)
plot(table)
summary(regMult)
summary(regMult)
reg_log = glm(resQuality ~ fixed.acidity + volatile.acidity + citric.acid + residual.sugar+chlorides+free.sulfur.dioxide+total.sulfur.dioxide+density+pH+sulphates+alcohol,data=table2,family="binomial(logit)")
exp(coef(reg_log))
#graph des effets
library(effects)
install.packages("C:/Program Files/R/effects_4.0-0.tar.gz", repos = NULL, type = "source")
#graph des effets
library(effects)
#graph des effets
library('effects')
#graph des effets
library(effect)
library(effects)
install.packages("C:/Program Files/R/effects_4.0-0.tar.gz", repos = NULL, type = "source")
install.pakages("effects")
library(effects)
install.packages("C:/Program Files/R/effects_4.0-0.zip", repos = NULL, type = "win.binary")
install.pakages("effects")
library(effects)
install.packages("C:/Program Files/R/carData_3.0-0.zip", repos = NULL, type = "win.binary")
install.pakages("effects")
library(effects)
p=predict(reg_log, type = "response",newdata = d)
head(p)
table(p > 0.5, d$sport)
p=predict(reg_log, type = "response",newdata = d)
p=predict(reg_log, type = "response",newdata = d)
head(p)
d
View(`T`)
View(`T`)
p=predict(reg_log, type = "response")
head(p)
d
table(p > 0.5, resQuality)
p
lenght(p)
length(p)
length(resQuality)
library(readr)
table = read.csv("C:/Données/Aline/Master SAAD/R/Projet Analyse de données/winequality-white.csv",sep=",",header = T)
attach(table)
head(table)
#Bon ou mauvais
resQuality=rep(1,nrow(table))
resQuality
resQuality[table$quality<=5]=0
resQuality
length(resQuality)
p=predict(reg_log, type = "response")
length(p)
length(resQuality)
head(p)
d
table(p > 0.5, resQuality)
summary(reg_log)
p1 = 0.2
;
p2 = 0.3
;
p3 = 0.5
n = 50
s1 = c(1, 2)
;
s2 = c(3, 1)
;
s3 = c(1.5, 2)
s = rbind(s1, s2, s3)
m1 = c(1, 2)
;
m2 = c(6, 6)
;
m3 = c(6, -2)
m = rbind(m1, m2, m3)
c = sample(c(1, 2, 3), size = n, prob = c(p1, p2, p3), replace = TRUE)
x = cbind(rnorm(n, m[c, 1], s[c, 1]), rnorm(n, m[c, 2], s[c, 2]))
couleur = rep("red", n)
couleur[c == 2] = "blue"
couleur[c == 3] = "green"
x11()
plot(x,col = couleur, pch = 3 + c, lwd = 3, , xlab = " ", ylab = " ")
library(MASS)
T = as.factor(couleur)
x.lda = lda(x, T)
len = 50
xp = seq(min(x[ ,1]), max(x[ ,1]), length = len)
yp = seq(min(x[,2]), max(x[,2]), length = len)
grille = expand.grid(z1 = xp, z2 = yp)
Z = predict(x.lda, grille)
T.lda = predict(x.lda)$class
zp = Z$post[ ,3] - pmax(Z$post[ ,2], Z$post[ ,1])
contour(xp, yp, matrix(zp, len), add = TRUE, levels = 0, drawlabels = FALSE, col = "blue")
zp = Z$post[ ,1] - pmax(Z$post[ ,2], Z$post[ ,3])
contour(xp, yp, matrix(zp, len), add = TRUE, levels = 0, drawlabels = FALSE, col = "blue")
w = read.table("https://chesneau.users.lmno.cnrs.fr/anesthésie.txt", header = T)
attach(w)
library(stats)
reg = glm(Y ~ X1, family = binomial)
summary(reg)
predict.glm(reg, data.frame(X1 = 1.25), type = "response")
confint.default(reg, level = 0.95)
w = read.table("https://chesneau.users.lmno.cnrs.fr/anesthésie.txt", header = T)
attach(w)
library(stats)
reg = glm(Y ~ X1, family = binomial)
summary(reg)
predict.glm(reg, data.frame(X1 = 1.25), type = "response")
confint.default(reg, level = 0.95)
plot(X1, Y)
curve(predict(reg, data.frame(X1 = x), type = "response"), add = T)
pred.prob = predict(reg, type = "response")
pred.mod = factor(ifelse(pred.prob >0.5, "1", "0"))
mc = table(Y, pred.mod)
t = (mc[1, 2] + mc[2, 1]) / sum(mc)
t
mc = table(Y, pred.mod)
mc
w=read.table("https://chesneau.users.lmno.cnrs.fr/Etude1.txt",header=T)
w
attach(w)
library(MASS)
Y
X1
X2
reg = lm(Y~X1+X2)
graph = logtrans(reg, alpha = seq(0.1, 10, length = 20))
hat_alpha = graph$x[which.max(graph$y)]
reg2 = lm(log(Y + hat_alpha)~X1 + X2 )
summary(reg2)
V=c(1,3,7,4,5,2)
V2=seq(from=0, to=1000, by=5)
V3=V2[5]
V4=V2[c(5,9)]
V4
V4=V2[c(5,9)]
V5=V2[5:9]
V6=V2[seq(from=2,to=200,by=2)]
V2[1]=6
V6
V2[1]=6
V2[1:5]=rep(4,5)
V2>6
all(V2>6)
any(V2>6)
is.vector(V2)
V2.names
V2.names=LETTERS[1:6]
V7=V[-c(5,6)]
V7
V8=c(V7,V)
V9=rep(V7,2500)
length(V9)
V+1
V8[1:8]-V7
V8[1:8]*V7
Vd=diff(V)    #diffÃ©rence entre les termes adjacents du vecteur
Vd
Gabarit<-c("grand", "petit", "moyen", "grand", "moyen", "grand")
names(Gabarit)<-c("Luc", "Paul", "Max", "Bob", "Jean" ,"MaÃ©")
Gabarit[Gabarit!="petit"]
Noms=Gabarit[Gabarit=="grand"]
Noms
Noms=names(Gabarit[which(Gabarit=="grand")])
Maliste=list("Petitv"=c(1,2),"Sous-liste"=list("Nantua"="brochet","Moyenv"=c(3,4,5),"Grandv"=c(6,7,8,9)))
Maliste
Maliste[[2]][[2]] #affichage du 2Ã¨me terme inclus dans le 2Ã¨me Ã©lÃ©ment -> 3,4,5
Maliste[[2]][[2]][[1]]
Maliste[[2]][[3]] [[4]]
nchar(Maliste[[2]][[1]]) #nombre de caractÃ¨res du premier mot du 2Ã¨me Ã©lÃ©ment ->7
substr(Maliste[[2]][[1]],2,6) #affichage du premier mot du 2Ã¨me Ã©lÃ©ment de la 2Ã¨me Ã  la 6Ã¨me lettres->roche
VM<-c(180,167,185,173,75,51.5,90,70,32,22,40,35)
M2<-matrix(VM,nrow=4) #creation matrice de 4 lignes lue par colonnes
M=M2
M5=M[-4,]
M5
M3=M[,-3]
M3
M2%*%M5
Moy1<-colMeans(M) #moyenne des colonnes
Moy2<-apply(M,2,mean) #2=colonnes / 1=lignes
Moy3<-c(1/4,1/4,1/4,1/4)%*%M
M
colnames(M)=c("taille","poids","age")
rownames(M)=c("leo","lea","leon","lou")
M
Louis=c(20,80,190)
M=rbind(M,Louis)
IMC=M[,2]/M[,1]^2
IMC=IMC*100
IMC
M=cbind(M,IMC)
M
rm(IMC) #suppression du vecteur IMC tout en le laissant dans la matrice
Verdict=NULL
as.vector(Verdict)
for (i in 1:5) {
if (M[i,4]>25)  {Verdict[i]="surpoids"}
else if (M[i,4]<19)  {Verdict[i]="sous-poids"}
else {Verdict[i]="normal"}
}
Verdict
M=cbind(M[,1], M[,2],Verdict,M[,4])
M[,-4]
colnames(M)=c("taille","poids","Verdict","age")
M
Prob=rownames(M)[which(M[,3]!="normal")] #creation vecteurs avec personnes ayant un problÃ¨me de poids
Prob
knitr::opts_chunk$set(echo = TRUE)
example_kelvin <- 282.15
example_kelvin <- 282.15
example_kelvin <- 282.15
example_kelvin <- 282.15
plot(cars)
plot(cars)
plot(cars)
plot(cars)
plot(cars)
plot(pressure)
``rSys.Date()``
`rSys.Date()`
`Sys.Date()`
Sys.Date()
#Pour obtenir la date :
`r Sys.Date()`
`str(mtcars)`
str(mtcars)
my_code.R
my_code.R
"four" + "five"
library(dplyr)
plot(pressure)
plot(pressure)
The factorial of four is `r factorial(4)`.
The factorial of four is `r factorial(4)`.
"four" + "five"
install.packages("nasaweather")
knitr::opts_chunk$set(echo = TRUE)
```
install.packages("ggvis")
shiny::runApp('essai2')
variable="a"
sprintf(("select * from A where date=%s",variable))
variable=a
sprintf(("select * from A where date=%s",variable))
sprintf("select * from A where date=%s",variable)
variable="a"
sprintf("select * from A where date=%s",variable)
cat("SELECTION : date de debut :", datedebut, " ; date de fin :", datefin )
#entrer les dates de debut et fin de periode de sélection
datedebut='2018-10-12'
datefin= '2018-10-16'
#entrez le nom procede
procede='E00420QA'
cat("SELECTION : date de debut :", datedebut, " ; date de fin :", datefin )
cat(" Procédé : ", procede)
# entrer les dates de debut et fin de periode de sélection
datedebut='2018-10-12'
datefin= '2018-10-16'
# entrez le nom procede
procede='E00420QA'
library(dplyr)
library(odbc)
install.packages("odbc")
library(dplyr)
library(odbc)
library(tidyr)
variable1="a"
sprintf("select * from A where date=%s",variable1)
variable2="b"
sprintf("select * from A where date=%s et %s",variable1, variable2)
# entrer les dates de debut et fin de periode de sélection
datedebut='2018-10-12'
datefin= '2018-10-16'
# entrez le nom procede
procede='E00420QA'
cat("Date de debut :", datedebut, " ; Date de fin :", datefin )
cat(" Procédé : ", procede)
library(dplyr)
library(odbc)
library(tidyr)
con <- DBI::dbConnect(odbc::odbc(), Driver = "Cloudera ODBC Driver for Impala",
Host = "172.16.26.33", Port = 21050, Schema = "temp_eric_dwh",
AuthMech = 3, UseSASL = 1, UID = "reader", PWD = "reader")
sprintf( "SELECT * FROM processed_pcm.fichierspcm
WHERE (procede= %s
AND createdat >= %s
AND createdat < %s)
ORDER BY type,batid"),procede, datedebut, datefin)
sprintf( "SELECT * FROM processed_pcm.fichierspcm
WHERE (procede= %s
AND createdat >= %s
AND createdat < %s)
ORDER BY type,batid") , procede , datedebut , datefin)
sprintf( "SELECT * FROM processed_pcm.fichierspcm
WHERE (procede= %s
AND createdat >= %s
AND createdat < %s)
ORDER BY type,batid")  procede , datedebut , datefin)
procede='E00420QA'
datedebut='2018-10-12'
datefin= '2018-10-16'
sprintf( "SELECT * FROM processed_pcm.fichierspcm
WHERE (procede= %s
AND createdat >= %s
AND createdat < %s)
ORDER BY type,batid"),  procede , datedebut , datefin)
sprintf( "SELECT * FROM processed_pcm.fichierspcm
WHERE (procede= %s AND createdat >= %s AND createdat < %s)  ORDER BY type,batid",  procede , datedebut , datefin)
sprintf( "SELECT * FROM processed_pcm.fichierspcm
WHERE (procede= %s
AND createdat >= %s
AND createdat < %s)
ORDER BY type,batid",  procede , datedebut , datefin)
z=sprintf( "SELECT * FROM processed_pcm.fichierspcm
WHERE (procede= %s
AND createdat >= %s
AND createdat < %s)
ORDER BY type,batid",  procede , datedebut , datefin)
z
View(z)
con <- DBI::dbConnect(odbc::odbc(), Driver = "Cloudera ODBC Driver for Impala",
Host = "172.16.26.33", Port = 21050, Schema = "temp_eric_dwh",
AuthMech = 3, UseSASL = 1, UID = "reader", PWD = "reader")
sprintf( "SELECT * FROM processed_pcm.fichierspcm
WHERE (procede= %s
AND createdat >= %s
AND createdat < %s)
ORDER BY type,batid",procede, datedebut, datefin)
summarize(hflights,
n_obs = n(),
n_carrier = n_distinct(UniqueCarrier),
n_dest = n_distinct(Dest))
library(hflights)
summarize(hflights,
n_obs = n(),
n_carrier = n_distinct(UniqueCarrier),
n_dest = n_distinct(Dest))
summarise(group_by(filter(selectiontbl, value <min | value > max),parameter),QuantitéOutliers=length(value))
sprintf( "SELECT * FROM processed_pcm.fichierspcm
WHERE (procede= %s
AND createdat >= %s
AND createdat < %s)
ORDER BY type,batid",procede, datedebut, datefin)
load real_time_series_1
#
##################################################################
# A polynomial piecewise regression model for the optimal segmentation of a
# time series with regime changes. It uses dynamic programming for the segmentation
# and the LSE for the estimation of the regression parameters.
#
# by Faicel Chamroukhi Decembre 2008.
#
## Please cite the following papers for this code:
#
# @article{chamroukhi_et_al_NN2009,
# 	Address = {Oxford, UK, UK},
# 	Author = {Chamroukhi, F. and Sam\'{e}, A. and Govaert, G. and Aknin, P.},
# 	Date-Added = {2014-10-22 20:08:41 +0000},
# 	Date-Modified = {2014-10-22 20:08:41 +0000},
# 	Journal = {Neural Networks},
# 	Number = {5-6},
# 	Pages = {593--602},
# 	Publisher = {Elsevier Science Ltd.},
# 	Title = {Time series modeling by a regression approach based on a latent process},
# 	Volume = {22},
# 	Year = {2009},
# 	url  = {https://chamroukhi.users.lmno.cnrs.fr/papers/Chamroukhi_Neural_Networks_2009.pdf}
# 	}
#
# @INPROCEEDINGS{Chamroukhi-IJCNN-2009,
#   AUTHOR =       {Chamroukhi, F. and Sam\'e,  A. and Govaert, G. and Aknin, P.},
#   TITLE =        {A regression model with a hidden logistic process for feature extraction from time series},
#   BOOKTITLE =    {International Joint Conference on Neural Networks (IJCNN)},
#   YEAR =         {2009},
#   month = {June},
#   pages = {489--496},
#   Address = {Atlanta, GA},
#  url = {https://chamroukhi.users.lmno.cnrs.fr/papers/chamroukhi_ijcnn2009.pdf}
# }
#
# @article{Chamroukhi-FDA-2018,
# 	Journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
# 	Author = {Faicel Chamroukhi and Hien D. Nguyen},
# 	Note = {DOI: 10.1002/widm.1298.},
# 	Volume = {},
# 	Title = {Model-Based Clustering and Classification of Functional Data},
# 	Year = {2019},
# 	Month = {to appear},
# 	url =  {https://chamroukhi.com/papers/MBCC-FDA.pdf}
# 	}
#
#
#
# Faicel Chamroukhi Decembre 2008.
##################################################################
library(matlib)
library(matrixcalc)
rm(list=ls()) # remove and rm can be used to remove objects
## toy time series with regime changes
# y =[randn(100,1); 7+randn(120,1);4+randn(200,1); -2+randn(100,1); 3.5+randn(150,1);]';
# n = length(y);
# x = linspace(0,1,n);
setwd("D:/Projet Master2 series temp/Project_04022019/Code R")
source("fit_PWR_fisher.R")
source("designmatrix.R")
source("cost_matrix_PPWR.R")
source("dynamic_prog.R")
source("show_PWR_results_essai.R")
library(R.matlab)
simulated_time_series = readMat("simulated_time_series.mat")
x = simulated_time_series$x
y = simulated_time_series$y
# model specification
K = 5;# number of segments
p = 3; # polynomial degree
pwr = fit_PWR_fisher(x, y, K, p)
cat(sprintf("elapsed time = %1.1f", pwr$stats$cputime), "s")
#################################################################################
#################################################################################
# Progression
#################################################################################
#################################################################################
show_PWR_results_essai(x, y, pwr)
## some real time series with regime changes
#load real_time_series_1
#load real_time_series_2
#
# pwr = fit_PWR_fisher(x, y, K, p);
#
# fprintf('elapsed time = #g\n', pwr.stats.cputime);
# #fprintf('objective value = #f\n',pwr.stats.objective);
# yaxislim = [240, 600];
# show_PWR_results(x, y, pwr, yaxislim);
