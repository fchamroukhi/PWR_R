
@article{Hubert1985,
  title={Comparing partitions},
  author={Hubert, L. and Arabie, P.},
  journal={Journal of classification},
  year={1985},
  volume={2},
  number = {1},
  pages={193-218}
}

@INPROCEEDINGS{Wang2009, 
author={K. Wang and S. Ng and G. J. McLachlan}, 
booktitle={2009 Digital Image Computing: Techniques and Applications}, 
title={Multivariate Skew t Mixture Models: Applications to Fluorescence-Activated Cell Sorting Data}, 
year={2009}, 
volume={}, 
number={}, 
pages={526-531}, 
keywords={expectation-maximisation algorithm;pattern recognition;sorting;multivariate skew t mixture models;fluorescence-activated cell sorting data;pattern recognition;highly asymmetric observations;data skewness;pseudo-components;fitted mixture components;EM algorithm;maximum likelihood estimates;Fluorescence;Sorting;Pattern recognition;Maximum likelihood estimation;Context modeling;Mathematics;Parameter estimation;Robustness;Covariance matrix;Digital images;Asymmetric multivariate data;EM algorithm;fluorescence-activated cell sorting;mixture models;skewed t}, 
doi={10.1109/DICTA.2009.88}, 
ISSN={}, 
month={Dec},}

@article{Scrucca2016mclust5C,
  title={{mclust 5: Clustering, Classification and Density Estimation Using Gaussian Finite Mixture Models.}},
  author={Luca Scrucca and Michael Fop and Thomas Brendan Murphy and Adrian E. Raftery},
  journal={The R journal},
  year={2016},
  volume={8 1},
  pages={289-317}
}


@book{Cormen2009,
	Author = {Cormen, T. H. and Leiserson, C. E. and Rivest, R. L. and Stein, C.},
	Publisher = {MIT press.},
	Title = {Introduction to algorithms},
	Year = {2009},
	edition = {Third}
	}
	
@article{HunterLangeMM04,
  title={{A tutorial on MM algorithms}},
  author={Hunter, D. R. and Lange, K.},
  journal={The American Statistician},
  volume={58},
  number={1},
  pages={30--37},
  year={2004},
  publisher={Taylor \& Francis}
}

@article{baudry2015,
author = "Baudry, Jean-Patrick",
journal = "Electronic Journal of Statistics",
number = "1",
pages = "1041--1077",
publisher = "The Institute of Mathematical Statistics and the Bernoulli Society",
title = "Estimation and model selection for model-based clustering with the conditional classification likelihood",
volume = "9",
year = "2015"
}

@ARTICLE{Jiang_and_tanner_NN_99,
    author = {Wenxin Jiang and Martin A. Tanner},
    title = {On the Identifiability of Mixtures-of-Experts},
    journal = {Neural Networks},
    year = {1999},
    volume = {12},
    pages = {197--220}
}

@ARTICLE{Ingrassia2012,
title = {Local Statistical Modeling via a Cluster-Weighted Approach with Elliptical Distributions},
author = {Ingrassia, Salvatore and Minotti, Simona and Vittadini, Giorgio},
year = {2012},
journal = {Journal of Classification},
volume = {29},
number = {3},
pages = {363-401},
abstract = { Cluster-weighted modeling (CWM) is a mixture approach to modeling the joint probability of data coming from a heterogeneous population. Under Gaussian assumptions, we investigate statistical properties of CWM from both theoretical and numerical point of view; in particular, we show that Gaussian CWM includes mixtures of distributions and mixtures of regressions as special cases. Further, we introduce CWM based on Student-t distributions, which provides a more robust fit for groups of observations with longer than normal tails or noise data. Theoretical results are illustrated using some empirical studies, considering both simulated and real data. Some generalizations of such models are also outlined. Copyright Springer Science+Business Media, LLC 2012},
keywords = {Cluster-weighted modeling; Mixture models; Model-based clustering}
}
 

@INPROCEEDINGS{Chamroukhi-IJCNN-2009,
  AUTHOR =       {Chamroukhi, F. and Sam\'e,  A. and Govaert, G. and Aknin, P.},
  TITLE =        {A regression model with a hidden logistic process for feature extraction from time series},
  BOOKTITLE =    {International Joint Conference on Neural Networks (IJCNN)},
  YEAR =         {2009},
  month = {June},
  pages = {489--496},
  Address = {Atlanta, GA}
}


@INPROCEEDINGS{Chamroukhi-ESANN-2012-2,
    Author         = {D. Trabelsi  and S. Mohammed and F. Chamroukhi and L. Oukhellou and Y. Amirat},
    BOOKTITLE       = {Proceedings of the XXth European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN)},
    Pages          = {417--422},
    Address = {Bruges, Belgium}, 
    Title   = {Supervised and unsupervised classification approaches for human activity recognition using
body-mounted sensors},
    Month = {April},
    Year           = {2012}
}


@InProceedings{Chamroukhi-IJCNN-2011,
  author = {F. Chamroukhi and A. Sam\'e  and P. Aknin and G. Govaert},
  title = {Model-based clustering with Hidden Markov Model regression for time series with regime changes},
  Booktitle = {Proceedings of the International Joint Conference on Neural Networks (IJCNN), IEEE},
  Pages = {2814--2821},
  Adress = {San Jose, California, USA},
  year = {2011},
  month = {Jul-Aug}
} 


@article{Chamroukhi-BSSRM-2015,
	Author = {F. Chamroukhi},
	Journal = {arXiv:1508.00635},
	Title = {Bayesian mixtures of spatial spline regressions},
	Volume = {},
	url= {http://arxiv.org/pdf/1508.00635.pdf},
	Year = {2015}}
	%month = {Aug},
	


@article{Chamroukhi-RobustEMMixReg2016,
	Author = {F. Chamroukhi},
	Journal = {Journal of Statistical Computation and Simulation},
	Publisher = {Taylor \& Francis},
	Title = {Unsupervised learning of regression mixture models with unknown number of components},
	Volume = {86},
	Issue = {12},
	pages = {2308-2334},
	note= {Published online: 05 Nov 2015.},
	Year = {2016}}
%	url = {http://chamroukhi.univ-tln.fr/papers/Chamroukhi-JSCS-2015.pdf},

@article{Chamroukhi-PWRM-2016,
author    = {Faicel Chamroukhi},
  title     = {Piecewise Regression Mixture for Simultaneous Functional Data Clustering
               and Optimal Segmentation},
  journal   = {Journal of Classification},
  volume    = {33},
  number    = {3},
  pages     = {374--411},
  year      = {2016}
	}
 
 
@article{Chamroukhi-MRHLP-2013,
	Author = {F. Chamroukhi and D. Trabelsi and S. Mohammed and L. Oukhellou and Y. Amirat},
	Journal = {Neurocomputing},
	Month = {November},
	Pages = {633--644},
	Publisher = {Elsevier},
	Title = {Joint segmentation of multivariate time series with hidden process regression for human activity recognition},
	Volume = {120},
	Year = {2013},
	note = {},
	url  = {http://chamroukhi.univ-tln.fr/papers/chamroukhi_et_al_neucomp2013b.pdf}
	}
	
	
@article{Chamroukhi-FMDA-neucomp2013,
	Author = {Chamroukhi, F. and Glotin, H. and Sam{\'e}, A.},
	Journal = {Neurocomputing},
	Pages = {153-163},
	Title = {Model-based functional mixture discriminant analysis with hidden process regression for curve classification},
	Volume = {112},
	Year = {2013}
		}
	

@article{Chamroukhi-MHMMR-2013,
	Author = {Trabelsi, D. and Mohammed, S. and Chamroukhi, F. and Oukhellou, L. and Amirat, Y.},
	Date-Added = {2014-10-22 20:08:41 +0000},
	Date-Modified = {2014-10-22 20:08:41 +0000},
	Journal = {IEEE Transactions on Automation Science and Engineering},
	Number = {10},
	Pages = {829--335},
	Title = {An unsupervised approach for automatic activity recognition based on Hidden Markov Model Regression},
	Volume = {3},
	Year = {2013}
	}
 

@article{Chamroukhi-MixRHLP-2011,
	Author = {Sam{\'e}, A. and Chamroukhi, F. and Govaert, G{\'e}rard and Aknin, P.},
	Issue = 4,
	Journal = {Advances in Data Analysis and Classification},
	Pages = {301--321},
	Publisher = {Springer Berlin / Heidelberg},
	Title = {Model-based clustering and segmentation of time series with changes in regime},
	Volume = 5,
	Year = {2011}
	}

@article{chamroukhi_et_al_neurocomp2010,
	Author = {Chamroukhi, F. and Sam\'{e}, A. and Govaert, G. and Aknin, P.},
	Date-Added = {2014-10-22 20:08:41 +0000},
	Date-Modified = {2014-10-22 20:08:41 +0000},
	Journal = {Neurocomputing},
	Number = {7-9},
	Pages = {1210--1221},
	Title = {A hidden process regression model for functional data description. Application to curve discrimination},
	Volume = {73},
	Year = {2010}
	}

@article{chamroukhi_et_al_NN2009,
	Address = {Oxford, UK, UK},
	Author = {Chamroukhi, F. and Sam\'{e}, A. and Govaert, G. and Aknin, P.},
	Date-Added = {2014-10-22 20:08:41 +0000},
	Date-Modified = {2014-10-22 20:08:41 +0000},
	Journal = {Neural Networks},
	Number = {5-6},
	Pages = {593--602},
	Publisher = {Elsevier Science Ltd.},
	Title = {Time series modeling by a regression approach based on a latent process},
	Volume = {22},
	Year = {2009}
	}
 

@InProceedings{Chamroukhi-IJCNN-2013,
  author = {F. Chamroukhi},
  title = {Robust {EM} algorithm for model-based curve clustering},
  Booktitle = {Proceedings of the International Joint Conference on Neural Networks (IJCNN), IEEE},
  Pages = {1--8},
    Address = {Dallas, Texas},
  month = {August},
  year = {2013}
}


@InProceedings{Chamroukhi-IJCNN-2012,
  author = {F. Chamroukhi and H. Glotin},
  title = {Mixture model-based functional discriminant analysis for curve classification},
  Booktitle = {Proceedings of the International Joint Conference on Neural Networks (IJCNN), IEEE},
  Pages = {1--8},
    Address = {Brisbane, Australia},
  month = {June},
  year = {2012}
}
 

 
@INPROCEEDINGS{Chamroukhi-ESANN-2012,
    Author         = {F. Chamroukhi and H.  Glotin and C. Rabouy},
    BOOKTITLE       = {Proceedings of XXth European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN)},
    Pages          = {281--286},
    Address = {Bruges, Belgium},
    Title          = {Functional Mixture Discriminant Analysis with hidden process regression for curve classification},
    Year           = {2012},
            Month = {April}
}


@phdthesis{Chamroukhi-HDR-2015,
	Author = {Chamroukhi, F.},
	School = {Universit\'e de Toulon},
	Title = {Statistical learning of latent data models for complex data analysis},
	Type = {{Accreditation to Supervise Research Thesis (HDR)}},
	Year = {2015}}
	
	
@phdthesis{Chamroukhi-PhD-2010,
	Author = {Chamroukhi, F.},
	School = {Universit\'e de Technologie de Compi\`egne},
	Title = {Hidden process regression for curve modeling, classification and tracking},
	Type = {Ph.{D}. Thesis},
	Year = {2010}}

@phdthesis{PhD-Marius-Bartcus,
	Author = {Bartcus, M.},
	School = {Universit\'e de Toulon, Laboratoire des Sciences de l'Information et des Syst\`emes (LSIS)},
	Title = {Bayesian non-parametric parsimonious mixtures for model-based clustering},
	Type = {Ph.{D}. Thesis},
	Month = {October},
	Year = {2015}}
@phdthesis{PhD-Dorra-Trabelsi,
	Author = {Trabelsi, D.},
	School = {Universit\'e Paris-Est Cr\'eteil, Laboratoire Images, Signaux et Syst\`emes Intelligents
(LiSSi)},
	Title = {Contribution \`a la reconnaissance non-intrusive d'activit\'es humaines.},
	Type = {Ph.{D}. Thesis},
	Month = {June},
	Year = {2013}}

%% Saved with string encoding Unicode (UTF-8) 
@article{Celeux-Letter-To-Editor-2011,
  TITLE = {{Letter to the editor: ''{A} framework for feature selection in clustering''}},
  AUTHOR = {Celeux, Gilles and Martin-Magniette, Marie-Laure and Maugis, Cathy and Raftery, Adrian E.},
  JOURNAL = {{Journal of the American Statistical Association}},
  PUBLISHER = {{Taylor \& Francis: SSH Journals}},
  VOLUME = {106},
  PAGES = {383},
  YEAR = {2011},
  DOI = {10.1198/jasa.2011.tm10681}
  }

@article {Zhou-2009,
    author       = {Zhou, Hui and Pan, Wei and Shen, Xiaotong},
    title        = {Penalized model-based clustering with unconstrained covariance matrices.},
    year         = {2009},
    journal      = {Electronic Journal of Statistics [electronic only]},
    volume       = {3},
    issn         = {1935-7524},
    pages        = {1473-1496, electronic only},
    publisher    = {Sponsored by Institute of Mathematical Statistics (IMS), Beachwood, OH and Bernoulli Society},
    abstract     = {Summary: Clustering is one of the most useful tools for high-dimensional analysis, e.g., for microarray data. It becomes challenging in presence of a large number of noise variables, which may mask underlying clustering structures. Therefore, noise removal through variable selection is necessary. One effective way is regularization for simultaneous parameter estimation and variable selection in model-based clustering. However, existing methods focus on regularizing the mean parameters representing centers of clusters, ignoring dependencies among variables within clusters, leading to incorrect orientations or shapes of the resulting clusters. In this article, we propose a regularized Gaussian mixture model with general covariance matrices, taking various dependencies into account. At the same time, this approach shrinks the means and covariance matrices, achieving better clustering and variable selection. To overcome one technical challenge in estimating possibly large covariance matrices, we derive an E-M algorithm to utilize the graphical lasso (Friedman et al. 2007) for parameter estimation. Numerical examples, including applications to microarray gene expression data, demonstrate the utility of the proposed method.}
    }

@ARTICLE{Tibshirani2010,
title = {A Framework for Feature Selection in Clustering},
author = {Witten, Daniela M. and Tibshirani, Robert},
year = {2010},
journal = {Journal of the American Statistical Association},
volume = {105},
number = {490},
pages = {713-726}
}


@ARTICLE{Raftery2006,
title = {Variable Selection for Model-Based Clustering},
author = {Raftery, Adrian E. and Dean, Nema},
year = {2006},
journal = {Journal of the American Statistical Association},
volume = {101},
pages = {168-178}
}

@ARTICLE{Maugis-2009-CSDA,
title = {Variable selection in model-based clustering: A general variable role modeling},
author = {Maugis, C. and Celeux, G. and Martin-Magniette, M.-L.},
year = {2009},
journal = {Computational Statistics \& Data Analysis},
volume = {53},
number = {11},
pages = {3872-3882},
abstract = {The currently available variable selection procedures in model-based clustering assume that the irrelevant clustering variables are all independent or are all linked with the relevant clustering variables. A more versatile variable selection model is proposed, taking into account three possible roles for each variable: The relevant clustering variables, the irrelevant clustering variables dependent on a part of the relevant clustering variables and the irrelevant clustering variables totally independent of all the relevant variables. A model selection criterion and a variable selection algorithm are derived for this new variable role modeling. The model identifiability and the consistency of the variable selection criterion are also established. Numerical experiments highlight the interest of this new modeling.}
}

@ARTICLE{Maugis-2009-Biometrics,
title = {Variable Selection for Clustering with Gaussian Mixture Models},
author = {Maugis, Cathy and Celeux, Gilles and Martin-Magniette, Marie-Laure},
year = {2009},
journal = {Biometrics},
volume = {65},
number = {3},
pages = {701-709}
}

@article{Law-2004,
    abstract = {{Clustering is a common unsupervised learning technique used to discover group structure in a set of data. While there exist many algorithms for clustering, the important issue of feature selection, that is, what attributes of the data should be used by the clustering algorithms, is rarely touched upon. Feature selection for clustering is difficult because, unlike in supervised learning, there are no class labels for the data and, thus, no obvious criteria to guide the search. Another important problem in clustering is the determination of the number of clusters, which clearly impacts and is influenced by the feature selection issue. In this paper, we propose the concept of feature saliency and introduce an expectation-maximization (EM) algorithm to estimate it, in the context of mixture-based clustering. Due to the introduction of a minimum message length model selection criterion, the saliency of irrelevant features is driven toward zero, which corresponds to performing feature selection. The criterion and algorithm are then extended to simultaneously estimate the feature saliencies and the number of clusters.}},
    address = {Washington, DC, USA},
    author = {Law, M. H. C. and Figueiredo, M. A. T. and Jain, A. K.}, 
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
    month = sep,
    number = {9},
    pages = {1154--1166}, 
    publisher = {IEEE Computer Society},
    title = {{Simultaneous feature selection and clustering using mixture models}},
    volume = {26},
    year = {2004}
}



@incollection{celeux_labelswitching98,
year={1998},
booktitle={COMPSTAT},
editor={Payne, Roger and Green, Peter},
title={{Bayesian Inference for Mixture: The Label Switching Problem}},
publisher={Physica-Verlag HD},
keywords={MCMC algorithm; labelling latent structure; k-means algorithm},
author={Celeux, Gilles},
pages={227-232}
}


@article{Abraham2003,
	Abstract = { Data in many different fields come to practitioners through a process naturally described as functional. Although data are gathered as finite vector and may contain measurement errors, the functional form have to be taken into account. We propose a clustering procedure of such data emphasizing the functional nature of the objects. The new clustering method consists of two stages: fitting the functional data by B-splines and partitioning the estimated model coefficients using a "k"-means algorithm. Strong consistency of the clustering method is proved and a real-world example from food industry is given.},
	Author = {Abraham, C. and Cornillon, P. A.  and Matzner-Lober, E. and Molinari, N.},
	Journal = {Scandinavian Journal of Statistics},
	Number = {3},
	Pages = {581-595},
	Title = {{Unsupervised Curve Clustering using B-Splines}},
	Volume = {30},
	Year = {2003}}

@article{FerratyANDVieu2002,
	Author = {F. Ferraty and P. Vieu},
	Journal = {Computational Statistics},
	Number = {4},
	Pages = {545--564},
	Title = {The functional nonparametric model and application to spectrometric data},
	Volume = {17},
	Year = {2002}}

@article{TehHDP2006,
	Abstract = {We consider problems involving groups of data, where each observation within a group is a draw from a mixture model, and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the "Chinese restaurant franchise". We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures, and describe applications to problems in information retrieval and text modelling.},
	Author = {Teh, Yee Whye and Jordan, Michael I. and Beal, Matthew J. and Blei, David M.},
	Journal = {Journal of the American Statistical Association},
	Number = {476},
	Pages = {1566--1581},
	Title = {Hierarchical Dirichlet Processes},
	Volume = {101},
	Year = {2006}}

@book{Govaert_Nadif-Co-ClusteingBook,
	Author = {G\'erard Govaert and Mohamed Nadif},
	Month = {November},
	Note = {256 pages},
	Publisher = {Wiley-ISTE},
	Series = {Computer engineering series},
	Title = {Co-Clustering},
	Year = {2013}}

@article{Govaert_Nadif2008,
	Author = {G\'erard Govaert and Mohamed Nadif},
	Journal = {Computational Statistics \& Data Analysis},
	Keywords = {Co-clustering},
	Number = {6},
	Pages = {3233 --3245},
	Title = {Block clustering with Bernoulli mixture models: Comparison of different approaches},
	Volume = {52},
	Year = {2008}}

@article{Govaert_Nadif2006,
	Author = {G{\'e}rard Govaert and Mohamed Nadif},
	Journal = {Soft Computing},
	Number = {5},
	Pages = {415--422},
	Title = {Fuzzy clustering to estimate the parameters of block mixture models},
	Volume = {10},
	Year = {2006}}

@article{Govaert_Nadif2003,
	Author = {G{\'e}rard Govaert and Mohamed Nadif},
	Journal = {Pattern Recognition},
	Keywords = {Block \{CEM\} algorithm},
	Note = {Biometrics},
	Number = {2},
	Pages = {463 - 473},
	Title = {Clustering with block mixture models},
	Volume = {36},
	Year = {2003}}

@article{BleiANDJordan2006,
	Added-At = {2010-03-25T16:34:23.000+0100},
	Author = {Blei, David M. and Jordan, Michael I.},
	Doi = {10.1214/06-BA104},
	Journal = {Bayesian Analysis},
	Keywords = {Dirichlet process Variational methods},
	Number = {1},
	Pages = {121--144},
	Title = {Variational inference for Dirichlet process mixtures},
	Volume = {1},
	Year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1214/06-BA104}}

@article{celeuxetgovaert_mixture_classif_93,
	Author = {Celeux, G. and Govaert, G.},
	Journal = {Journal of Statistical Computation and Simulation},
	Pages = {127--146},
	Title = {Comparison of the Mixture and the Classification Maximum likelihood in Cluster Analysis},
	Volume = {47},
	Year = {1993}}

@book{mclachlan_basford88,
	Author = {G.J. McLachlan and K.E. Basford},
	Publisher = {Marcel Dekker, New York},
	Title = {Mixture Models: Inference and Applications to Clustering},
	Year = {1988}}

@article{mbc_banfield_raftery93,
	Author = {Banfield, Jeffrey D. and Raftery, Adrian E.},
	Journal = {Biometrics},
	Number = {3},
	Pages = {803--821},
	Title = {Model-Based {G}aussian and Non-{G}aussian Clustering},
	Volume = {49},
	Year = {1993}}

@techreport{KavukcuogluEtAl2008,
	Author = {Koray Kavukcuoglu and Marc'Aurelio Ranzato and Yann LeCun},
	Institution = {Department of Computer Science, Courant Institute of Mathematical Sciences, New York University},
	Number = {CBLL-TR-2008-12-01},
	Title = {Fast Inference in Sparse Coding Algorithms with Applications to Object Recognition},
	Year = {2008}}

@article{Hinton-2006-Science,
	Author = {G.E. Hinton and R.R. Salakhutdinov},
	Journal = {Science},
	Number = {5786},
	Pages = { 504--507},
	Title = {Reducing the Dimensionality of Data with Neural Networks},
	Volume = {313},
	Month = {July},
	Year = {2006}
	} 
@ARTICLE{Bengio-2009,
    author = {Bengio, Yoshua},
     title = {Learning deep architectures for {AI}},
   journal = {Foundations and Trends in Machine Learning},
    volume = {2},
    number = {1},
      year = {2009},
     pages = {1--127},
      note = {Also published as a book. Now Publishers, 2009.},
       doi = {10.1561/2200000006},
  abstract = {Theoretical results suggest that in order to learn the kind of
complicated functions that can represent high-level abstractions (e.g. in
vision, language, and other AI-level tasks), one may need {\insist deep
architectures}. Deep architectures are composed of multiple levels of non-linear
operations, such as in neural nets with many hidden layers or in complicated
propositional formulae re-using many sub-formulae. Searching the
parameter space of deep architectures is a difficult task, but
learning algorithms such as those for Deep Belief Networks have recently been proposed
to tackle this problem with notable success, beating the state-of-the-art
in certain areas. This paper discusses the motivations and principles regarding 
learning algorithms for deep architectures,  in particular those exploiting as
building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines,
used to construct deeper models such as Deep Belief Networks.}
}

@INCOLLECTION{Bengio+chapter2007,
     author = {Bengio, Yoshua and {LeCun}, Yann},
     editor = {Bottou, {L{\'{e}}on} and Chapelle, Olivier and DeCoste, D. and Weston, J.},
      title = {Scaling Learning Algorithms towards {AI}},
  booktitle = {Large Scale Kernel Machines},
       year = {2007},
  publisher = {MIT Press},
   abstract = {One long-term goal of machine learning research is to produce methods that
are applicable to highly complex tasks, such as perception (vision, audition), reasoning,
intelligent control, and other artificially intelligent behaviors. We argue
that in order to progress toward this goal, the Machine Learning community must
endeavor to discover algorithms that can learn highly complex functions, with minimal
need for prior knowledge, and with minimal human intervention. We present
mathematical and empirical evidence suggesting that many popular approaches
to non-parametric learning, particularly kernel methods, are fundamentally limited
in their ability to learn complex high-dimensional functions. Our analysis
focuses on two problems. First, kernel machines are shallow architectures, in
which one large layer of simple template matchers is followed by a single layer
of trainable coefficients. We argue that shallow architectures can be very inefficient
in terms of required number of computational elements and examples. Second,
we analyze a limitation of kernel machines with a local kernel, linked to the
curse of dimensionality, that applies to supervised, unsupervised (manifold learning)
and semi-supervised kernel machines. Using empirical results on invariant
image recognition tasks, kernel methods are compared with deep architectures, in
which lower-level features or concepts are progressively combined into more abstract
and higher-level representations. We argue that deep architectures have the
potential to generalize in non-local ways, i.e., beyond immediate neighbors, and
that this is crucial in order to make progress on the kind of complex tasks required
for artificial intelligence.},
cat={B},topics={HighDimensional},
}
@article{Hinton-2006-ContrastiveDivergence,
 author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
 title = {A Fast Learning Algorithm for Deep Belief Nets},
 journal = {Neural Computation},
 volume = {18},
 number = {7},
 month = July,
 year = {2006},
 pages = {1527--1554},
 numpages = {28}
 } 
 
@inproceedings{Ranzato2008,
title = {Sparse Feature Learning for Deep Belief Networks},
author = {M. A. Ranzato and Y. Boureau and Y. L. Cun},
booktitle = {Advances in Neural Information Processing Systems (NIPS) 20},
editor = {J.C. Platt and D. Koller and Y. Singer and S.T. Roweis},
pages = {1185--1192},
year = {2008}
}


@techreport{Patel2015,
	Author = {Ankit B. Patel and Tan Nguyen and Richard G. Baraniuk},
	Institution = {Rice University Electrical and Computer Engineering Dept.},
	Number = {Technical Report No 2015-1},
	Title = {A Probabilistic Theory of Deep Learning},
	Month = {April},
	Year = {2015},
	Url = {http://arxiv.org/abs/1504.00641v1}
	}
 


@phdthesis{KavukcuogluPhD,
	Author = {Koray Kavukcuoglu},
	School = {Department of Computer Science, New York University},
	Title = {Learning Feature Hierarchies for Object Recognition},
	Year = {2011}}

@misc{blockclustePackage,
	Author = {Parmeet Singh Bhatia and Serge Iovleff and Gerard Goavert and Vincent Brault and Christophe Biernacki and Gilles Celeux.},
	Howpublished = {Software},
	Month = {july},
	Title = {\textsf{blockcluster} Package, Version 3.0.1},
	Year = {2014}}

@article{BiernackiCGL06,
	Author = {Christophe Biernacki and Gilles Celeux and G{\'{e}}rard Govaert and Florent Langrognet},
	Journal = {Computational Statistics {\&} Data Analysis},
	Number = {2},
	Pages = {587--600},
	Title = {Model-based cluster and discriminant analysis with the {MIXMOD} software},
	Volume = {51},
	Year = {2006}}

@article{celeux_and_gilda_NEC_96,
	Abstract = {In this paper, we consider an entropy criterion to estimate the number of clusters arising from a mixture model. This criterion is derived from a relation linking the likelihood and the classification likelihood of a mixture. Its performance is investigated through Monte Carlo experiments, and it shows favorable results compared to other classical criteria.},
	Author = {Celeux, G. and Soromenho, G.},
	Journal = {Journal of Classification},
	Month = {September},
	Number = {2},
	Pages = {195-212},
	Title = {An entropy criterion for assessing the number of clusters in a mixture model},
	Volume = {13},
	Year = {1996}}

@techreport{celeux_and_gilda_NEC_93,
	Author = {Celeux, G. and Soromenho, G.},
	Institution = {The French National Institute for Research in Computer Science and Control (INRIA)},
	Number = {RR-1874},
	Title = {An entropy criterion for assessing the number of clusters in a mixture model},
	Year = {1993}}

@article{biernacki_et_al_improvement_NEC,
	Author = {Biernacki, C. and G. Celeux and Govaert, G.},
	Journal = {Pattern Recognition Letters},
	Number = {3},
	Pages = {267-272},
	Title = {An improvement of the NEC criterion for assessing the number of clusters in a mixture model},
	Volume = {20},
	Year = {1999}}

@article{celeux_and_soromenho_NEC,
	Author = {Celeux, G.. and Soromenho, Gilda.},
	Journal = {Journal of Classification},
	Month = {septembre},
	Number = {2},
	Pages = {195-212},
	Title = {An entropy criterion for assessing the number of clusters in a mixture model},
	Volume = {13},
	Year = {1996}}

@article{Neal2000MCMC-DPM,
	Author = {Radford M. Neal},
	Journal = {Journal of Computational and Graphical Statistics},
	Number = {2},
	Pages = {249--265},
	Title = {Markov chain sampling methods for Dirichlet process mixture models},
	Volume = {9},
	Year = {2000}}

@article{Ishwaren_dp2002,
	Author = {Ishwaren, H. and Zarepour, M.},
	Journal = {Canadian Journal of Statistics},
	Pages = {269-283},
	Title = {Exact and approximate representations for the sum Dirichlet process},
	Volume = {30},
	Year = {2002}}

@article{Diebolt_and_Robert_1994,
	Author = {Diebolt, Jean and Robert, Christian P.},
	Journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	Number = {2},
	Pages = {363--375},
	Ttitle = {stimation of Finite Mixture Distributions through Bayesian Sampling},
	Volume = {56},
	Year = {1994}}

@article{blackwellm73,
	Added-At = {2011-05-09T23:10:52.000+0200},
	Author = {Blackwell, D. and MacQueen, J.},
	Biburl = {http://www.bibsonomy.org/bibtex/2a826a37ef03800a8d0fb4746b0ea781d/josephausterwei},
	Interhash = {a9f56393197f3c5055aceb941dd16bfc},
	Intrahash = {a826a37ef03800a8d0fb4746b0ea781d},
	Journal = {The Annals of Statistics},
	Keywords = {imported},
	Pages = {353-355},
	Timestamp = {2011-05-09T23:10:52.000+0200},
	Title = {Ferguson distributions via {P}olya urn schemes},
	Volume = 1,
	Year = 1973}

@article{Antoniadis2013,
	Author = {A. Antoniadis and X. Brossat and J. Cugliari and J.-M. Poggi},
	Journal = {International Journal of Wavelets, Multiresolution and Information Processing},
	Number = {1},
	Optkey = {DOI: 10.1142/S0219691313500033},
	Title = {Functional Clustering using Wavelets},
	Volume = {11},
	Year = {2013}}

@phdthesis{Lomet_PhD_2012,
	Author = {Lomet, Aurore},
	School = {Universit\'e de Technologie de Compi\`egne},
	Title = {S\'election de mod\`ele pour la classification crois\'ee de donn\'ees continues},
	Type = {Ph.{D}. Thesis},
	Year = {2012}}

@inproceedings{LometEtAl2012b,
	Author = {Lomet, Aurore and Govaert, G{\'e}rard and Grandvalet, Yves},
	Booktitle = {ICDM Workshops},
	Pages = {147--153},
	Title = {An Approximation of the Integrated Classification Likelihood for the Latent Block Model},
	Year = {2012}}

@inproceedings{LometEtAl2012a,
	Author = {Lomet, Aurore and Govaert, G{\'e}rard and Grandvalet, Yves},
	Booktitle = {20th International Conference on Computational Statistics (COMPSTAT)},
	Pages = {519--530},
	Title = {Model selection in block clustering by the integrated classification likelihood},
	Year = {2012}}

@incollection{Celeux2009DataAnalysis,
	Address = {NY},
	Author = {Celeux, G.},
	Booktitle = {Data Analysis},
	Chapter = {7},
	Editor = {Govaert, G.},
	Pages = {181--214},
	Publisher = {John Wiley},
	Title = {Discriminant Analysis},
	Year = {2009}}

@techreport{Devijver2014-MBC-FDA,
	Archiveprefix = {arXiv},
	Author = {Devijver, E.},
	Date-Modified = {2015-08-21 21:30:37 +0000},
	Eprint = {1409.1333},
	Institution = {D\'epartement de Math\'ematiques, Universit\'e Paris-Sud},
	Journal = {ArXiv e-prints},
	Primaryclass = {math.ST},
	Title = {{Model-based clustering for high-dimensional data. Application to functional data}},
	Year = 2014}

@misc{crp_intro_navaro_perfors,
	Author = {Navarro, D. and Perfors, A.},
	Institution = {University of Adelaide},
	Journal = {COMPSCI 3016: Computational Cognitive Science},
	Title = {The Chinese restaurant process}}

@inbook{neal_and_hinton_incremental_EM_1998,
	Author = {Neal, R. and Hinton, G. E.},
	Booktitle = {in M. I. Jordan (editor) Learning in Graphical Models},
	Pages = {355--368},
	Posted-At = {2009-08-18 14:04:16},
	Publisher = {Dordrecht: Kluwer Academic Publishers},
	Title = {A view of the EM algorithm that justifies incremental, sparse, and other variants},
	Year = {1998}}

@article{Ng_and_McLachlan_2014,
	Author = {S.K. Ng and G.J. McLachlan},
	Journal = {Computational Statistics \& Data Analysis},
	Keywords = {\{EM\} algorithm},
	Number = {0},
	Pages = {43-- 51},
	Title = {Mixture models for clustering multilevel growth trajectories},
	Volume = {71},
	Year = {2014}}

@article{Devijver2014ArxivRegMix-Slect,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1409.1331D},
	Archiveprefix = {arXiv},
	Author = {{Devijver}, E.},
	Eprint = {1409.1331},
	Journal = {ArXiv e-prints},
	Keywords = {Mathematics - Statistics Theory},
	Month = sep,
	Primaryclass = {math.ST},
	Title = {{Finite mixture regression: A sparse variable selection by model selection for clustering}},
	Year = 2014}

@inproceedings{keribin2010latenblockSEM,
	Address = {Marseille},
	Author = {Keribin, Christine and Govaert, G{\'e}rard and Celeux, Gilles},
	Booktitle = {42{\`e}mes Journ{\'e}es de Statistique},
	Title = {Estimation d'un mod{\`e}le {\`a} blocs latents par l'algorithme {SEM}},
	Year = {2010}}

@inproceedings{keribin2012latenblockVB,
	Author = {Keribin, Christine and Brault, Vincent and Celeux, Gilles and Govaert, G{\'e}rard},
	Booktitle = {Proceedings of COMPSTAT},
	Title = {Model selection for the binary latent block model},
	Year = {2012}}

@article{keribin2014latenblockVB,
	Author = {Keribin, Christine and Brault, Vincent and Celeux, Gilles and Govaert, G{\'e}rard},
	Doi = {10.1007/s11222-014-9472-2},
	Issn = {0960-3174},
	Journal = {Statistics and Computing},
	Keywords = {EM algorithm; Variational approximation; Stochastic EM; Bayesian inference; Gibbs sampling; BIC criterion; Integrated completed likelihood},
	Language = {English},
	Pages = {1-16},
	Publisher = {Springer US},
	Title = {Estimation and selection for the latent block model on categorical data},
	Url = {http://dx.doi.org/10.1007/s11222-014-9472-2},
	Year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s11222-014-9472-2}}

@article{Caillol1997,
	Author = {H. Caillol and W. Pieczynski and A. Hillion},
	Bibsource = {DBLP, http://dblp.uni-trier.de},
	Ee = {http://dx.doi.org/10.1109/83.557353},
	Journal = {IEEE Transactions on Image Processing},
	Number = {3},
	Pages = {425-440},
	Title = {Estimation of fuzzy Gaussian mixture and unsupervised statistical image segmentation},
	Volume = {6},
	Year = {1997}}

@article{Tibshirani96LASSO,
	Author = {Robert Tibshirani},
	Journal = {Journal of the Royal Statistical Society, Series B},
	Number = {1},
	Pages = {267--288},
	Title = {Regression Shrinkage and Selection Via the Lasso},
	Volume = {58},
	Year = {1996}}

@article{BasisPursuit,
	Author = {S. Chen and D. Donoho and M. Saunders},
	Journal = {SIAM Journal on Scientific Computing},
	Optnumber = {1},
	Optpages = {33-61},
	Optvolume = {20},
	Title = {Atomic decomposition by basis pursuit,},
	Year = {1999}}

@inproceedings{HerzetANDDremeau2014,
	Author = {C. Herzet and A. Dr\'emeau},
	Booktitle = {CoRR abs/1401.7538},
	Title = {Bayesian pursuit algorithms},
	Year = {2014}}

@inproceedings{DremeauANDHerzet2011,
	Author = {A. Dr\'emeau and C. Herzet},
	Booktitle = {in IEEE International Workshop on Statistical Signal Processing SSP'11)},
	Title = {Soft bayesian pursuit algorithm for sparse representations},
	Year = {2011}}

@phdthesis{YuanquingLinPhD,
	Author = {Yuanquing Lin},
	School = {University of Pennsylvania},
	Title = {$l_1$-Norm sparse Bayesian learning: Theory and applications},
	Year = {2008}}

@phdthesis{DavidPaulWipfPhD,
	Author = {Wipf, David Paul},
	School = {University of California, San Diego},
	Title = {Bayesian Methods for Finding Sparse Representations},
	Year = {2006}}

@article{OlshausenANDField1997,
	Author = {B. A. Olshausen and D. J. Field},
	Journal = {Vision Research},
	Optnumber = {23},
	Optpages = {3311-3325},
	Optvolume = {37},
	Title = {Sparse coding with an overcomplete basis set: a strategy employed by V1?},
	Year = {1997}}

@article{OlshausenANDFieldNature1996,
	Author = {B. A. Olshausen and D. J. Field},
	Journal = {Nature},
	Pages = {607-609},
	Title = {Emergence of simple celle receptive field properties by learning a sparse code for nature images},
	Volume = {381},
	Year = {1996}}

@article{OlshausenANDField2004,
	Author = {B. A. Olshausen and D. J. Field},
	Journal = {Current Opinion in Neurobiology},
	Title = {Sparse coding of sensory inputs},
	Year = {2004}}

@article{FOCUS,
	Author = {I. Gorodnitsky and D. R. Bhaskar},
	Journal = {IEEE Transactions on Signal Processing},
	Number = {3},
	Pages = {600-616},
	Title = {Sparse signal reconstruction from limited data using {FOCUSS}: a re-weighted minimum norm algorithm},
	Volume = {45},
	Year = {1997}}

@article{Fearnhead2006,
	Author = {Fearnhead, Paul},
	Issue = {2},
	Journal = {Statistics and Computing},
	Pages = {203--213},
	Title = {Exact and efficient {B}ayesian inference for multiple changepoint problems},
	Volume = {16},
	Year = {2006}}

@article{Zhao,
	Author = {Y. Zhao and Y. Zhang},
	Journal = {Advances in Space Research},
	Pages = {1955-1959},
	Title = {Comparison of decision tree methods for finding active objects},
	Volume = {41},
	Year = {2008}}

@misc{Xsens,
	Author = {B.V. Enschede},
	Key = {Xsens Technologies},
	Title = {MTi and MTx User Manual and Technical Documentation},
	Url = {www.xsens.com},
	Year = {2009},
	Bdsk-Url-1 = {www.xsens.com}}

@article{Altun,
	Author = {K. Altun and B. Barshan and O. Tuncel},
	Issue = {10},
	Journal = {Pattern Recognition},
	Month = {October},
	Pages = {3605--3620},
	Title = {Comparative study on classifying human activities with miniature inertial and magnetic sensors},
	Volume = {43},
	Year = {2010}}

@article{Preece,
	Author = {S.J. Preece and J.Y. Goulermas and L.P.J. Kenney and D.Howard and K.Meijer and R.Crompton},
	Journal = {Physiol. Meas},
	Number = {4},
	Title = {Activity identification using body-mounted sensors--a review of classification techniques},
	Volume = {30},
	Year = {2009}}

@article{Kavanagh,
	Author = {J.J. Kavanagh and H.B. Menz},
	Journal = {Gait \& Posture},
	Pages = {1--15},
	Title = {Accelerometry: a technique for quantifying movement patterns during walking},
	Volume = {28},
	Year = {2008}}

@article{Mathie,
	Author = {Mathie, M. J. and Celler, B. G. and Lovell, N. H. and Coster, A. C.},
	Journal = {Medical \& biological engineering \& computing},
	Keywords = {accelerometer, activity\_recognition, behavior},
	Number = {5},
	Pages = {679--687},
	Title = {Classification of basic daily movements using a triaxial accelerometer.},
	Volume = {42},
	Year = {2004}}

@article{Kawada,
	Author = {T. Kawada and T. Shimizu and A. Fujii and Y. Kuratomi and S. Suto and T. Kanai and A. Nishime and K. Sato, Y. Otsuka},
	Journal = {Work},
	Pages = {157--160},
	Title = {Activity and sleeping time monitored by an accelerometer in rotating shift workers},
	Volume = {30},
	Year = {2008}}

@article{Menz,
	Abstract = {{Background. A large proportion of falls in older people occur when walking, however the mechanisms underlying impaired balance during gait are poorly understood. This study evaluated acceleration patterns of the head and pelvis when walking on a level and an unpredictably irregular surface to determine whether older people at risk of falling demonstrate an impaired ability to stabilize the body under challenging conditions. Methods. One hundred community-dwelling older people aged between 75 and 93 years were evaluated for their risk of falling using a range of physiological tests previously found to be accurate predictors of falling in prospective studies. Temporo-spatial gait parameters and acceleration patterns at the head and pelvis were then measured in three orthogonal planes while subjects walked on a flat corridor and an unpredictably irregular walkway. Harmonic ratios of head and pelvis accelerations in each plane were calculated to provide an indicator of stability. Results. Subjects with a high risk of falling exhibited reduced temporo-spatial gait parameters and increased step timing variability. Harmonic ratios of acceleration patterns were reduced at the head and pelvis in the vertical and antero-posterior directions. These differences were particularly evident when walking on the irregular surface. Conclusion. Older people at risk of falling adopt a more conservative basic walking pattern, but this does not ensure that the movements of the head and pelvis are stable. The irregular pelvis and head accelerations evident in the high risk group suggests that these subjects may have difficulty controlling trunk motion and maintaining a stable visual field when walking, particularly on irregular terrain.}},
	Author = {Menz, H. B. and Lord, S. R. and Fitzpatrick, R. C.},
	Journal = {Journals of Gerontology Series A: Biological and Medical Sciences},
	Keywords = {accelerometer, gait, motion},
	Month = may,
	Number = {5},
	Pages = {446--452},
	Publisher = {Geron Soc America},
	Title = {{Acceleration Patterns of the Head and Pelvis When Walking Are Associated With Risk of Falling in Community-Dwelling Older People}},
	Volume = {58},
	Year = {2003}}

@article{Veltink,
	Abstract = {{Rehabilitation treatment may be improved by objective analysis of
activities of daily living. For this reason, the feasibility of
distinguishing several static and dynamic activities (standing, sitting,
lying, walking, ascending stairs, descending stairs, cycling) using a
small set of two or three uniaxial accelerometers mounted on the body
was investigated. The accelerometer signals can be measured with a
portable data acquisition system, which potentially makes it possible to
perform online detection of static and dynamic activities in the home
environment. However, the procedures described in this paper have yet to
be evaluated in the home environment. Experiments were conducted on ten
healthy subjects, with accelerometers mounted on several positions and
orientations on the body, performing static and dynamic activities
according to a fixed protocol. Specifically, accelerometers on the
sternum and thigh were evaluated. These accelerometers were oriented in
the sagittal plane, perpendicular to the long axis of the segment
(tangential), or along this axis (radial). First, discrimination between
the static or dynamic character of activities was investigated. This
appeared to be feasible using an rms-detector applied on the signal of
one sensor tangentially mounted on the thigh. Second, the distinction
between static activities was investigated. Standing, sitting, lying
supine, on a side and prone could be distinguished by observing the
static signals of two accelerometers, one mounted tangentially on the
thigh, and the second mounted radially on the sternum. Third, the
distinction between the cyclical dynamic activities walking, stair
ascent, stair descent and cycling was investigated. The discriminating
potentials of several features of the accelerometer signals were
assessed: the mean value, the standard deviation, the cycle time and the
morphology. Signal morphology was expressed by the maximal
cross-correlation coefficients with template signals for the different
dynamic activities. The mean signal values and signal morphology of
accelerometers mounted tangentially on the thigh and the sternum
appeared to contribute to the discrimination of dynamic activities with
varying detection performances. The standard deviation of the signal and
the cycle time were primarily related to the speed of the dynamic
activities, and did not contribute to the discrimination of the
activities. Therefore, discrimination of dynamic activities on the basis
of the combined evaluation of the mean signal value and signal
morphology is proposed}},
	Author = {Veltink, P. H. and Bussmann, HansB and de Vries, W. and Martens, WimL and Van Lummel, R. C.},
	Journal = {IEEE Transactions on Rehabilitation Engineering},
	Keywords = {accelerometer, activity\_recognition, posture},
	Month = dec,
	Number = {4},
	Pages = {375--385},
	Title = {{Detection of static and dynamic activities using uniaxial accelerometers}},
	Volume = {4},
	Year = {1996}}

@article{Lyons,
	Author = {Lyons, G. and Culhane, K. and Hilton, D. and Grace, P. and Lyons, D.},
	Journal = {Medical Engineering \& Physics},
	Keywords = {activity\_recognition},
	Month = jul,
	Number = {6},
	Pages = {497--504},
	Title = {{A description of an accelerometer-based mobility monitoring technique}},
	Volume = {27},
	Year = {2005}}

@article{Chuy,
	Author = {O. Chuy and Y. Hirata and W. Zhi Dong and K. Kosuge},
	Journal = {IEEE Transactions on Robotics},
	Number = {5},
	Pages = {899--908},
	Title = {A control approach based on passive behavior to enhance user interaction},
	Volume = {23},
	Year = {2007}}

@article{Tsukahara,
	Author = {A. Tsukahara and R. Kawanishi and Y. Hasegawa and Y. Sanka},
	Journal = {Advanced Robotics},
	Number = {11},
	Pages = {1615--1638},
	Title = {Sit-To-Stand and Stand-To-Sit Transfer Support for Complete Paraplegic Patients with Robot Suit HAL},
	Volume = {24},
	Year = {2010}}

@inproceedings{Hirata,
	Author = {Y. Hirata and S. Komatsuda and K. Kosuge},
	Booktitle = {Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems},
	Month = {September},
	Pages = {1222--1228},
	Title = {Fall prevention control of passive intelligent walker based on human model},
	Year = {2008}}

@article{Rothney,
	Author = {Rothney, Megan P. and Neumann, Megan and Beziat, Ashley and Chen, Kong Y.},
	Journal = {Journal of Applied Physiology},
	Keywords = {accelerometer, activities, bao, cit},
	Month = oct,
	Number = {4},
	Pages = {1419--1427},
	Priority = {2},
	Title = {{An artificial neural network model of energy expenditure using nonintegrated acceleration signals}},
	Volume = {103},
	Year = {2007}}

@article{Foerster,
	Author = {Foerster, F.},
	Journal = {Computers in Human Behavior},
	Keywords = {accelerometer, activity\_recognition, adl, monitoring, recognition\_tasks, validity},
	Month = sep,
	Number = {5},
	Pages = {571--583},
	Priority = {2},
	Title = {{Detection of posture and motion by accelerometry: a validation study in ambulatory monitoring}},
	Volume = {15},
	Year = {1999}}

@article{Lau,
	Author = {Lau, Hong-yin Y. and Tong, Kai-yu Y. and Zhu, Hailong},
	Journal = {Human movement science},
	Keywords = {classification, gait, reimport, stroke},
	Month = aug,
	Number = {4},
	Pages = {504--514},
	Title = {{Support vector machine for classification of walking conditions of persons after stroke with dropped foot.}},
	Volume = {28},
	Year = {2009}}

@inproceedings{Long,
	Author = {Long, Xi and Yin, Bin and Aarts, R. M.},
	Booktitle = {2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
	Keywords = {ieee-accelfeatures},
	Location = {Minneapolis, MN},
	Month = sep,
	Pages = {6107--6110},
	Publisher = {IEEE},
	Title = {{Single-accelerometer-based daily physical activity classification}},
	Year = {2009}}

@article{Allen,
	Abstract = {{Accelerometry shows promise in providing an inexpensive but effective means of long-term ambulatory monitoring of elderly patients. The accurate classification of everyday movements should allow such a monitoring system to exhibit greater 'intelligence', improving its ability to detect and predict falls by forming a more specific picture of the activities of a person and thereby allowing more accurate tracking of the health parameters associated with those activities. With this in mind, this study aims to develop more robust and effective methods for the classification of postures and motions from data obtained using a single, waist-mounted, triaxial accelerometer; in particular, aiming to improve the flexibility and generality of the monitoring system, making it better able to detect and identify short-duration movements and more adaptable to a specific person or device. Two movement classification methods were investigated: a rule-based Heuristic system and a Gaussian mixture model (GMM)-based system. A novel time-domain feature extraction method is proposed for the GMM system to allow better detection of short-duration movements. A method for adapting the GMMs to compensate for the problem of limited user-specific training data is also proposed and investigated. Classification performance was considered in relation to data gathered in an unsupervised, directed routine conducted in a three-month field trial involving six elderly subjects. The GMM system was found to achieve a mean accuracy of 91.3\%, distinguishing between three postures (sitting, standing and lying) and five movements (sit-to-stand, stand-to-sit, lie-to-stand, stand-to-lie and walking), compared to 71.1\% achieved by the Heuristic system. The adaptation method was found to offer a mean accuracy of 92.2\%; a relative improvement of 20.2\% over tests without subject-specific data and 4.5\% over tests using only a limited amount of subject-specific data. While limited to a restricted subset of possible motions and postures, these results provide a significant step in the search for a more robust and accurate ambulatory classification system.}},
	Author = {Allen, F. R. and Ambikairajah, E. and Lovell, N. H. and Celler, B. G.},
	Journal = {Physiological Measurement},
	Month = oct,
	Number = {10},
	Pages = {935--951},
	Posted-At = {2007-02-24 10:51:48},
	Priority = {2},
	Publisher = {Institute of Physics Publishing},
	Title = {{Classification of a known sequence of motions and postures from accelerometry data using adapted Gaussian mixture models}},
	Volume = {27},
	Year = {2006}}

@article{Mannini,
	Author = {A. Mannini and A.M. Sabatini},
	Journal = {Sensors},
	Pages = {1154--1175},
	Title = {Machine learning methods for classifying human physical activity from On-Body Accelerometers},
	Volume = {10},
	Year = {2010}}

@article{Pogorelc2012,
	Author = {Pogorelc, B. and Gams, M.},
	Journal = {Journal of Ambient Intelligence and Smart Environments, (to appear)},
	Title = {Home-based health monitoring of the elderly through gait recognition},
	Year = {2012}}

@article{cappe_and_moulines_online_EM_JRSS2009,
	Author = {Capp\'{e}, O. and Moulines, E.},
	Journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	Month = {June},
	Number = {3},
	Pages = {593--613},
	Title = {On-line expectation-maximization algorithm for latent data models},
	Volume = {71},
	Year = {2009}}

@article{Kostov1995,
	Author = {Kostov, A. and Andrews, B.J. and Popovic, D.B. and Stein, R.B. and Armstrong, W.W.},
	Journal = {IEEE Trans. Biomed. Eng.},
	Pages = {541-551},
	Title = {Machine learning in control of functional electrical stimulation systems for locomotion},
	Volume = {42},
	Year = {1995}}

@article{hussein2002,
	Author = {Hussein, S.E. and Granat, M.H.},
	Journal = {IEEE Engineering in Medicine and Biology Magazine},
	Number = {6},
	Pages = {123-129},
	Title = {Intention detection using a neuro-fuzzy EMG classifier},
	Volume = {21},
	Year = {2002}}

@inbook{Keogh93segmentingtime,
	Author = {Eamonn Keogh, Selina Chu, David Hart and Michael Pazzani},
	Chapter = {In an Edited Volume, Data mining in Time Series Databases},
	Pages = {1-22},
	Publisher = {Published by World Scientific},
	Title = {Segmenting Time Series: A Survey and Novel Approach},
	Year = {1993}}

@book{Nikiforov,
	Author = {M. Basseville and I. V. Nikiforov},
	Publisher = {Englewood Cliffs, NJ: Prentice-Hall},
	Title = {Detection of Abrupt Changes: Theory and Application},
	Year = {1993}}

@book{Brodsky93,
	Author = {B. Brodsky and B. Darkhovsky},
	Publisher = {Boston, MA: Kluwer Academic},
	Title = {Nonparametric Methods in Change-Point Problems},
	Year = {1993}}

@book{joachim_book,
	Author = {T. Joachims},
	Publisher = {Kluwer/Springer},
	Title = {Learning to Classify Text Using Support Vector Machines -- Methods, Theory, and Algorithms},
	Year = 2002}

@article{Ajo_Mataric_Jenkins,
	Author = {A. Fod and M. J. Mataric and O. C. Jenkins},
	Journal = {Autonomous Robots},
	Number = 1,
	Pages = {39-54},
	Title = {Automated Derivation of Primitives for Movement Classification},
	Volume = 12,
	Year = {2002}}

@inproceedings{Kohlmorgen02adynamic,
	Author = {Jens Kohlmorgen and Steven Lemm},
	Booktitle = {In Advances in Neural Information Processing Systems (NIPS)},
	Pages = {793-800},
	Title = {A Dynamic HMM for On-line Segmentation of Sequential Data},
	Year = {2002}}

@inproceedings{KulicAndAll2008,
	Address = {Pasadena, CA, USA},
	Author = {Dana Kuli\'c, Wataru Takano and Yoshihiko Nakamura},
	Booktitle = {In proceedings of the IEEE International Conference on Robotics and Automation},
	Month = {May 19-23},
	Pages = {2591-2598},
	Title = {Combining Automated On-line Segmentation and Incremental Clustering for Whole Body Motions},
	Year = {2008}}

@inproceedings{KulicAndNakamura2008,
	Address = {Nice, France},
	Author = {Dana Kuli\'c and Yoshihiko Nakamura},
	Booktitle = {In proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems},
	Month = {Sept, 22-26},
	Title = {Scaffolding On-line Segmentation of Full Body Human Motion Patterns},
	Year = {2008}}

@inproceedings{LinAndKulic2011,
	Address = {San Francisco, California},
	Author = {Lin, J. F-S.  and Kuli\'c, D.},
	Booktitle = {In Robotics for Neurology and Rehabilitation, Workshop at the IEEE/RSJ International Conference on Intelligent Robots and Systems},
	Title = {Automatic Human Motion Segmentation and Identification using Feature Guided HMM for Physical Rehabilitation Exercises},
	Year = {2011}}

@article{DobigeonAndAll2007,
	Author = {Nicolas Dobigeon and Jean-Yves Tourneret and Jeffrey D. Scargle},
	Journal = {IEEE Transactions on Signal Processing},
	Number = 2,
	Pages = {414--423},
	Title = {Joint segmentation of multivariate astronomical time series : Bayesian sampling with a hierarchical model},
	Volume = 55,
	Year = {2007}}
 

@article{DobigeonAndTourneret2010,
	Author = {Nicolas Dobigeon and Jean-Yves Tourneret},
	Journal = {IEEE Transactions on Signal Processing},
	Number = 5,
	Pages = {2675--2685},
	Title = {Bayesian orthogonal component analysis for sparse representation},
	Volume = 58,
	Year = {2010}}
@article{Bouten,
	Author = {Bouten, C.V. and Koekkoek, K.T. and Verduin, M. and Kodde, R. and Janssen, J.D.},
	Journal = {IEEE Transactions on Biomedical Engineering},
	Number = {3},
	Pages = {136-47},
	Title = {A triaxial accelerometer and portable data processing unit for the assessment of daily physical activity},
	Volume = {44},
	Year = {1997}}

@article{Jovanov05,
	Author = {Jovanov, Emil and Milenkovic, Aleksandar and Otto, Chris and de Groen, Piet},
	Journal = {Journal of NeuroEngineering and Rehabilitation},
	Month = mar,
	Number = {1},
	Pages = {6+},
	Title = {{A wireless body area network of intelligent motion sensors for computer assisted physical rehabilitation}},
	Volume = {2},
	Year = {2005}}

@article{JuhaEtall06,
	Author = {P{a}rkk{a}, Juha and Ermes, Miikka and Korpip{a}{a}, Panu and M{a}ntyj{a}rvi, Jani and Peltola, Johannes and Korhonen, Ilkka},
	Journal = {IEEE Transactions on Information Technology in Biomedicine},
	Number = 1,
	Pages = {119-128},
	Title = {Activity Classification Using Realistic Data From Wearable Sensors.},
	Volume = 10,
	Year = 2006}

@article{Aminian1999,
	Author = {Aminian, K. and Robert, Ph and Buchser, E. and Rutschmann, B. and Hayoz, D. and Depairon, M.},
	Day = {1},
	Journal = {Medical and Biological Engineering and Computing},
	Keywords = {accelerometer, activity\_recognition, validity, video},
	Month = may,
	Number = {3},
	Pages = {304--308},
	Title = {{Physical activity monitoring based on accelerometry: validation and comparison with video observation}},
	Volume = {37},
	Year = {1999}}

@inproceedings{Jonathan-W-IROS11,
	Author = {Jonathan Feng-Shun Lin and Dana Kuli{\'c}},
	Booktitle = {Robotics for Neurology and Rehabilitation, Workshop at IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2011},
	Month = {september},
	Title = {Automatic Human Motion Segmentation and Identification using Feature Guided HMM for Physical Rehabilitation Exercises},
	Year = {2011}}

@article{wang2003,
	Author = {Liang, Wang and Hu, Weiming and Tan, Tieniu},
	Journal = {Pattern Recognition},
	Number = 3,
	Pages = {585-601},
	Title = {Recent developments in human motion analysis.},
	Volume = 36,
	Year = 2003}

@article{Tan2005,
	Author = {Tan, Chin-Woo and Park, Sungsu},
	Journal = {IEEE T. Instrumentation and Measurement},
	Number = 6,
	Pages = {2520-2530},
	Title = {Design of accelerometer-based inertial navigation systems.},
	Volume = 54,
	Year = 2005}

@article{Lindemann2005,
	Author = {Lindemann, U. and Hock, A. and Stuber, M. and Keck, W. and Becker, C.},
	Journal = {Med. Biol. Engineering and Computing},
	Number = 5,
	Pages = {548-551},
	Title = {Evaluation of a fall detector based on accelerometers: A pilot study.},
	Volume = 43,
	Year = 2005}

@article{Kangas2008,
	Author = {Kangas, M. and Konttila, A. and Lindgren, P. and Winblad, I. and Jamsa, T.},
	Issn = {09666362},
	Journal = {Gait \& Posture},
	Keywords = {accelerometer, fall-detection, sensor},
	Month = aug,
	Number = {2},
	Pages = {285--291},
	Title = {{Comparison of low-complexity fall detection algorithms for body attached accelerometers}},
	Volume = {28},
	Year = {2008}}

@article{Scanaill2006,
	Author = {Scanaill, Cliodhna and Carew, Sheila and Barralon, Pierre and Noury, Norbert and Lyons, Declan and Lyons, Gerard},
	Day = {11},
	Journal = {Annals of Biomedical Engineering},
	Keywords = {lit-review, mobility-monitoring},
	Month = apr,
	Number = {4},
	Pages = {547--563},
	Publisher = {Springer},
	Title = {{A Review of Approaches to Mobility Telemonitoring of the Elderly in Their Living Environment}},
	Volume = {34},
	Year = {2006}}

@inproceedings{trabelsi_wIROS,
	Author = {D. Trabelsi and S. Mohammed and F. Chamroukhi and L. Oukhellou and Y. Amirat},
	Booktitle = {New and Emerging Technologies in Assistive Robotics, Workshop at the IEEE/RSJ International Conference on Intelligent Robots and Systems, San Francisco, California},
	Month = {september},
	Title = {Activity Recognition Using Hidden Markov Models},
	Year = {2011}}

@article{Raudys,
	Author = {S. Raudys},
	Journal = {Neural Networks},
	Pages = {17-19},
	Title = {How good are support vector machines?},
	Volume = {13},
	Year = {2000}}

@book{joachim_book,
	Author = {T. Joachims},
	Publisher = {Kluwer/Springer},
	Title = {Learning to Classify Text Using Support Vector Machines -- Methods, Theory, and Algorithms},
	Year = 2002}

@article{Duan,
	Author = {K. Duan and S.S. Keerthi and A.N. Poo},
	Journal = {Neurocomputing},
	Pages = {41-59},
	Title = {Evaluation of simple performance measures for tuning svm hyperparameters},
	Volume = {51},
	Year = {2003}}

@article{Bicocchi,
	Author = {N. Bicocchiand M. Mamei, F. Zambonelli},
	Journal = {Pervasive and Mobile Computing},
	Pages = {482-495},
	Title = {Detecting activities from body worn accelerometers via instance-based algorithms},
	Year = {2010}}

@article{Aminian,
	Author = {K. Aminian and P. Robert and E.E. Buchserand B. Rutschmann and D. Hayoz and M. Depairon},
	Journal = {Medical & Biological Engineering & Computing},
	Pages = {304308},
	Title = {Physical activity monitoring based on accelerometry: Validation and comparison with video observation},
	Volume = {37},
	Year = {1999}}

@article{Oscar,
	Author = {D. Lara. \'Oscar and J. P\'erez. Alfredo and Miguel A. Labradorand Posada. Jos\'e D.},
	Journal = {Pervasive and Mobile Computing},
	Title = {Centinela: A human activity recognition system based on acceleration and vital sign data},
	Year = {2011}}

@article{Yang,
	Author = {C. C. Yang and Y. L. Hsu},
	Journal = {Sensors},
	Pages = {7772-7788},
	Title = {A Review of Accelerometry-Based Wearable Motion Detectors for Physical Activity Monitoring},
	Volume = {10},
	Year = {2010}}

@article{CappozzoA,
	Author = {A. Cappozzo and F. Catani and A. Leardini and M.G Benedetti and .U.D. Croce},
	Issue = {2},
	Journal = {Clinical Biomechanics},
	Pages = {90-100},
	Title = {Position and orientation in space of bones during movement: experimental artefacts},
	Volume = {11},
	Year = {1996}}

@article{CappozzoB,
	Author = {A. Cappozzo and U.D.Croce and A. Leardini and L. Chiari},
	Issue = {2},
	Journal = {Gait \& Posture},
	Pages = {186-196},
	Title = {Human movement analysis using stereophotogrammetry: Part 1: theoretical background},
	Volume = {21},
	Year = {2005}}

@article{MacKenzie,
	Author = {D. MacKenzie},
	Journal = {MIT Press},
	Title = {Inventing accuracy A Historical sociology of nuclear missile guidance},
	Year = {1991}}

@inproceedings{Kozina,
	Adress = {Barcelona, Spain},
	Author = {S. Kozina and M. Lustrek and M. Gams},
	Booktitle = {STAMI (Space, Time and Ambient Intelligence), workshop at 22nd International Joint Conference on artificial Intelligence (IJCAI), Barcelona},
	Pages = {93-98},
	Title = {Dynamic signal segmentation for activity recognition},
	Year = {2011}}

@article{Carminati,
	Author = {M. Carminati and G. Ferrari and M. Sampietro and R. Grassetti},
	Issue = {1},
	Journal = {MIT Press},
	Pages = {509-512.},
	Title = {Fault detection and isolation enhancement of an aircraft attitude and heading reference system based on MEMS inertial sensor},
	Volume = {Procedia Chemistry 1},
	Year = {2009}}

@article{Noury,
	Author = {N. Noury and P. Barralon and G. Virone and P. Boissy and M. Hamel and P. Rumeau},
	Journal = {Engineering in Medicine and Biology Society},
	Pages = {3286-3289},
	Title = {A smart sensor based on rules and its evaluation in daily routines},
	Volume = {Procedia Chemistry 1},
	Year = {2009}}

@article{Barralon,
	Author = {P. Barralon and N.Vuillerme and N. Noury},
	Issue = {27},
	Journal = {Engineering in Medicine and Biology Society},
	Month = {4},
	Pages = {182-193},
	Title = {Classification of the activities of a person in independent living situations from accelerometers},
	Volume = {Elsevier},
	Year = {2006}}

@article{Trivino,
	Author = {G. Trivino and A.A. Alvarez and G. Bailador},
	Issue = {27},
	Journal = {Engineering in Medicine and Biology Society},
	Month = {4},
	Pages = {182-193},
	Title = {Classification of the activities of a person in independent living situations from accelerometers},
	Volume = {Elsevier},
	Year = {2006}}

@article{Ravi,
	Author = {N. Ravi and N. Dandekar and P. Mysore and M. Littman},
	Journal = {Proceedings of the National Conference on Artificial Intelligence, MIT Press},
	Pages = {1541-1546},
	Title = {Activity Recognition from Accelerometer Data},
	Year = {2005}}

@article{Vaganay,
	Author = {J. Vaganay and M.J. Aldon},
	Issue = {2},
	Journal = {Control Engineering Practice},
	Pages = {281-287},
	Title = {Attitude estimation for a vehicle using inertial sensors},
	Volume = {2},
	Year = {2005}}

@article{Dong,
	Author = {L. Dong and L. Che and L. Sun and Y. Wang},
	Issue = {2},
	Journal = {Sensors and Actuators A: Physical},
	Pages = {395-404},
	Title = {Effects of non-parallel combs on reliable operation conditions of capacitive inertial sensor for step and shock signals},
	Volume = {121},
	Year = {2005}}

@article{Faber,
	Author = {G.S. Faber and I. Kingma and S.M. Bruijn and J.H. Van Dieen},
	Issue = {2},
	Journal = {Sensors and Actuators A: Physical},
	Pages = {395-404},
	Title = {Effects of non-parallel combs on reliable operation conditions of capacitive inertial sensor for step and shock signals},
	Volume = {121},
	Year = {2005}}

@article{Yang08,
	Author = {Yang, Jhun Y. and Wang, Jeen S. and Chen, Yen P.},
	Journal = {Pattern Recognition Letters},
	Keywords = {accelerometer, activity\_recognition, behavior, machine\_learning},
	Number = {16},
	Pages = {2213--2220},
	Title = {{Using acceleration measurements for activity recognition: An effective learning algorithm for constructing neural classifiers}},
	Volume = {29},
	Year = {2008}}

@article{Qian,
	Author = {H. Qian and Y. Mao and W. Xiang and Z. Wang},
	Issue = {2},
	Journal = {Pattern Recognition Letters},
	Pages = {100-111},
	Title = {Recognition of human activities using SVM multi-class classifier},
	Volume = {31},
	Year = {2010}}

@article{Liu,
	Author = {C.L. Liu and C.H. Lee and P.M. Lin},
	Issue = {10},
	Journal = {Expert Systems with Applications},
	Pages = {7174-7181},
	Title = {A fall detection system using k-nearest neighbor classifier},
	Volume = {37},
	Year = {2010}}

@article{Jiayang,
	Author = {L. Jiayang and Z. Lin and J. Wickramasuriya and V. Vasudevan},
	Journal = {Pervasive and Mobile Computing},
	Pages = {657-675},
	Title = {uWave: Accelerometer-based personalized gesture recognition and its applications},
	Year = {2009}}

@article{Wassink,
	Author = {R.G.V. Wassink and C.T.M. Baten and P.H. Veltink and R.N.J. Veldhuis and J.H. Smeding},
	Journal = {Gait \& Posture},
	Pages = {109-110},
	Title = {Monitoring of human activities using a trainable system based on Hidden Markov modelling technology},
	Volume = {24},
	Year = {2006}}

@article{CoverEtHart,
	Author = {T. Cover and P. Hart},
	Journal = {IEEE Transactions on Information Theory},
	Pages = {21-27},
	Title = {Nearest Neighbour Pattern Classification},
	Volume = {13},
	Year = {1967}}

@article{cover,
	Author = {Cover65, T.M.},
	Journal = {IEEE Transactions on Electronic Computers},
	Pages = {326-334},
	Title = {Geometrical and Statistical properties of systems of linear inequalities with applications in pattern recognition},
	Volume = {14},
	Year = {1965}}

@article{Hilden,
	Author = {J. Hilden},
	Issue = {4},
	Journal = {Computers in Biology and Medicine},
	Pages = {429-435},
	Title = {Statistical diagnosis based on conditional independence does not require it. Computers in Biology and Medicine},
	Volume = {14},
	Year = {1984}}

@article{Rumelhart,
	Author = {D. E. Rumelhart and J. L. McClelland.},
	Journal = {Computers in Biology and Medicine},
	Title = {Parallel distributed processing explorations in the microstructure of cognition. Cambridge: Bradford Books},
	Year = {1986}}

@article{Hill,
	Author = {T. Hill and P. Lewicki},
	Journal = {Statistics Methods and Applications. StatSoft},
	Title = {Parallel distributed processing explorations in the microstructure of cognition. Cambridge: Bradford Books},
	Year = {2007}}

@article{Raudys,
	Author = {S. Raudys},
	Journal = {Neural Networks},
	Pages = {17-19},
	Title = {The Nature of Statistical Learning Theory},
	Volume = {13},
	Year = {2000}}

@book{Brent1973,
	Author = {Brent, Richard P.},
	Isbn = {0-13-022335-2},
	Publisher = {Englewood Cliffs, N.J. Prentice-Hall},
	Series = {Prentice-Hall series in automatic computation},
	Title = {Algorithms for minimization without derivatives},
	Year = 1973}

@inproceedings{Waterhouse96bayesianMoE,
	Author = {Steve Waterhouse and David Mackay and Tony Robinson},
	Booktitle = {NIPS},
	Date-Modified = {2015-08-21 22:49:28 +0000},
	Pages = {351--357},
	Publisher = {MIT Press},
	Title = {Bayesian Methods for Mixtures of Experts},
	Year = {1996}}

@inproceedings{Rasmussen01infiniteMoE,
	Author = {Carl Edward Rasmussen and Zoubin Ghahramani},
	Booktitle = {In Advances in Neural Information Processing Systems 14},
	Pages = {881--888},
	Publisher = {MIT Press},
	Title = {Infinite Mixtures of Gaussian Process Experts},
	Year = {2001}}

@misc{TemperatureAnomalyData,
	Annote = {Trinnovim {LLC}, {C}olumbia {U}niversity},
	Author = {R. Ruedy and M. Sato and K. Lo},
	Howpublished = {DOI: 10.3334/CDIAC/cli.001},
	Note = {Center for Climate Systems Research, {NASA} Goddard Institute for Space Studies 2880 Broadway, {N}ew {Y}ork, {NY} 10025 {USA}},
	Title = {{NASA GISS} Surface Temperature ({GISTEMP}) Analysis}}

@article{Hansen1999,
	Author = {Hansen, J. and Ruedy, R. and Glascoe, J. and Sato, M.},
	Journal = {Journal of Geophysical Research},
	Pages = {30997--31022},
	Title = {GISS analysis of surface temperature change},
	Volume = 104,
	Year = 1999}

@article{Hansen2001,
	Author = {Hansen, J. and Ruedy, R. and Sato M. and Imhoff, M. and Lawrence, W. and Easterling, D. and Peterson, T. and Karl, T.},
	Journal = {Journal of Geophysical Research},
	Pages = {23947--23963},
	Title = {A closer look at United States and global surface temperature change},
	Volume = 106,
	Year = 2001}

@article{NgM-EM-IRLS-04,
	Author = {Ng, Shu-Kay and McLachlan, Geoffrey J.},
	Journal = {IEEE Transactions on Neural Networks},
	Number = 3,
	Pages = {738--749},
	Title = {Using the EM algorithm to train neural networks: misconceptions and a new algorithm for multiclass classification.},
	Volume = 15,
	Year = 2004}

@inproceedings{Bishop_BayesianMoE,
	Author = {C. Bishop and M. Svens\'en},
	Booktitle = {In Uncertainty in Artificial Intelligence},
	Title = {Bayesian hierarchical mixtures of experts},
	Year = {2003}}

@article{Henze1986,
	Author = {Norbert Henze},
	Journal = {Scandinavian Journal of Statistics},
	Optnumber = {4},
	Optvolume = {13},
	Pages = {271--275},
	Title = {A Probabilistic Representation of the Skew-Normal Distribution},
	Year = {1986}}

@article{Azzalini1985,
	Author = {A. Azzalini},
	Journal = {Scandinavian Journal of Statistics},
	Optvolume = {12},
	Pages = {171--178},
	Title = {A class of distributions which includes the normal ones},
	Year = {1985}}

@article{Azzalini1986,
	Author = {A. Azzalini},
	Journal = {Scandinavian Journal of Statistics},
	Optvolume = {46},
	Pages = {199--208},
	Title = {Further results on a class of distributions which includes the normal ones},
	Year = {1986}}

@article{Cohen1984,
	Author = {Elizabeth A. Cohen},
	Journal = {Music Perception},
	Optpages = {323--349},
	Title = {Some effects of inharmonic partials on interval perception},
	Volume = {1},
	Year = {1984}}

@article{Song2014,
	Abstract = {Abstract A robust estimation procedure for mixture linear regression models is proposed by assuming that the error terms follow a Laplace distribution. Using the fact that the Laplace distribution can be written as a scale mixture of a normal and a latent distribution, this procedure is implemented by an \{EM\} algorithm which incorporates two types of missing information from the mixture class membership and the latent variable. Finite sample performance of the proposed algorithm is evaluated by simulations. The proposed method is compared with other procedures, and a sensitivity study is also conducted based on a real data set. },
	Author = {Weixing Song and Weixin Yao and Yanru Xing},
	Journal = {Computational Statistics \& Data Analysis},
	Keywords = {Laplace distribution},
	Number = {0},
	Pages = {128 - 137},
	Title = {Robust mixture regression model fitting by Laplace distribution},
	Volume = {71},
	Year = {2014}}

@techreport{Wei2012,
	Author = {Wei, Y.},
	Institution = {Master Report, Department of Statistics, Kansas State University},
	Title = {Robust mixture regression models using t-distribution},
	Year = {2012}}

@article{Bai2012,
	Abstract = {The existing methods for fitting mixture regression models assume a normal distribution for error and then estimate the regression parameters by the maximum likelihood estimate (MLE). In this article, we demonstrate that the MLE, like the least squares estimate, is sensitive to outliers and heavy-tailed error distributions. We propose a robust estimation procedure and an EM-type algorithm to estimate the mixture regression models. Using a Monte Carlo simulation study, we demonstrate that the proposed new estimation method is robust and works much better than the \{MLE\} when there are outliers or the error distribution has heavy tails. In addition, the proposed robust method works comparably to the \{MLE\} when there are no outliers and the error is normal. A real data application is used to illustrate the success of the proposed robust estimation procedure. },
	Author = {Xiuqin Bai and Weixin Yao and John E. Boyer},
	Journal = {Computational Statistics \& Data Analysis},
	Keywords = {Robust regression},
	Number = {7},
	Pages = {2347 - 2359},
	Title = {Robust fitting of mixture regression models},
	Volume = {56},
	Year = {2012}}

@article{YukselWG12,
	Author = {Yuksel, Seniha Esen and Wilson, Joseph N. and Gader, Paul D.},
	Ee = {http://dx.doi.org/10.1109/TNNLS.2012.2200299},
	Journal = {IEEE Trans. Neural Netw. Learning Syst.},
	Number = 8,
	Pages = {1177-1193},
	Title = {Twenty Years of Mixture of Experts.},
	Volume = 23,
	Year = 2012}

@article{Pyne2009,
	Author = {Pyne, S and Hu, X and Wang, K and Rossin, E and Lin, TI and Maier, LM and Baecher-Allan, C and McLachlan, GJ and Tamayo, P and Hafler, DA and De Jager, PL and Mesirow, JP},
	volume = {106},
	number = {21},
	pages= {8519-8524},
	Journal = {Proceedings of the National Academy of Sciences USA},
	Title = {Automated high-dimensional flow cytometric data analysis},
	Year = {2009}}

@article{HuangElAll2015,
	Author = {Chao Huang and Martin Styner and Hongtu Zhu},
	Doi = {http://dx.doi.org/10.1080/01621459.2015.1034802},
	Journal = {Journal of the American Statistical Association},
	Month = {April},
	Title = {Clustering High-Dimensional Landmark-based Two-dimensional Shape Data},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1080/01621459.2015.1034802}}

@article{Genton2001,
	Author = {Marc G. Genton and Li He and Xiangwei Liu},
	Journal = {Statistics \& Probability Letters},
	Pages = {319--325},
	Title = {Moments of skew-normal random vectors and their quadratic forms},
	Volume = {51},
	Year = {2001}}

@article{AzzaliniAndCapitanio2003,
	Author = {A. Azzalini and A. Capitanio},
	Journal = {Journal of the Royal Statistical Society, Series B},
	Pages = {367--389},
	Title = {Distributions generated by perturbation of symmetry with emphasis on a multivariate skew {\it t} distribution},
	Volume = {65},
	Year = {2003}}

@article{Lin2010SkewtMvMixture,
	Author = {Tsung I. Lin},
	Journal = {Statistics and Computing},
	Number = {3},
	Pages = {343--356},
	Title = {Robust mixture modeling using multivariate skew {\it t} distributions},
	Volume = {20},
	Year = {2010}}

@article{Lin07univSkewtMixture,
	Author = {Tsung I. Lin and Jack C. Lee and Wan J. Hsieh},
	Journal = {Statistics and Computing},
	Number = {2},
	Pages = {81--92},
	Title = {Robust mixture modeling using the skew \emph{t} distribution},
	Volume = {17},
	Year = {2007}}

@article{Lin07univSkewNMixture,
	Author = {Tsung I. Lin and Jack C. Lee and Shu Y Yen},
	Journal = {Statistica Sinica},
	Pages = {909--927},
	Title = {Finite mixture modelling using the skew normal distribution},
	Volume = {17},
	Year = {2007}}

@article{Zeller15SkewNMixReg,
	Author = {Zeller, C. B. and Lachos, V. H. and Cabral, C.R.},
	Journal = {Test (revision invited)},
	Title = {Robust Mixture Regression Modelling based on Scale Mixtures of Skew-Normal Distributions},
	Year = {2015}}

@article{Fruhwirth10BayesSkewMixtures,
	Author = {Fr{\"u}hwirth-Schnatter, S. and Pyne, S.},
	Journal = {Biostatistics},
	Number = {2},
	Pages = {317--336},
	Title = {Bayesian inference for finite mixtures of univariate and multivariate skew-normal and skew-t distributions},
	Volume = {11},
	Year = {2010}}

@article{LeeAndMcLachlan15-CFUST,
	Author = {Sharon X. Lee and Geoffrey J. McLachlan},
	Doi = {10.1007/s11222-015-9545-x},
	Journal = {Statistics and Computing (To appear)},
	Title = {Finite mixtures of canonical fundamental skew {\it t}-distributions},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s11222-015-9545-x}}

@article{LeeAndMcLachlan14-skewtmix,
	Author = {Sharon X. Lee and Geoffrey J. McLachlan},
	Journal = {Statistics and Computing},
	Number = {2},
	Pages = {181--202},
	Title = {Finite mixtures of multivariate skew t-distributions: some recent and new results},
	Volume = {24},
	Year = {2014}}

@article{LeeAndMcLachlan13skew,
	Author = {Sharon X. Lee and Geoffrey J. McLachlan},
	Journal = {Advances in Data Analysis and Classification},
	Number = {3},
	Pages = {241--266},
	Title = {On mixtures of skew normal and skew {\it t}-distributions},
	Volume = {7},
	Year = {2013}}

@article{LeeAndMchLachlan13non-normal-mix,
	Author = {Sharon X. Lee and Geoffrey J. McLachlan},
	Journal = {Statistical Methods and Applications},
	Number = {4},
	Pages = {427--454},
	Title = {Model-based clustering and classification with non-normal mixture distributions},
	Volume = {22},
	Year = {2013}}

@inproceedings{Mclachlan98robustTmixture,
	Author = {Geoffrey J. Mclachlan and David Peel},
	Booktitle = {Lecture Notes in Computer Science},
	Pages = {658--666},
	Publisher = {Springer-Verlag},
	Title = {Robust Cluster Analysis Via Mixtures Of Multivariate t-Distributions},
	Year = {1998}}

@article{Kent1994multivariateT,
	Author = {Kent, J.T. and Tyler, D.E. and Vardi, Y},
	Issue = {2},
	Journal = {Communications in Statistics - Simulation and Computation},
	Pages = {441--453},
	Title = {A curious likelihood identity for the multivariate t-distribution},
	Volume = {23},
	Year = {1994}}

@article{Peel2000robusTtmixture,
	Author = {D. Peel and G. J. Mclachlan},
	Journal = {Statistics and Computing},
	Pages = {339--348},
	Title = {Robust mixture modelling using the t distribution},
	Volume = {10},
	Year = {2000}}

@article{HunterANDYoung,
	Author = {Hunter, DR and Young, DS},
	Journal = {Journal of Nonparametric Statistics},
	Number = {1},
	Pages = {19-38},
	Title = {Semiparametric Mixtures of Regressions},
	Volume = {24},
	Year = {2012}}

@book{RobertBayesianChoiceBook2007,
	Author = {Robert, Christian P.},
	Date-Modified = {2015-08-21 22:38:01 +0000},
	Edition = {Second},
	Publisher = {Springer-Verlag},
	Title = {{T}he {B}ayesian {C}hoice: From {D}ecision-{T}heoretic {F}oundations to {C}omputational {I}mplementation},
	Year = {2007}}

@article{RichardsonANDGreen97,
	Author = {Sylvia Richardson and Peter J. Green},
	Journal = {Journal of the Royal Statistical Society},
	Number = {4},
	Pages = {731-792},
	Series = {B (Methodological)},
	Title = {{O}n {B}ayesian {A}nalysis of {M}ixtures with an {U}nknown {N}umber of {C}omponents},
	Volume = {59},
	Year = {1997}}

@article{EscobarANDWest-95-BayesianMixtures,
	Author = {Michael D. Escobar and Mike West},
	Date-Modified = {2015-08-21 21:35:26 +0000},
	Journal = {Journal of the American Statistical Association},
	Number = {430},
	Pages = {577--588},
	Title = {{B}ayesian {D}ensity {E}stimation and {I}nference {U}sing {M}ixtures},
	Volume = {90},
	Year = {1994}}

@article{DieboltAndRobert1994,
	Author = {Diebolt, Jean and Robert, C. P.},
	Journal = {Journal of the Royal Statistical Society, Series B},
	Number = {2},
	Pages = {363-375},
	Title = {{Estimation of Finite Mixture Distributions through Bayesian Sampling}},
	Volume = {56},
	Year = {1994}}

@article{CeleuxJASA2000,
	Author = {G. Celeux and M. Hurn and C. P. Robert},
	Date-Modified = {2015-08-21 21:25:24 +0000},
	Journal = {Journal of the American Statistical Association},
	Number = {451},
	Pages = {957--970},
	Title = {Computational and Inferential Difficulties with Mixture Posterior Distributions},
	Volume = {95},
	Year = {2000}}

@inproceedings{rafterygibbs,
	Author = {Adrian E. Raftery and Steven Lewis},
	Booktitle = {In Bayesian Statistics 4},
	Pages = {763--773},
	Publisher = {Oxford University Press},
	Title = {{How Many Iterations in the Gibbs Sampler?}},
	Year = {1992}}

@book{SylviaFruhwirthBook2006,
	Address = {New York},
	Author = {Fr\"{u}hwirth-Schnatter, S.},
	Publisher = {Springer Verlag},
	Title = {Finite Mixture and Markov Switching Models (Springer Series in Statistics)},
	Year = {2006}}

@book{ShiGPR_Book2011,
	Author = {J. Q. Shi and T. Choi},
	Publisher = {Chapman \& Hall/CRC Press},
	Title = {Gaussian Process Regression Analysis for Functional Data},
	Address={Boca Raton},
	Year = {2011}}

@article{Shi_etal_GPR,
	Author = {J.Q. Shi and B. Wang and R. Murray-Smith and D.M. Titterington},
	Journal = {Biometrics},
	Pages = {714-723},
	Title = {Gaussian process functional regression modeling for batch data},
	Volume = {63},
	Year = {2007}}

@book{RassmussenAndwillams_2006,
	Author = {C. E. Rasmussen and C. K. I. Williams},
	Publisher = {Cambridge, MA: The MIT Press},
	Title = {Gaussian Processes for Machine Learning},
	Year = {2006}}

@book{RobertAndCasella1999,
 author = {Robert, Christian P. and Casella, George},
 title = {Monte Carlo Statistical Methods (Springer Texts in Statistics)},
 year = {1999},
 isbn = {1441919392},
 publisher = {Springer-Verlag New York, Inc.},
 address = {Secaucus, NJ, USA}
} 

@article{Robert2011,
	Author = {Robert, C. and Casella, G.},
	Journal = {Statistical Science},
	Month = {Feb},
	Number = {1},
	Pages = {102--115},
	Publisher = {Institute of Mathematical Statistics},
	Title = {A Short History of {Markov} Chain {Monte} {Carlo}: Subjective Recollections from Incomplete Data},
	Volume = {26},
	Year = {2011}}

@article{CeleuxMLMM2005,
	Author = {G. Celeux and O. Martin and C. Lavergne},
	Journal = {Statistical Modelling},
	Pages = {1-25},
	Title = {Mixture of linear mixed models for clustering gene expression profiles from repeated microarray experiments},
	Volume = {5},
	Year = {2005}}

@article{Bouveyron2011,
	Author = {Bouveyron, Charles and Jacques, Julien},
	Journal = {Adv. Data Analysis and Classification},
	Number = 4,
	Pages = {281-300},
	Title = {Model-based clustering of time series in group-specific functional subspaces.},
	Volume = 5,
	Year = 2011}

@article{Jacques2014,
	Author = {Julien Jacques and Cristian Preda},
	Journal = {Computational Statistics \& Data Analysis},
	Pages = {92-106},
	Title = {Model-based clustering for multivariate functional data},
	Volume = {71},
	Year = {2014}}

@article{LiuAndRubin95,
	Author = {Liu, Chuanhai and Rubin, Donald B.},
	Citeulike-Article-Id = {4456677},
	Journal = {Statistica Sinica},
	Keywords = {algorithm, parameter-estimation},
	Pages = {19--39},
	Posted-At = {2009-05-04 01:39:49},
	Priority = {0},
	Title = {{ML} estimation of the t distribution using {EM} and its extensions, {ECM} and {ECME}},
	Volume = {5},
	Year = {1995}}

@article{YoungANDHunter,
	Author = {Young, DS and Hunter, DR},
	Journal = {Computational Statistics and Data Analysis},
	Number = {10},
	Pages = {2253-2266},
	Title = {Mixtures of Regressions with Predictor-Dependent Mixing Proportions},
	Volume = {55},
	Year = {2010}}

@article{KooperbergANDStone1991,
	Author = {Kooperberg, Charles and Stone, Charles J.},
	Journal = {Computational Statistics \& Data Analysis},
	Number = {3},
	Pages = {327-347},
	Title = {A study of logspline density estimation},
	Volume = {12},
	Year = {1991}}

@article{YeungMBC2001,
	Author = {Ka Yee Yeung and Chris Fraley and A. Murua and Adrian E. Raftery and Walter L. Ruzzo},
	Journal = {Bioinformatics},
	Number = {10},
	Pages = {977--987},
	Title = {Model-based clustering and data transformations for gene expression data},
	Volume = {17},
	Year = {2001}}

@article{Hastie95penalizeddiscriminant,
	Author = {Trevor Hastie and Andreas Buja and Robert Tibshirani},
	Journal = {Annals of Statistics},
	Pages = {73--102},
	Title = {{Penalized Discriminant Analysis}},
	Volume = {23},
	Year = {1995}}

@article{QuandtANDRamsey1978,
	Author = {R. E. Quandt and J. B. Ramsey},
	Journal = {Journal of the American Statistical Association},
	Number = {364},
	Pages = {730--738},
	Title = {Esimating Mixtures of Normal Distributions and Switching Regressions},
	Volume = {73},
	Year = {1978}}


	

@article{XiongY04,
	Author = {Xiong, Yimin and Yeung, Dit-Yan},
	Journal = {Pattern Recognition},
	Number = 8,
	Pages = {1675-1689},
	Title = {Time series clustering with {ARMA} mixtures.},
	Volume = 37,
	Year = 2004}}

@article{Wu-convergence-EM,
	Abstract = {Two convergence aspects of the EM algorithm are studied: (i) does the EM algorithm find a local maximum or a stationary value of the (incomplete-data) likelihood function? (ii) does the sequence of parameter estimates generated by EM converge? Several convergence results are obtained under conditions that are applicable to many practical situations. Two useful special cases are: (a) if the unobserved complete-data specification can be described by a curved exponential family with compact parameter space, all the limit points of any EM sequence are stationary points of the likelihood function; (b) if the likelihood function is unimodal and a certain differentiability condition is satisfied, then any EM sequence converges to the unique maximum likelihood estimate. A list of key properties of the algorithm is included.},
	Author = {Wu, C. F. Jeff},
	Journal = {The Annals of Statistics},
	Keywords = {em-algorithm, learning},
	Number = {1},
	Pages = {95--103},
	Publisher = {Institute of Mathematical Statistics},
	Title = {{On the Convergence Properties of the EM Algorithm}},
	Volume = {11},
	Year = {1983}}

@article{makarenkov,
	Author = {Makarenkov, V. and Legendre, P},
	Journal = {Math\'ematiques, informatique et sciences humaines},
	Title = {Une m\'ethode d'analyse canonique non-lin\'eaire et son application \`a des donn\'ees biologiques},
	Year = {1999}}

@book{mitchell-book-1997,
	Address = {New York},
	Author = {Mitchell, T. M.},
	Keywords = {machine learning},
	Publisher = {McGraw-Hill},
	Title = {Machine Learning},
	Year = {1997}}

@inproceedings{MacQueen-Kmeans,
	Author = {MacQueen, J. B.},
	Booktitle = {Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability},
	Pages = {281--297},
	Title = {Some methods for classification and analysis of multivariate observations},
	Year = {1967}}

@book{TitteringtonBookMixtures,
	Author = {Titterington, D. and Smith, A. and Makov, U.},
	Publisher = {John Wiley \& Sons},
	Title = {Statistical Analysis of Finite Mixture Distributions},
	Year = {1985}}



@article{hastieANDtibshiraniMDA,
	Author = {Hastie, T. and Tibshirani, R.},
	Journal = {Journal of the Royal Statistical Society, B},
	Pages = {155-176},
	Title = {Discriminant Analysis by Gaussian Mixtures},
	Volume = {58},
	Year = {1996}}

@book{vapnik99,
	Author = {Vapnik, V. N.},
	Citeulike-Article-Id = {115106},
	Howpublished = {Hardcover},
	Keywords = {learning theory},
	Publisher = {Springer},
	Title = {The Nature of Statistical Learning Theory (Information Science and Statistics)},
	Year = {1999}}

@misc{VC,
	Author = {Vapnik, V. N. and Chervonenkis, V.},
	Title = {Teoriya Raspoznavaniya Obrazov: Statisticheskie Problemy Obucheniya (Theory of Pattern Recognition: Statistical Problems of Learning)},
	Year = {1974}}

@book{vapnik98,
	Author = {Vapnik, V. N.},
	Howpublished = {Hardcover},
	Keywords = {learning theory},
	Publisher = {John Wiley \& Sons},
	Title = {Statistical Learning Theory},
	Year = {1998}}

@article{RossiAndVillaSVM-functional-data-Neurocomputing-2006,
	Author = {Rossi, F. and Villa, N.},
	Journal = {Neurocomputing},
	Month = {March},
	Number = {7--9},
	Pages = {730--742},
	Title = {Support Vector Machine For Functional Data Classification},
	Volume = {69},
	Year = {2006}}

@article{RossiConanGuez05NeuralNetworks,
	Author = {Rossi, F. and Conan-Guez, B.},
	Journal = {Neural Networks},
	Month = {January},
	Number = {1},
	Pages = {45--60},
	Title = {Functional Multi-Layer Perceptron: a nonlinear tool for functional data analysis},
	Volume = {18},
	Year = {2005}}

@book{Tomassone-etal,
	Address = {Paris},
	Author = {R. Tomassone and E. Lesquoy and C. Millier},
	Language = {\Fr},
	Publisher = {Masson},
	Title = {La R{\'e}gression nouveaux regards sur une ancienne m{\'e}thode statistique},
	Year = {1992}}

@article{McGee,
	Author = {McGee, V. E. and Carleton, W. T.},
	Journal = {Journal of the American Statistical Association},
	Pages = {1109-1124},
	Title = {Piecewise regression},
	Volume = {65},
	Year = {1970}}

@techreport{Fridman,
	Author = {Fridman, M.},
	Institution = {Institute of mathematics, University of Minnesota},
	Title = {Hidden Markov Model Regression},
	Year = {1993}}

@article{Stone1961,
	Author = {Stone, H.},
	Journal = {Mathematics of Computation},
	Number = {73},
	Pages = {40-47},
	Title = {Approximation of curves by line segments},
	Volume = {15},
	Year = {1961}}

@article{jordan-and-xu-1995,
	Author = {Jordan, M. I. and Xu, L.},
	Journal = {Neural Networks},
	Number = {9},
	Pages = {1409-1431},
	Title = {Convergence results for the {EM} approach to mixtures of experts architectures},
	Volume = {8},
	Year = {1995}}

@article{xu-and-jordan-1996,
	Author = {Xu, L. and Jordan, M. I.},
	Journal = {Neural Computation},
	Number = {1},
	Pages = {129-151},
	Title = {On Convergence Properties of the {EM} Algorithm for Gaussian Mixtures},
	Volume = {8},
	Year = {1996}}

@article{celeuxetgovaert92-CEM,
	Author = {Celeux, G. and Govaert, G.},
	Journal = {Computational Statistics and Data Analysis},
	Pages = {315--332},
	Title = {A classification {EM} algorithm for clustering and two stochastic versions},
	Volume = {14},
	Year = {1992}}

@article{celeux-and-soromenho-NEC,
	Author = {Celeux, G.. and Soromenho, Gilda.},
	Journal = {Journal of Classification},
	Month = {septembre},
	Number = {2},
	Pages = {195-212},
	Title = {An entropy criterion for assessing the number of clusters in a mixture model},
	Volume = {13},
	Year = {1996}}

@article{wongANDlimixturelogistic2001,
	Author = {Wong, C.S. and Li, W.K.},
	Journal = {Biometrika},
	Number = {3},
	Pages = {833--846},
	Title = {On a logistic mixture autoregressive model},
	Volume = {88},
	Year = {2001}}

@article{smyth-1994-PR,
	Author = {Smyth, P.},
	Journal = {Pattern recognition},
	Number = {1},
	Pages = {149--164},
	Title = {Hidden Markov models for fault detection in dynamic systems},
	Volume = {27},
	Year = {1994}}

@article{smyth-1994-IEEE,
	Author = {Smyth, P.},
	Issue = {9},
	Journal = {IEEE Journal on Selected Areas in Communications},
	Pages = {1600--1612},
	Title = {Markov monitoring with unknown states},
	Volume = {12},
	Year = {1994}}

@inproceedings{padhraic-smyth-HMM-NN-ieee-sp-93,
	Author = {Smyth, P.},
	Booktitle = {Neural Networks for Signal Processing},
	Journal = {Neural Networks for Signal Processing [1993] III. Proceedings of the 1993 IEEE-SP Workshop},
	Pages = {582--592},
	Title = {Hidden Markov models and neural networks for fault detection in dynamic systems},
	Year = {1993}}

@article{celeux-etal-ARHMM,
	Author = {Celeux, G. and Nascimento, J.C. and Marques, J.S.},
	Journal = {PR},
	Month = {September},
	Number = {9},
	Pages = {1841--1853},
	Title = {Learning switching dynamic models for objects tracking},
	Volume = {37},
	Year = {2004}}

@article{Ng-and-Mclachlan-2006,
	Address = {Essex, UK},
	Author = {Ng, S-K. and McLachlan, G. J. and Lee, A. H.},
	Journal = {Artificial Intelligence in Medicine},
	Number = {3},
	Pages = {257--267},
	Publisher = {Elsevier Science Publishers Ltd.},
	Title = {An incremental EM-based learning approach for on-line prediction of hospital resource utilization},
	Volume = {36},
	Year = {2006}}

@inbook{neal-and-hinton-incremental-EM-1998,
	Author = {Neal, R. and Hinton, G. E.},
	Booktitle = {in M. I. Jordan (editor) Learning in Graphical Models},
	Pages = {355--368},
	Posted-At = {2009-08-18 14:04:16},
	Publisher = {Dordrecht: Kluwer Academic Publishers},
	Title = {A view of the EM algorithm that justifies incremental, sparse, and other variants},
	Year = {1998}}

@article{titterington-1984,
	Author = {Titterington, D. M.},
	Journal = {Journal of the Royal Statistical Society},
	Number = {2},
	Pages = {257--267},
	Serie = {B},
	Title = {Recursive parameter estimation using incomplete data},
	Volume = {46},
	Year = {1984}}

@inproceedings{Meila-et-al-nips96,
	Author = {Meila, M. and Jordan, M. I.},
	Booktitle = {Advances in Neural Information Processing Systems 8},
	Pages = {1003--1009},
	Publisher = {MIT Press},
	Title = {Learning fine motion by Markov mixtures of experts},
	Year = {1996}}

@article{meng-and-rubin-ECM-93,
	Author = {Meng, X. L. and Rubin, D. B.},
	Journal = {Biometrika},
	Keywords = {cem},
	Number = {2},
	Pages = {267--278},
	Title = {Maximum likelihood estimation via the {ECM} algorithm: A general framework},
	Volume = {80},
	Year = {1993}}

@article{north-etal-IEEE-PAMI-2000,
	Address = {Washington, DC, USA},
	Author = {North, B. and Blake, A. and Isard, M. and Rittscher, J.},
	Journal = {IEEE Transactions Pattern Analysis and Machine Intelligence},
	Number = {9},
	Pages = {1016--1034},
	Publisher = {IEEE Computer Society},
	Title = {Learning and Classification of Complex Dynamics},
	Volume = {22},
	Year = {2000}}

@techreport{minka-logreg-2001,
	Author = {T. P. Minka},
	Institution = {Carnegie Mellon University},
	Number = {758},
	Title = {Algorithms for maximum-likelihood logistic regression},
	Year = {2001}}

@article{carvalho-and-tanner2006,
	Author = {A. X. Carvalho and M. A. Tanner},
	Journal = {International Journal of Mathematics and Mathematical Sciences},
	Optnumber = {1--22},
	Optvolume = {9},
	Title = {Modeling nonlinearities with mixtures of experts of time series models},
	Year = {2006}}

@techreport{AIC3,
	Author = {Bozdogan, H.},
	Institution = {Quantitative Methods Department, University of Illinois at Chicago},
	Month = {June},
	Title = {Determining the Number of Component Clusters in the Standard Multi-variate Normal Mixture Model Using Model-Selection Criteria},
	Year = {1983}}

@misc{matrix-cookbook,
	Abstract = {Matrix identities, relations and approximations. A desktop reference for quick overview of mathematics of matrices.},
	Author = {K. B. Petersen and M. S. Pedersen},
	Keywords = {Matrix identity, matrix relations, inverse, matrix derivative},
	Month = {oct},
	Note = {Version 2008},
	Publisher = {Technical University of Denmark},
	Title = {The Matrix Cookbook},
	Url = {http://www2.imm.dtu.dk/pubdb/p.php?3274},
	Year = {2008},
	Bdsk-Url-1 = {http://www2.imm.dtu.dk/pubdb/p.php?3274}}

@phdthesis{Now91,
	Author = {S.J. Nowlan},
	School = {Carnegie Mellon University, Pittsburgh},
	Title = {Soft competitive adaptation: Neural Network Learning Algorithms based on Fitting Statistical Mixtures},
	Year = {1991}}

@article{sato-online-EM2000,
	Author = {Sato, M-A. and Ishii, S.},
	Citeulike-Article-Id = {2587790},
	Citeulike-Linkout-0 = {http://neco.mitpress.org/cgi/content/abstract/12/2/407},
	Citeulike-Linkout-1 = {http://view.ncbi.nlm.nih.gov/pubmed/10636949},
	Citeulike-Linkout-2 = {http://www.hubmed.org/display.cgi?uids=10636949},
	Day = {1},
	Journal = {Neural Computation},
	Keywords = {bayesian, graphical-models, stochastic-approximation},
	Month = {February},
	Number = {2},
	Pages = {407--432},
	Posted-At = {2008-03-25 22:45:45},
	Priority = {2},
	Title = {On-line {EM} Algorithm for the {N}ormalized {G}aussian {N}etwork},
	Volume = {12},
	Year = {2000}}

@inproceedings{chamroukhi-etal-rcm2009,
	Address = {Dublin, UK},
	Author = {Chamroukhi, F. and Sam\'e, A. and Aknin, P.},
	Booktitle = {The VIth International Conference on Condition Monitoring and Machinery Failure Prevention Technologies},
	Title = {A probabilistic approach for the classification of railway switch operating states},
	Year = {2009}}

@inproceedings{chamroukhi-etal-rcm2008,
	Address = {Derby, UK},
	Author = {Chamroukhi, F. and Sam\'e, A. and Aknin, P.},
	Booktitle = {The 4th IET International Conference on Railway Condition Monitoring RCM},
	Issn = {0537-9989},
	Month = {June},
	Pages = {1-4},
	Title = {Switch mechanism diagnosis using a pattern recognition approach},
	Year = {2008}}

@article{Mingoti-comparing-SOM-Kmeans-FKmeans-Hier,
	Author = {Mingoti, Sueli A. and Lima, Joab O.},
	Journal = {European Journal of Operational Research},
	Month = {November},
	Number = {3},
	Pages = {1742-1759},
	Title = {Comparing SOM neural network with Fuzzy c-means, K-means and traditional hierarchical clustering algorithms},
	Volume = {174},
	Year = {2006}}

@article{Juang-IEEE-IT-86,
	Author = {B.-H. Juang and S. E. Levinson and M. M. Sondhi},
	Journal = {IEEE Transactions on Information Theory},
	Month = {March},
	Number = {2},
	Pages = {307-309},
	Title = {Maximum likelihood estimation for multivariate mixture observations of Markov chains},
	Volume = {32},
	Year = {1986}}

@inproceedings{Smyth-hmmclust-96,
	Author = {P. Smyth},
	Booktitle = {Advances in NIPS},
	Pages = {648-654},
	Title = {Clustering Sequences with Hidden Markov Models},
	Year = {1996}}

@article{Jiang-and-tanner-IEEEinft-99,
	Author = {Wenxin Jiang and M. A. Tanner},
	Journal = {IEEE Transactions on Information Theory},
	Pages = {1005-1013},
	Title = {On the Asymptotic Normality of Hierarchical Mixtures-of-Experts for Generalized Linear Models},
	Volume = {46},
	Year = {1999}}

@article{Carvalho-and-tanner-2006,
	Abstract = {We discuss a class of nonlinear models based on mixtures-of-experts of regressions of exponential family time series models, where the covariates include functions of lags of the dependent variable as well as external covariates. The discussion covers results on model identifiability, stochastic stability, parameter estimation via maximum likelihood estimation, and model selection via standard information criteria. Applications using real and simulated data are presented to illustrate how mixtures-of-experts of time series models can be employed both for data description, where the usual mixture structure based on an unobserved latent variable may be particularly important, as well as for prediction, where only the mixtures-of-experts flexibility matters.},
	Author = {Alexandre X. Carvalho and Martin A. Tanner},
	Journal = {International journal of mathematics and mathematical sciences},
	Number = {6},
	Pages = {1-22},
	Title = {Modeling nonlinearities with mixtures-of-experts of time series models},
	Volume = {2006},
	Year = {2006}}

@article{liu-and-rubin-ECME,
	Author = {Liu, C. and Rubin, D. B.},
	Citeulike-Article-Id = {1619156},
	Citeulike-Linkout-0 = {http://dx.doi.org/10.1093/biomet/81.4.633},
	Citeulike-Linkout-1 = {http://biomet.oxfordjournals.org/cgi/content/abstract/81/4/633},
	Day = {1},
	Doi = {10.1093/biomet/81.4.633},
	Journal = {Biometrika},
	Keywords = {ecme},
	Month = {December},
	Number = {4},
	Pages = {633--648},
	Posted-At = {2007-09-04 14:01:01},
	Priority = {2},
	Title = {The ECME algorithm: A simple extension of {EM} and ECM with faster monotone convergence},
	Volume = {81},
	Year = {1994},
	Bdsk-Url-1 = {http://dx.doi.org/10.1093/biomet/81.4.633}}

@article{wei-and-tanner-MCEM,
	Author = {Wei, G. C. G. and Tanner, M. A.},
	Journal = {Journal of the American Statistical Association},
	Number = {411},
	Pages = {699-704},
	Title = {A Monte Carlo Implementation of the {EM} Algorithm and the Poor Man's Data Augmentation Algorithms},
	Volume = {85},
	Year = {1990}}

@article{celeux-et-diebolt-SEM-85,
	Author = {Celeux, G. and Diebolt, J.},
	Journal = {Computational Statistics Quarterly},
	Number = {1},
	Pages = {73-82},
	Title = {The {SEM} algorithm a probabilistic teacher algorithm derived from the {EM} algorithm for the mixture problem},
	Volume = {2},
	Year = {1985}}

@techreport{celeux-et-diebolt-SAEM-RRinria-91,
	Author = {Celeux, G. and Diebolt, J.},
	Institution = {The French National Institute for Research in Computer Science and Control (INRIA)},
	Number = {RR-1383},
	Title = {A stochastic approximation type {EM} algorithm for the mixture problem},
	Year = {1991}}

@techreport{celeux-et-al-SEM-RRinria-95,
	Author = {Celeux, G. and Chauveau, D. and Diebolt, J.},
	Institution = {The French National Institute for Research in Computer Science and Control (INRIA)},
	Number = {RR-2514},
	Title = {On Stochastic Versions of the {EM} Algorithm},
	Year = {1995}}

@techreport{celeux-and-gilda-NEC-93,
	Author = {Celeux, G. and Soromenho, G.},
	Institution = {The French National Institute for Research in Computer Science and Control (INRIA)},
	Number = {RR-1874},
	Title = {An entropy criterion for assessing the number of clusters in a mixture model},
	Year = {1993}}

@inproceedings{U-matrix,
	Abstract = {In this paper we describe experiments with self organizing feature maps that have been implemented on a transputer system. The use of feature maps for clustering is investigated and it is shown that naive application of Kohonen's algorithm, although preserving the topology of the input data, is not able to capture clusters. A new method, called U-matrix, is proposed which is capable to classify correctly all examples. First experiments with medical data of high dimensionality show a high correlation with expert clustering of data.},
	Author = {Ultsch, A. and Siemon, H. P.},
	Booktitle = {Proceedings of International Neural Networks Conference (INNC)},
	Pages = {305-308},
	Publisher = {Kluwer Academic Press},
	Title = {Kohonen's Self Organizing Feature Maps for Exploratory Data Analysis},
	Year = {1990}}

@phdthesis{Scholkopf-thesis-1997,
	Author = {Bernhard Sch\"{o}lkopf},
	Note = {Published by: R. Oldenbourg Verlag, Munich},
	School = {Technischen Universit\"{a}t Berlin},
	Title = {Support Vector Learning},
	Year = {1997}}

@book{Scholkopf-kernel-2001,
	Author = {Sch\"{o}lkopf, Bernhard and Smola, Alexander J.},
	Edition = {1st},
	Keywords = {kernel-method, learning-theory, machine-learning, pattern-recognition},
	Publisher = {The MIT Press},
	Title = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond (Adaptive Computation and Machine Learning)},
	Year = {2001}}

@book{Nello-cristianini-book-SVM-kernel,
	Author = {N. Cristianini and J. Shawe-Taylor},
	Publisher = {Cambridge University Press},
	Title = {An Introduction to Support Vector Machines and other kernel-base learning methods},
	Year = {2000}}

@book{shawe-taylor-kernel-2004,
	Author = {Shawe-Taylor, J. and Cristianini, N.},
	Howpublished = {Hardcover},
	Keywords = {pattern analysis, kernel methods, support vector machines},
	Month = {June},
	Publisher = {Cambridge University Press},
	Title = {Kernel Methods for Pattern Analysis},
	Year = {2004}}

@article{spearman04,
	Author = {Spearman, C.},
	Journal = {American Journal of psychology},
	Pages = {201-293},
	Title = {General intelligence, objectively determined and measured},
	Volume = {15},
	Year = {1904}}

@inproceedings{smyth-nips1994,
	Author = {P. Smyth and U. M. Fayyad and M. C. Burl and P. Perona and P. Baldi},
	Booktitle = {NIPS},
	Pages = {1085-1092},
	Title = {Inferring Ground Truth from Subjective Labelling of Venus Images},
	Year = {1994}}

@inproceedings{smyth-text-nips2006,
	Author = {Chemudugunta, C. and Smyth, P. and Steyvers, M.},
	Booktitle = {Advances in Neural Information Processing Systems},
	Title = {Modeling General and Specific Aspects of Documents with a Probabilistic Topic Model},
	Year = {2006}}

@article{viterbi,
	Author = {Viterbi, A. J.},
	Journal = {IEEE Transactions on Information Theory},
	Number = {2},
	Pages = {260--269},
	Title = {Error bounds for convolutional codes and an asymptotically optimum decoding algorithm},
	Volume = {13},
	Year = {1967}}

@techreport{Stephens99,
    author = {Matthew Stephens},
    title = {{Dealing with Multimodal Posteriors and Non-Identifiability in Mixture Models}},
    institution = {Department of Statistics, Oxford University},
    year = {1999}
}

@ARTICLE{Stephens2000Jasa,
    author = {Matthew Stephens},
    title = {Dealing with label switching in mixture models},
    journal = {Journal of the Royal Statistical Society, Series B},
    year = {2000},
    volume = {62},
    pages = {795--809}
}

@article{Stephens98bayesiananalysis-mixtures,
	Author = {Stephens, M.},
	Journal = {Annals of Statistics},
	Number = {1},
	Pages = {40--74},
	Title = {Bayesian Analysis of Mixture Models with an Unknown Number of Components -- an alternative to reversible jump methods},
	Volume = {28},
	Year = {2000}}

@phdthesis{Stephens-thesis-97,
	Author = {Stephens, M.},
	School = {University of Oxford},
	Title = {Bayesian Methods for Mixtures of Normal Distributions},
	Year = {1997}}

@article{lange-GradienEM,
	Author = {Lange, K.},
	Journal = {Journal of the Royal Statistical Society, B},
	Pages = {425--437.},
	Title = {A gradient algorithm locally equivalent to the {EM} algorithm},
	Volume = {57},
	Year = {1995}}

@phdthesis{Mo-Dang-thesis,
	Author = {Dang, M. V.},
	School = {Universit\'{e} de Technologie de Compi\`{e}gne},
	Title = {Classification de Donn\'{e}es Spatiales : Mod\`{e}les Probabilistes et Crit\`{e}res de Partitionnement},
	Year = {1998}}

@phdthesis{muri-thesis,
	Author = {Muri, F.},
	School = {Universit\'{e} Paris Descartes, Paris V},
	Title = {Comparaison d'algorithmes d'identification de cha\^{i}nes de Markov cach\'{e}es et application \`a la d\'{e}tection de r\'{e}gions homog\`{e}nes dans les s\'{e}quences ADN},
	Year = {1997}}

@article{Ramsay-Dalzell-97,
	Abstract = {Multivariate data analysis permits the study of observations which are finite sets of numbers, but modern data collection situations can involve data, or the processes giving rise to them, which are functions. Functional data analysis involves infinite dimensional processes and/or data. The paper shows how the theory of L-splines can support generalizations of linear modelling and principal components analysis to samples drawn from random functions. Spline smoothing rests on a partition of a function space into two orthogonal subspaces, one of which contains the obvious or structural components of variation among a set of observed functions, and the other of which contains residual components. This partitioning is achieved through the use of a linear differential operator, and we show how the theory of polynomial splines can be applied more generally with an arbitrary operator and associated boundary constraints. These data analysis tools are illustrated by a study of variation in temperature-precipitation patterns among some Canadian weather-stations.},
	Author = {Ramsay, J. O. and Dalzell, C. J.},
	Journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	Keywords = {fda},
	Number = {3},
	Pages = {539--572},
	Posted-At = {2010-08-13 13:05:59},
	Priority = {2},
	Publisher = {Blackwell Publishing for the Royal Statistical Society},
	Title = {Some Tools for Functional Data Analysis},
	Volume = {53},
	Year = {1991}}

@book{ruppert-etal-semiparametricregression,
	Author = {Ruppert, D. Wand, M.P. and Carroll, R.J.},
	Publisher = {Cambridge University Press},
	Title = {Semiparametric Regression},
	Year = {2003}}

@inproceedings{Smyth96,
	Author = {P. Smyth},
	Booktitle = {Advances in Neural Information Processing Systems 9, NIPS},
	Pages = {648--654},
	Title = {Clustering Sequences with Hidden Markov Models},
	Year = {1996}}

@phdthesis{murphy-thesis,
	Author = {Murphy, K. P.},
	School = {UC Berkeley, Computer Science Division},
	Title = {Dynamic Bayesian Networks: Representation, Inference and Learning},
	Year = {2002}}

@article{celeuxetgovaert-mixture-classif-93,
	Author = {Celeux, G. and Govaert, G.},
	Journal = {Journal of Statistical Computation and Simulation},
	Pages = {127--146},
	Title = {Comparison of the Mixture and the Classification Maximum likelihood in Cluster Analysis},
	Volume = {47},
	Year = {1993}}

@book{mclachlan-basford88,
	Author = {G.J. McLachlan and K.E. Basford},
	Publisher = {Marcel Dekker, New York},
	Title = {Mixture Models: Inference and Applications to Clustering},
	Year = 1988}

@article{Scott71Symons,
	Author = {A. J. Scott and M. J. Symons},
	Journal = {Biometrics},
	Pages = {387--397},
	Title = {Clustering methods based on likelihood ratio criteria},
	Volume = 27,
	Year = {1971}}

@incollection{McLachlanCEM1982,
author = {McLachlan, G. J.},
year = {1982},
title = {The classification and mixture maximum likelihood approaches to cluster analysis},
booktitle = {In Handbook of Statistics, Vol. 2},
Editor = {P.R. Krishnaiah and L. Kanal},
publisher = {Amsterdam: North-Holland},
pages = {199-208},
}

@book{rabiner-book,
	Address = {Upper Saddle River, NJ, USA},
	Author = {Rabiner, L. and Juang, B.-H.},
	Publisher = {Prentice-Hall, Inc.},
	Title = {Fundamentals of speech recognition},
	Year = {1993}}

@phdthesis{oukhellou97,
	Author = {Oukhellou, L.},
	School = {Universit\'e PARIS-SUD, Centre d'ORSAY},
	Title = {Param\'etrisation et Classification de Signaux en Contr\^ole Non Destructif. Application \`a la Reconnaissance des D\'efauts de Rails par Courants de Foucault},
	Type = {Th\`ese de doctorat},
	Year = {1997}}

@techreport{Bleakley2009,
	Author = {K. Bleakley and J.-P. Vert},
	Institution = {HAL-00422430},
	Optmonth = {October},
	Title = {Joint segmentation of many aCGH profiles using fast group LARS},
	Year = {2009}}

@book{FerratyANDVieuBook,
	Author = {Ferraty, Fr{\'e}d{\'e}ric and Vieu, Philippe},
	Isbn = {0-387-30369-3},
	Publisher = {Springer series in statistics},
	Title = {Nonparametric functional data analysis : theory and practice},
	Year = {2006}}

@article{Cho1998,
	Author = {Raymond J. Cho and Michael J. Campbell and Elizabeth A. Winzeler and Lars Steinmetz and Andrew Conway and Lisa Wodicka and Tyra G. Wolfsberg and Andrei E. Gabrielian and David Landsman and David J. Lockhart and Ronald W. Davis},
	Journal = {Molecular Cell},
	Number = {1},
	Pages = {65--73},
	Title = {A Genome-Wide Transcriptional Analysis of the Mitotic Cell Cycle},
	Volume = {2},
	Year = {1998}}

@article{Quandt1972,
	Author = {R. E. Quandt},
	Journal = {Journal of the American Statistical Association},
	Number = {338},
	Pages = {306--310},
	Title = {A new approach to estimating switching regressions},
	Volume = {67},
	Year = {1972}}

@article{DeVeaux1989,
	Abstract = {The purpose of this article is to develop the technology of models based on mixtures of linear regressions and, in particular, to draw out the relevance of the \{EM\} algorithm to the associated maximum likelihood equations. A √n-consistent starting point for the \{EM\} algorithm is presented. The data from an experiment in music perception are analyzed using this technology. Performance of the estimators are examined via both simulated and actual data sets. },
	Author = {Richard D. De Veaux},
	Journal = {Computational Statistics and Data Analysis},
	Number = {3},
	Pages = {227--245},
	Title = {Mixtures of linear regressions},
	Volume = {8},
	Year = {1989}}

@article{DeSarboAndCron1988,
	Author = {DeSarbo, Wayne and Cron, William},
	Journal = {Journal of Classification},
	Keywords = {Cluster analysis; Multiple regression; Maximum likelihood estimation; E-M algorithm; Marketing trade shows},
	Number = {2},
	Pages = {249--282},
	Title = {A maximum likelihood methodology for clusterwise linear regression},
	Volume = {5},
	Year = {1988}}

@article{VerbekeAndLesaffre1996,
	Abstract = {{This article investigates the impact of the normality assumption for random effects on their estimates in the linear mixed-effects model. It shows that if the distribution of random effects is a finite mixture of normal distributions, then the random effects may be badly estimated if normality is assumed, and the current methods for inspecting the appropriateness of the model assumptions are not sound. Further, it is argued that a better way to detect the components of the mixture is to build this assumption in the model and then "compare" the fitted model with the Gaussian model. All of this is illustrated on two practical examples}},
	Author = {Verbeke, Geert and Lesaffre, Emmanuel},
	Journal = {Journal of the American Statistical Association},
	Number = {433},
	Pages = {217--221},
	Title = {A Linear Mixed-Effects Model With Heterogeneity in the Random-Effects Population},
	Volume = {91},
	Year = {1996}}

@article{XuAndHedeker2001,
	Author = {Xu, W and Hedeker, D},
	Journal = {Journal of Biopharmaceutical Statistics},
	Number = {4},
	Pages = {253--73},
	Title = {A random-effects mixture model for classifying treatment response in longitudinal clinical trials.},
	Volume = {11},
	Year = {2001}}

@article{LairdAndWare1982,
	Abstract = {{Models for the analysis of longitudinal data must recognize the relationship between serial observations on the same unit. Multivariate models with general covariance structure are often difficult to apply to highly unbalanced data, whereas two-stage random-effects models can be used easily. In two-stage models, the probability distributions for the response vectors of different individuals belong to a single family, but some random-effects parameters vary across individuals, with a distribution specified at the second stage. A general family of models is discussed, which includes both growth models and repeated-measures models as special cases. A unified approach to fitting these models, based on a combination of empirical Bayes and maximum likelihood estimation of model parameters and using the EM algorithm, is discussed. Two examples are taken from a current epidemiological study of the health effects of air pollution.}},
	Author = {Laird, N. M. and Ware, J. H.},
	Journal = {Biometrics},
	Month = dec,
	Number = {4},
	Pages = {963--974},
	Title = {{Random-effects models for longitudinal data.}},
	Volume = {38},
	Year = {1982}}

@article{JonesANDMcLachlan1992,
	Author = {P. N. Jones and G. J. McLachlan},
	Journal = {Australian Journal of Statistics},
	Month = {June},
	Number = {2},
	Pages = {233--240},
	Title = {FITTING FINITE MIXTURE MODELS IN A REGRESSION CONTEXT},
	Volume = {34},
	Year = {1992}}

@article{VieleANDTong2002,
	Author = {Viele, K. and Tong, B.},
	Journal = {Statistics and Computing},
	Pages = {315--330},
	Title = {Modeling with Mixtures of Linear Regressions},
	Volume = {12},
	Year = {2002}}

@article{FariaANDSoromenho2010,
	Author = {Susana Faria and Gilda Soromenho},
	Journal = {Journal of Statistical Computation and Simulation},
	Month = {February},
	Number = {2},
	Pages = {201--225},
	Title = {Fitting mixtures of linear regressions},
	Volume = {80},
	Year = {2010}}

@article{HartiganAndWong:Kmeans,
	Author = {J. A. Hartigan and M. A. Wong},
	Entrydate = 20030618,
	Journal = {Applied Statistics},
	Pages = {100--108},
	Title = {A {K}-Means Clustering Algorithm},
	Volume = 28,
	Year = 1979}

@article{PropertiesKmeansAndTrimmedKmeans,
	Author = {Garcia-Escudero, L.A. and Gordaliza, A.},
	Journal = {Journal of the American Statistical Association},
	Pages = {956--969},
	Title = {Robustness properties of $k$-means and trimmed $k$-means},
	Volume = 94,
	Year = 1999}

@article{TrimmedKmeans,
	Author = {Cuesta, J.A. and Albertos, J.A.C. and C. Matran},
	Journal = {AnnalsofStatistics},
	Pages = {553--576},
	Title = {Trimmed k-means: an attempt to robustify quantizers},
	Volume = 25,
	Year = 1997}

@article{Reddy:2008,
	Address = {Washington, DC, USA},
	Author = {Reddy, Chandan K. and Chiang, Hsiao-Dong and Rajaratnam, Bala},
	Issn = {0162-8828},
	Journal = {IEEE Transactions Pattern Analysis and Machine Intelligence},
	Number = {7},
	Numpages = {12},
	Pages = {1146--1157},
	Publisher = {IEEE Computer Society},
	Title = {TRUST-TECH-Based Expectation Maximization for Learning Finite Mixture Models},
	Volume = {30},
	Year = {2008}}

@book{McLachlan2000FMM,
	Author = {McLachlan, G. J. and D. Peel.},
	Publisher = {New York: Wiley},
	Title = {Finite mixture models},
	Year = {2000}}

@article{HebrailEtAl2010,
	Author = {H{\'e}brail, G. and Hugueney, B. and Lechevallier, Y. and Rossi, F.},
	Date-Modified = {2015-08-21 21:58:33 +0000},
	Journal = {Neurocomputing},
	Month = {March},
	Number = {7-9},
	Pages = {1125--1141},
	Title = {Exploratory analysis of functional data via clustering and optimal segmentation},
	Volume = {73},
	Year = {2010}}

@article{Fraley2002-MBC,
	Author = {Fraley, C. and Raftery, A. E.},
	Date-Modified = {2015-08-21 21:46:08 +0000},
	Journal = {Journal of the American Statistical Association},
	Pages = {611--631},
	Title = {Model-Based Clustering, Discriminant Analysis, and Density Estimation},
	Volume = {97},
	Year = {2002}}

@inproceedings{Cabanes-IICAI2009,
	Address = {Tumkur, India},
	Author = {Gu{\'e}na{\"e}l Cabanes and Dominique Fresneau and Ugo Galassi and Attilio Giordana},
	Booktitle = {4th International Indian Conference on Artificial Intelligence (IICAI'09)},
	Pages = {2127-2139},
	Title = {A HMM-Based Approach to Modeling Ant Behavior},
	Year = {2009}}

@inproceedings{Cabanes-ISMIS2009,
	Address = {Prague, Czech Republic},
	Author = {Gu{\'e}na{\"e}l Cabanes and Dominique Fresneau and Ugo Galassi and Attilio Giordana},
	Booktitle = {International Symposium on Methodologies for Intelligent Systems (ISMIS'09)},
	Pages = {341-350},
	Title = {Modeling Ant Activity by Means of Structured HMMs},
	Year = {2009}}

@inproceedings{Jaziri-ICANN2011,
	Author = {Jaziri, Rakia and Lebbah, Mustapha and Bennani, Younes and Chenot, Jean-Hugues},
	Booktitle = {In Proceedings of the 21st International Conference on Artificial Neural Networks (ICANN 2011)},
	Editor = {Honkela, Timo and Duch, Wlodzislaw and Girolami, Mark A. and Kaski, Samuel},
	Pages = {87-94},
	Publisher = {Springer},
	Series = {Lecture Notes in Computer Science},
	Title = {SOS-HMM: Self-Organizing Structure of Hidden Markov Model.},
	Volume = 6792,
	Year = 2011}

@inproceedings{Galassi2005,
	Author = {Galassi, U. and Giordana, A. and Saitta, L. and Botta, M.},
	Booktitle = {IJCAI},
	Editor = {Kaelbling, Leslie Pack and Saffiotti, Alessandro},
	Pages = {1600-1601},
	Publisher = {Professional Book Center},
	Title = {Learning Complex Event Descriptions by Abstraction.},
	Year = 2005}

@inproceedings{Galassi2007,
	Author = {Galassi, U. and Giordana, A. and Saitta, L.},
	Booktitle = {IJCAI},
	Editor = {Veloso, Manuela M.},
	Pages = {798-803},
	Title = {Incremental Construction of Structured Hidden Markov Models.},
	Year = 2007}

@article{Galassi2007a,
	Author = {Galassi, U. and Botta, M. and Giordana, A.},
	Journal = {Fundamenta Informaticae},
	Number = 4,
	Pages = {487-505},
	Title = {Hierarchical Hidden Markov Models for User/Process Profile Learning.},
	Volume = 78,
	Year = 2007}

@inproceedings{Logan1997,
	Author = {Logan, B. T. and Robinson, A. J.},
	Booktitle = {Proc. 5th European Conference on Speech Communication and Technology},
	Editor = {Kokkinakis, George and Fakotakis, Nikos and Dermatas, Evangelos},
	Pages = {2103--2106},
	Publisher = {ISCA},
	Title = {Improving autoregressive hidden Markov model recognition accuracy using a non-linear frequency scale with application to speech enhancement.},
	Year = 1997}

@inproceedings{Rogovschi2013,
	Author = {Rogovschi, N. and Labiod, L. and Nadif, M.},
	Booktitle = {IJCNN},
	Pages = {1-6},
	Publisher = {IEEE},
	Title = {A topographical nonnegative matrix factorization algorithm.},
	Year = 2013}

@article{Hanczar2012,
	Author = {Hanczar, B. and Nadif, M.},
	Journal = {Pattern Recognition},
	Number = {11},
	Pages = {3938 - 3949},
	Title = {Ensemble methods for biclustering tasks},
	Volume = {45},
	Year = {2012}}

@article{Hanczar2011,
	Author = {Hanczar, B. and Nadif, M.},
	Journal = {Neurocomputing},
	Number = 10,
	Pages = {1595-1605},
	Title = {Using the bagging approach for biclustering of gene expression data.},
	Volume = 74,
	Year = 2011}

@inproceedings{Labiod2011,
	Author = {Labiod, L. and Nadif, M.},
	Booktitle = {ICONIP (2)},
	Editor = {Lu, Bao-Liang and Zhang, Liqing and Kwok, James T.},
	Pages = {709-717},
	Publisher = {Springer},
	Series = {Lecture Notes in Computer Science},
	Title = {Co-clustering under Nonnegative Matrix Tri-Factorization.},
	Volume = 7063,
	Year = 2011}

@article{BayesFactors-KassAndRaftery-1995,
	Abstract = {{In a 1935 paper and in his book Theory of probability, Jeffresy developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpies was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P-values, less attention has been given to the Bayes as a practical tool of applied statistics. In this article we review and discuss the uses of Bayes factors in the context of five scientific applications in genetics, sports, ecology, sociology, and psychology. We emphasize the following points: From Jeffrey's Bayesian viewpoint, the purpose of hypothesis testing is to evaluate the evidence in favor of a scientific theory. Bayes factors offer a way of evaluating evidence in favor of a null hypothesis. Bayes factors provide a way of incorporating external information into the evaluation of evidence about a hypothesis. Bayes factors are very general and do not require alternative models to be nested. Several techniques are available for computing Bayes factors, including asymptotic approximations that are easy to compute using the output from standard packages that maximize likelihoods. In "non-Bayesian significance tests. The Schwarz criterion (or BIC) gives a rough approximation to the logarithm of the Bayes factor, which is easy to use and does not require evaluation of prior distributions. When one is interested in estimation or prediction, Bayes factors may be converted to weights to be attached to various models so that a composite estimate or prediction may be obtained that takes account of structural or model uncertainty. Algorithms have been proposed that allow model uncertainty to be taken into account when the class of models initially considered is very large. Bayes factors are useful for guiding an evolutionary model-building process. It is important, and feasible, to assess the sensitivity of conclusions to the prior distributions used.}},
	Author = {Kass, Robert E. and Raftery, Adrian E.},
	Date-Modified = {2015-08-21 22:04:42 +0000},
	Journal = {Journal of the American Statistical Association},
	Number = {430},
	Pages = {773--795},
	Publisher = {American Statistical Association},
	Title = {{Bayes Factors}},
	Volume = {90},
	Year = {1995}}

@inproceedings{Chamroukhi-ICPR2014,
	Address = {Stockholm},
	Author = {F. Chamroukhi and Marius Bartcus and Herv{\'e} Glotin},
	Booktitle = {Proceedings of 22nd International Conference on Pattern Recognition (ICPR)},
	Month = {August},
	Title = {Bayesian non-parametric parsimonious clustering},
	Year = {2014}}

@inproceedings{Chamroukhi-ESANN2014,
	Address = {Bruges, Belgium},
	Author = {F. Chamroukhi and Marius Bartcus and Herv{\'e} Glotin},
	Booktitle = {Proceedings of 22nd European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN)},
	Month = {April},
	Title = {Bayesian non-parametric parsimonious clustering},
	Year = {2014}}

@inproceedings{Chamroukhi2013-NipsWksp,
	Address = {Nevada, USA},
	Author = {Marius Bartcus and Faicel Chamroukhi and and Herv{\'e} Glotin},
	Booktitle = {Proceedings of the NIPS4B workshop, Neural Information Processing Systems (NIPS)},
	Pages = {205--211},
	Title = {Unsupervised whale song decomposition with Bayesian non-parametric Gaussian mixture},
	Year = {2013}}

@phdthesis{Jaziri-PhD,
	Author = {Rakia Jaziri},
	Month = {June},
	School = {Universit{\'e} Paris 13},
	Title = {Mod{\`e}les de m{\'e}langes topologiques pour la classification de donn{\'e}es structur{\'e}es en s{\'e}quences},
	Year = {2013}}

@inproceedings{Priam2013,
	Author = {Rodolphe Priam and Mohamed Nadif and G{\'e}rard Govaert},
	Booktitle = {Intelligent Data Analysis},
	Pages = {345--356},
	Title = {Gaussian Topographic Co-clustering Model},
	Year = {2013}}

@article{Hanczar-Nadif2012,
	Author = {Blaise Hanczar and Mohamed Nadif},
	Journal = {Pattern Recognition},
	Number = {11},
	Pages = {3938--3949},
	Title = {Ensemble methods for biclustering tasks},
	Volume = {45},
	Year = {2012}}

@article{Govaert-Nadif2008,
	Author = {G\'erard Govaert and Mohamed Nadif},
	Journal = {Computational Statistics and Data Analysis},
	Keywords = {Co-clustering},
	Number = {6},
	Pages = {3233 --3245},
	Title = {Block clustering with Bernoulli mixture models: Comparison of different approaches},
	Volume = {52},
	Year = {2008}}

@article{Govaert-Nadif2006,
	Author = {G{\'e}rard Govaert and Mohamed Nadif},
	Journal = {Soft Computing},
	Number = {5},
	Pages = {415--422},
	Title = {Fuzzy clustering to estimate the parameters of block mixture models},
	Volume = {10},
	Year = {2006}}

@inproceedings{Priam08,
	Author = {Rodolphe Priam and Mohamed Nadif and G{\'e}rard Govaert},
	Booktitle = {Artificial Neural Networks in Pattern Recognition, Third IAPR Workshop, ANNPR 2008},
	Pages = {13--23},
	Title = {The Block Generative Topographic Mapping},
	Year = {2008}}

@article{Govaert-Nadif2003,
	Author = {G{\'e}rard Govaert and Mohamed Nadif},
	Journal = {Pattern Recognition},
	Keywords = {Block \{CEM\} algorithm},
	Note = {Biometrics},
	Number = {2},
	Pages = {463 - 473},
	Title = {Clustering with block mixture models},
	Volume = {36},
	Year = {2003}}

@book{Efron-and-Tibshirani1993-Bootstrap,
	Author = {B. Efron and R. J. Tibshirani},
	Publisher = {Chapman \& Hall},
	Title = {An Introduction to the Bootstrap},
	Year = {1993}}

@article{Efron79-Bootstrap,
	Author = {B. Efron},
	Journal = {Annals of Statistics},
	Pages = {1--26},
	Title = {Bootstrap Methods: Another Look at the Jackknife},
	Volume = {7},
	Year = {1979}}

@inproceedings{Kohavi95-CV-Bootstrap,
	Author = {Ron Kohavi},
	Booktitle = {Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence},
	Pages = {1137--1143},
	Publisher = {Morgan Kaufmann},
	Title = {A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection},
	Year = {1995}}


@article{NguyenChamroukhi-MoE-ArXiv,
	Archiveprefix = {arXiv},
	Author = {Hien D. Nguyen and Faicel Chamroukhi},
	Journal = {ArXiv preprint  arXiv:1707.03538v1},
	Month = {Jul},
	Title = {An Introduction to the Practical and Theoretical Aspects of Mixture-of-Experts Modeling},
	Url = {https://arxiv.org/abs/1707.03538v1},
	Year = {2017}}

@article{NguyenChamroukhi-MoE,
	Author = {Hien D. Nguyen and Faicel Chamroukhi},
	Journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
	Title = {Practical and theoretical aspects of mixture-of-experts modeling: An overview},
	publisher = {Wiley Periodicals, Inc},
	issn = {1942-4795},
	url = {http://dx.doi.org/10.1002/widm.1246},
	pages = {e1246--n/a},
	keywords = {classification, clustering, mixture models, mixture of experts, neural networks},
	Month = {Feb},
Year = {2018},
doi = {10.1002/widm.1246}
}

@article{Nguyen2014MixSSR,
	Author = {Hien D. Nguyen and Geoffrey J. McLachlan and Ian A. Wood},
	Journal = {ArXiv preprint  arXiv:1306.3014v2},
	Title = {{Mixtures of Spatial Spline Regressions}},
	Url = {https://arxiv.org/abs/1306.3014v2},
	Year = {2013}}


@ARTICLE{Nguyen-MixAR-2016,
title = {Spatial clustering of time series via mixture of autoregressions models and Markov random fields},
author = {Nguyen, Hien D. and McLachlan, Geoffrey J. and Ullmann, Jeremy F. P. and Janke, Andrew L.},
year = {2016},
journal = {Statistica Neerlandica},
volume = {70},
number = {4},
pages = {414-439}
}

@article {Nguyen-SADM-2017,
title = {Whole-volume clustering of time series data from zebrafish brain calcium images via mixture modeling},
author = {Nguyen, Hien D. and Ullmann, Jeremy F. P. and McLachlan, Geoffrey J. and Voleti, Venkatakaushik and Li, Wenze and Hillman, Elizabeth M. C. and Reutens, David C. and Janke, Andrew L.},
journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
publisher = {Wiley Subscription Services, Inc., A Wiley Company},
issn = {1932-1872},
volume = {11},
number = {1},
doi = {10.1002/sam.11366},
pages = {5--16},
year={2018},
keywords = {calcium imaging, mixture model, time series data, zebrafish},
}


@article {NguyenMMtuto2017,
author = {Nguyen, Hien D.},
title = {{An introduction to Majorization-Minimization algorithms for machine learning and statistical estimation}},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
volume = {7},
number = {2},
publisher = {Wiley Periodicals, Inc},
issn = {1942-4795},
url = {http://dx.doi.org/10.1002/widm.1198},
doi = {10.1002/widm.1198},
pages = {e1198--n/a},
year = {2017},
note = {e1198},
}



	
@article{Nguyen2016MixSSR,
title = "Mixtures of spatial spline regressions for clustering and classification",
journal = "Computational Statistics \& Data Analysis",
volume = "93",
pages = "76 - 85",
year = "2016",
issn = "0167-9473",
doi = "https://doi.org/10.1016/j.csda.2014.01.011",
author = "Hien D. Nguyen and Geoffrey J. McLachlan and Ian A. Wood",
keywords = "Functional data, Mixture model, Classification, Clustering, Spatial spline"
}

@article{Ng-and-McLachlan-2014,
	Author = {S. K. Ng and G. J. McLachlan},
	Journal = {Computational Statistics and Data Analysis},
	Keywords = {\{EM\} algorithm},
	Number = {0},
	Pages = {43-- 51},
	Title = {Mixture models for clustering multilevel growth trajectories},
	Volume = {71},
	Year = {2014}}

@book{Alan2005,
	Address = {Philadelphia, PA},
	Author = {Alan, J. L.},
	Publisher = {SIAM: Society for Industrial and Applied Mathematics},
	Title = {Matrix Analysis for Scientists and Engineers},
	Year = {2005}}

@incollection{Aldous1985,
	Author = {Aldous, D. J.},
	Booktitle = {{\'E}cole d'{\'E}t{\'e} St Flour 1983},
	Keyw = {exchangeable},
	Keywords = {imported},
	Note = {Lecture Notes in Math. 1117},
	Pages = {1--198},
	Publisher = {Springer-Verlag},
	Title = {Exchangeability and Related Topics},
	Year = {1985}}

@inproceedings{Alon2003,
	Abstract = {A new approach is proposed for clustering time-series data. The approach can be used to discover groupings of similar object motions that were observed in a video collection. A finite mixture of hidden Markov models (HMMs) is fitted to the motion data using the expectation-maximization (EM) framework. Previous approaches for HMM-based clustering employ a k-means formulation, where each sequence is assigned to only a single HMM. In contrast, the formulation presented in this paper allows each sequence to belong to more than a single HMM with some probability, and the hard decision about the sequence class membership can be deferred until a later time when such a decision is required. Experiments with simulated data demonstrate the benefit of using this EM-based approach when there is more "overlap" in the processes generating the data. Experiments with real data show the promising potential of HMM-based motion clustering in a number of applications.},
	Address = {Los Alamitos, CA, USA},
	Author = {Alon, J. and Sclaroff, S. and Kollios, G. and Pavlovic, V.},
	Booktitle = {Proceedings of the 2003 IEEE computer society conference on Computer vision and pattern recognition (CVPR)},
	Pages = {375--381},
	Title = {Discovering Clusters in Motion Time-Series Data},
	Year = {2003}}

@inproceedings{Alon-03,
	Abstract = {A new approach is proposed for clustering time-series data. The approach can be used to discover groupings of similar object motions that were observed in a video collection. A finite mixture of hidden Markov models (HMMs) is fitted to the motion data using the expectation-maximization (EM) framework. Previous approaches for HMM-based clustering employ a k-means formulation, where each sequence is assigned to only a single HMM. In contrast, the formulation presented in this paper allows each sequence to belong to more than a single HMM with some probability, and the hard decision about the sequence class membership can be deferred until a later time when such a decision is required. Experiments with simulated data demonstrate the benefit of using this EM-based approach when there is more "overlap" in the processes generating the data. Experiments with real data show the promising potential of HMM-based motion clustering in a number of applications.},
	Address = {Los Alamitos, CA, USA},
	Author = {Alon, J. and Sclaroff, S. and Kollios, G. and Pavlovic, V.},
	Booktitle = {Proceedings of the 2003 IEEE computer society conference on Computer vision and pattern recognition (CVPR)},
	Pages = {375--381},
	Title = {Discovering Clusters in Motion Time-Series Data},
	Year = {2003}}

@inproceedings{Amari1996,
	Author = {Amari, S. and Cichocki, A. and Yang, H. H.},
	Booktitle = {Proceedings of the 8th Conference on Advances in Neural Information Processing Systems (NIPS)},
	Citeulike-Article-Id = {315648},
	Pages = {757--763},
	Posted-At = {2008-02-28 15:56:48},
	Priority = {2},
	Publisher = {MIT Press},
	Title = {A New Learning Algorithm for Blind Signal Separation},
	Volume = {8},
	Year = {1996}}

@article{Anderson1991,
	Author = {Anderson, J.},
	Journal = {Psychological Review},
	Pages = {409-429},
	Title = {The adaptive nature of human categorization},
	Volume = {98},
	Year = {1991}}

@book{AndrewGelman2003,
	Author = {Andrew Gelman and John Carlin and Hal Stern and David Dunson and Aki Vehtari and Donald Rubin},
	Publisher = {Chapman and Hall/CRC},
	Title = {Bayesian Data Analysis},
	Year = {2003}}

@book{reg-non-lin,
	Author = {A. Antoniadis and J. Berruyer and R. Carmona},
	Publisher = {Economica},
	Title = {Rgression non linaire et applications},
	Year = {1992}}

@article{Antoniak1974,
	Abstract = {{A random process called the Dirichlet process whose sample functions are almost surely probability measures has been proposed by Ferguson as an approach to analyzing nonparametric problems from a Bayesian viewpoint. An important result obtained by Ferguson in this approach is that if observations are made on a random variable whose distribution is a random sample function of a Dirichlet process, then the conditional distribution of the random measure can be easily calculated, and is again a Dirichlet process. This paper extends Ferguson's result to cases where the random measure is a mixing distribution for a parameter which determines the distribution from which observations are made. The conditional distribution of the random measure, given the observations, is no longer that of a simple Dirichlet process, but can be described as being a mixture of Dirichlet processes. This paper gives a formal definition for these mixtures and develops several theorems about their properties, the most important of which is a closure property for such mixtures. Formulas for computing the conditional distribution are derived and applications to problems in bio-assay, discrimination, regression, and mixing distributions are given.}},
	Author = {Antoniak, Charles E.},
	Journal = {The Annals of Statistics},
	Keywords = {bayesian, dirichletprocess, nonparametrics},
	Number = {6},
	Pages = {1152--1174},
	Priority = {3},
	Publisher = {Institute of Mathematical Statistics},
	Title = {{Mixtures of Dirichlet Processes with Applications to Bayesian Nonparametric Problems}},
	Volume = {2},
	Year = {1974}}

@article{Aseervatham2009,
	Author = {S. Aseervatham and Y. Bennani},
	Journal = {Pattern Recognition},
	Number = {9},
	Pages = {2067-2076},
	Title = {Semi-structured document categorization with a semantic kernel},
	Volume = {42},
	Year = {2009}}

@article{Bach2006,
	Author = {F. R. Bach and M. I. Jordan},
	Journal = {Journal of Machine Learning Research},
	Pages = {1963-2001},
	Title = {Learning Spectral Clustering, With Application To Speech Separation},
	Volume = {7},
	Year = {2006}}

@article{Baldi1994,
	Address = {Cambridge, MA, USA},
	Author = {Baldi, P. and Chauvin, Y.},
	Journal = {Neural Computation},
	Number = {2},
	Pages = {307--318},
	Publisher = {MIT Press},
	Title = {Smooth on-line learning algorithms for hidden Markov models},
	Volume = {6},
	Year = {1994}}

@article{mbc-banfield-raftery93,
	Author = {Banfield, Jeffrey D. and Raftery, Adrian E.},
	Journal = {Biometrics},
	Number = {3},
	Pages = {803--821},
	Title = {Model-Based {G}aussian and Non-{G}aussian Clustering},
	Volume = {49},
	Year = {1993}}

@article{Bell1995,
	Author = {Bell, A. J. and Sejnowski, T. J.},
	Citeulike-Article-Id = {819595},
	Journal = {Neural Computation},
	Keywords = {ica},
	Number = {6},
	Pages = {1129--1159},
	Posted-At = {2006-08-28 09:02:32},
	Priority = {2},
	Title = {An Information-Maximization Approach to Blind Separation and Blind Deconvolution},
	Volume = {7},
	Year = {1995}}

@article{Bellman1961,
	Author = {Bellman, R.},
	Journal = {Communications of the Association for Computing Machinery (CACM)},
	Number = {6},
	Pages = {284},
	Title = {On the approximation of curves by line segments using dynamic programming},
	Volume = {4},
	Year = {1961}}

@article{Benboudjema2007,
	Author = {D. Benboudjema and W. Pieczynski},
	Journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	Number = {8},
	Pages = {1367-1378},
	Title = {Unsupervised Statistical Segmentation of Nonstationary Images Using Triplet Markov Fields},
	Volume = {29},
	Year = {2007}}

@article{Bengio1996,
	Author = {Bengio, Y. and Frasconi, P.},
	Journal = {IEEE Transactions on Neural Networks},
	Number = {5},
	Title = {{I}nput {O}utput {HMM}'s for Sequences Processing},
	Volume = {7},
	Year = {1996}}

@inproceedings{Bengio1995,
	Author = {Bengio, Y. and Frasconi, P.},
	Booktitle = {Advances in Neural Information Processing Systems},
	Keywords = {discriminative, em, hmm, temporal},
	Pages = {427-434},
	Title = {An input output HMM architecture},
	Volume = {7},
	Year = {1995}}

@article{Bensmail-jasa1996,
	Author = {Bensmail, Halima and Celeux, Gilles},
	Journal = {Journal of the American statistical Association},
	Number = {436},
	Pages = {1743--1748},
	Publisher = {Taylor \& Francis Group},
	Title = {Regularized Gaussian discriminant analysis through eigenvalue decomposition},
	Volume = {91},
	Year = {1996}}

@article{Bezdek1974,
	Author = {Bezdek, J. C.},
	Journal = {Journal of Mathematical Biology},
	Pages = {57-71},
	Title = {Numerical taxonomy with fuzzy sets},
	Volume = {A},
	Year = {1974}}

@article{Biernacki2003,
	Author = {Biernacki, C. and Celeux, G. and Govaert, G.},
	Journal = {Computational Statistics and Data Analysis},
	Pages = {561-575},
	Title = {{Choosing Starting Values for the {EM} Algorithm for Getting the Highest Likelihood in Multivariate Gaussian Mixture Models}},
	Volume = {41},
	Year = {2003}}

@article{Biernacki1999,
	Author = {Biernacki, C. and G. Celeux and Govaert, G.},
	Journal = {Pattern Recognition Letters},
	Number = {3},
	Pages = {267-272},
	Title = {An improvement of the NEC criterion for assessing the number of clusters in a mixture model},
	Volume = {20},
	Year = {1999}}

@book{Bishop2006,
	Address = {U K},
	Author = {C. M. Bishop},
	Publisher = {Springer Verlag},
	Title = {Pattern recognition and machine learning},
	Year = {2006}}

@book{Bishop1995,
	Author = {Bishop, C. M.},
	Publisher = {Oxford University Press, USA},
	Title = {Neural Networks for Pattern Recognition},
	Year = {1995}}

@inproceedings{Bishop1997a,
	Author = {Christopher M. Bishop and Geoffrey E. Hinton and Iain G. D. Strachan},
	Booktitle = {IEE Fifth International Conference on Artificial Neural Networks},
	Pages = {111--116},
	Title = {GTM through time},
	Year = {1997}}

@inproceedings{Bishop97gtmthroughtime,
	Author = {Christopher M. Bishop and Geoffrey E. Hinton and Iain G. D. Strachan},
	Booktitle = {IEE Fifth International Conference on Artificial Neural Networks},
	Pages = {111--116},
	Title = {GTM through time},
	Year = {1997}}

@article{Bishop1998,
	Author = {Christopher M. Bishop and Markus Svens\'en and Christopher K. I. Williams},
	Journal = {Neural Computation},
	Pages = {215--234},
	Title = {GTM: The generative topographic mapping},
	Volume = {10},
	Year = {1998}}

@article{BishopGtm98,
	Author = {Christopher M. Bishop and Markus Svens\'en and Christopher K. I. Williams},
	Journal = {Neural Computation},
	Pages = {215--234},
	Title = {GTM: The generative topographic mapping},
	Volume = {10},
	Year = {1998}}

@inproceedings{Bishop1997,
	Author = {Christopher M. Bishop and Christopher K. I. Williams},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {354--360},
	Publisher = {Springer-Verlag},
	Title = {GTM: A Principled Alternative to the Self-Organizing Map},
	Year = {1997}}

@inproceedings{Bishop97gtm,
	Author = {Christopher M. Bishop and Christopher K. I. Williams},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {354--360},
	Publisher = {Springer-Verlag},
	Title = {GTM: A Principled Alternative to the Self-Organizing Map},
	Year = {1997}}

@article{Blei2006,
	Added-At = {2010-03-25T16:34:23.000+0100},
	Author = {Blei, David M. and Jordan, Michael I.},
	Biburl = {http://www.bibsonomy.org/bibtex/239a088c43ebf61d1ed32144fbb2162cd/3mta3},
	Doi = {10.1214/06-BA104},
	File = {blei2006.pdf:Papers/blei2006.pdf:PDF},
	Interhash = {b981abbc2ceea082b118da76a36a4782},
	Intrahash = {39a088c43ebf61d1ed32144fbb2162cd},
	Journal = {Bayesian Analysis},
	Keywords = {Dirichletprocess Variationalmethods},
	Number = {1},
	Pages = {121--144},
	Timestamp = {2010-03-25T16:34:23.000+0100},
	Title = {Variational inference for Dirichlet process mixtures},
	Volume = {1},
	Year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1214/06-BA104}}

@article{variational-dp-blei-jordan2006,
	Added-At = {2010-03-25T16:34:23.000+0100},
	Author = {Blei, David M. and Jordan, Michael I.},
	Biburl = {http://www.bibsonomy.org/bibtex/239a088c43ebf61d1ed32144fbb2162cd/3mta3},
	Doi = {10.1214/06-BA104},
	File = {blei2006.pdf:Papers/blei2006.pdf:PDF},
	Interhash = {b981abbc2ceea082b118da76a36a4782},
	Intrahash = {39a088c43ebf61d1ed32144fbb2162cd},
	Journal = {Bayesian Analysis},
	Keywords = {Dirichletprocess Variationalmethods},
	Number = {1},
	Pages = {121--144},
	Timestamp = {2010-03-25T16:34:23.000+0100},
	Title = {Variational inference for Dirichlet process mixtures},
	Volume = {1},
	Year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1214/06-BA104}}

@phdthesis{Bordes2010,
	Address = {Computer Science Laboratory of Paris 6 (LIP6)},
	Author = {Bordes, A.},
	Month = {February},
	School = {Universit\'e Pierre et Marie Curie},
	Title = {New Algorithms for Large-Scale Support Vector Machines},
	Year = {2010}}

@incollection{Bottou2004,
	Address = {Berlin},
	Author = {Bottou, L.},
	Booktitle = {Advanced Lectures on Machine Learning},
	Editor = {Bousquet, O. and Von Luxburg, U.},
	Pages = {146-168},
	Publisher = {Springer Verlag},
	Series = {Lecture Notes in Artificial Intelligence, LNAI~3176},
	Title = {Stochastic Learning},
	Year = {2004}}

@incollection{Bottou1998,
	Address = {Cambridge, UK},
	Author = {Bottou, L.},
	Booktitle = {Online Learning and Neural Networks},
	Editor = {Saad, David},
	Publisher = {Cambridge University Press},
	Title = {Online Algorithms and Stochastic Approximations},
	Year = {1998}}

@inproceedings{Bottou1995,
	Author = {L. Bottou and Y. Bengio},
	Booktitle = {Advances in Neural Information Processing Systems 7},
	Pages = {585--592},
	Publisher = {MIT Press},
	Title = {Convergence Properties of the K-Means Algorithms},
	Year = {1995}}

@article{Bouguila2007,
	Author = {Bouguila, Nizar and Ziou, Djemel},
	Journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
	Number = {10},
	Pages = {1716-1731},
	Title = {High-Dimensional Unsupervised Selection and Estimation of a Finite Generalized Dirichlet Mixture Model Based on Minimum Message Length.},
	Volume = {29},
	Year = {2007}}

@book{convex_optimization_boyd_and_vandenberghe,
	Author = {Boyd, S. and Vandenberghe, L.},
	Publisher = {Cambridge University Press},
	Title = {Convex Optimization},
	Url = {http://www.stanford.edu/~boyd/cvxbook/},
	Year = {2004},
	Bdsk-Url-1 = {http://www.stanford.edu/~boyd/cvxbook/}}

@techreport{Bozdogan1983,
	Author = {Bozdogan, H.},
	Institution = {Quantitative Methods Department, University of Illinois at Chicago},
	Month = {June},
	Title = {Determining the Number of Component Clusters in the Standard Multi-variate Normal Mixture Model Using Model-Selection Criteria},
	Year = {1983}}

@article{Brailovsky1992,
	Author = {Brailovsky, V. L. and Kempner, Y.},
	Journal = {Pattern recognition},
	Number = {11},
	Pages = {1361-1370},
	Title = {Application of piecewise regression to detecting internal structure of signal.},
	Volume = {25},
	Year = {1992}}

@book{Breiman1984,
	Address = {New York},
	Author = {Breiman, L. and J. H. Friedman and R. A. Olshen and C. J. Stone},
	Publisher = {Wadsworth},
	Title = {Classification And Regression Trees},
	Year = {1984}}

@article{Come2009d,
	Author = {C\^ome, E. and Oukhellou, L. and Den{\oe}ux, T. and Aknin, P.},
	Journal = {Pattern recognition},
	Pages = {334-348},
	Title = {Learning form partially supervised data using mixture models and belief functions},
	Volume = {42},
	Year = {2009}}

@article{Come2009e,
	Author = {C\^ome, E. and Oukhellou, L. and Den{\oe}ux, T. and Aknin, P.},
	Journal = {Pattern recognition},
	Pages = {334-348},
	Title = {Learning form partially supervised data using mixture models and belief functions},
	Volume = {42},
	Year = {2009}}

@article{Come2009f,
	Author = {C\^ome, E. and Oukhellou, L. and Den{\oe}ux, T. and Aknin, P.},
	Journal = {Pattern recognition},
	Pages = {334-348},
	Title = {Learning form partially supervised data using mixture models and belief functions},
	Volume = {42},
	Year = {2009}}

@article{Come2009g,
	Author = {C\^ome, E. and Oukhellou, L. and Den{\oe}ux, T. and Aknin, P.},
	Journal = {Pattern recognition},
	Pages = {334-348},
	Title = {Learning form partially supervised data using mixture models and belief functions},
	Volume = {42},
	Year = {2009}}

@article{Caillol1997,
	Author = {H. Caillol and W. Pieczynski and A. Hillion},
	Bibsource = {DBLP, http://dblp.uni-trier.de},
	Ee = {http://dx.doi.org/10.1109/83.557353},
	Journal = {IEEE Transactions on Image Processing},
	Number = {3},
	Pages = {425-440},
	Title = {Estimation of fuzzy Gaussian mixture and unsupervised statistical image segmentation},
	Volume = {6},
	Year = {1997}}

@article{Cappe2009,
	Archiveprefix = {arXiv},
	Author = {Capp{\'e}, O.},
	Journal = {preprint},
	Month = {August},
	Title = {Online EM Algorithm for Hidden Markov Models},
	Url = {http://arxiv.org/abs/0908.2359},
	Year = {2009},
	Bdsk-Url-1 = {http://arxiv.org/abs/0908.2359}}

@article{Cappe2009d,
	Author = {Capp{\'e}, O. and Moulines, E.},
	Journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	Month = {June},
	Number = {3},
	Pages = {593--613},
	Title = {On-line expectation-maximization algorithm for latent data models},
	Volume = {71},
	Year = {2009}}

@book{Cappe2005,
	Author = {Capp{\'e}, O. and Moulines, E. and Ryd{\'e}n, T.},
	Month = {July},
	Publisher = {Springer},
	Series = {Springer Series in Statistics},
	Title = {Inference in Hidden Markov Models},
	Year = {2005}}

@article{Cardoso1999,
	Author = {Cardoso, J. F.},
	Journal = {Neural Computation},
	Number = {1},
	Pages = {157--192},
	Title = {High-order contrasts for independent component analysis},
	Volume = {11},
	Year = {1999}}

@article{cardoso99,
	Author = {Cardoso, J. F.},
	Journal = {Neural Computation},
	Number = {1},
	Pages = {157--192},
	Title = {High-order contrasts for independent component analysis},
	Volume = {11},
	Year = {1999}}

@article{Cardoso1997,
	Author = {Cardoso, J. F.},
	Journal = {IEEE Letters on Signal Processing},
	Number = {4},
	Pages = {112-114},
	Title = {Infomax and maximum likelihood for source separation},
	Volume = {4},
	Year = {1997}}

@article{cardoso97,
	Author = {Cardoso, J. F.},
	Journal = {IEEE Letters on Signal Processing},
	Number = {4},
	Pages = {112-114},
	Title = {Infomax and maximum likelihood for source separation},
	Volume = {4},
	Year = {1997}}

@article{Cardoso1996,
	Author = {Cardoso, J. F. and Laheld, B.},
	Citeulike-Article-Id = {2444303},
	Journal = {IEEE Transactions on Signal Processing},
	Keywords = {ica},
	Number = {12},
	Pages = {3017--3030},
	Posted-At = {2008-02-28 17:41:25},
	Priority = {0},
	Title = {Equivariant adaptive source separation},
	Volume = {44},
	Year = {1996}}

@article{cardoso96,
	Author = {Cardoso, J. F. and Laheld, B.},
	Citeulike-Article-Id = {2444303},
	Journal = {IEEE Transactions on Signal Processing},
	Keywords = {ica},
	Number = {12},
	Pages = {3017--3030},
	Posted-At = {2008-02-28 17:41:25},
	Priority = {0},
	Title = {Equivariant adaptive source separation},
	Volume = {44},
	Year = {1996}}

@article{Carvalho2007,
	Author = {Carvalho, A. X. and Tanner, M. A.},
	Journal = {Computational Statistics and Data Analysis},
	Number = {11},
	Pages = {5266--5294},
	Title = {Modelling nonlinear count time series with local mixtures of Poisson autoregressions},
	Volume = {51},
	Year = {2007}}

@article{Carvalho2006,
	Author = {Carvalho, A. X. and Tanner, M. A.},
	Journal = {International journal of mathematics and mathematical sciences},
	Number = {6},
	Pages = {1-22},
	Title = {Modeling nonlinearities with mixtures-of-experts of time series models},
	Volume = {2006},
	Year = {2006}}
	
@TechReport{Celeux99LabelSwitchingTechnicalReport,
 author = {G. Celeux},
    title = {{Bayesian inference for mixture: the label switching problem}},
institution = {INRIA Rhone-Alpes},
 year = {1999}
 }
   

@ARTICLE{Celeux99TechnicalReport,
    author = {Gilles Celeux and Merrilee Hurn and Christian P. Robert},
    title = {{Computational and Inferential Difficulties With Mixture Posterior Distributions}},
    journal = {Journal of the American Statistical Association},
    year = {1999},
    volume = {95},
    pages = {957--970}
}

@techreport{Celeux1995,
	Author = {Celeux, G. and Chauveau, D. and Diebolt, J.},
	Institution = {The French National Institute for Research in Computer Science and Control (INRIA)},
	Number = {RR-2514},
	Title = {On Stochastic Versions of the {EM} Algorithm},
	Year = {1995}}

@article{Celeux1985,
	Author = {Celeux, G. and Diebolt, J.},
	Journal = {Computational Statistics Quarterly},
	Number = {1},
	Pages = {73-82},
	Title = {The {SEM} algorithm a probabilistic teacher algorithm derived from the {EM} algorithm for the mixture problem},
	Volume = {2},
	Year = {1985}}

@article{Celeux2004,
	Author = {Celeux, G. and Nascimento, J.C. and Marques, J.S.},
	Journal = {PR},
	Month = {September},
	Number = {9},
	Pages = {1841--1853},
	Title = {Learning switching dynamic models for objects tracking},
	Volume = {37},
	Year = {2004}}

@inproceedings{chamroukhi-ijcnn2013,
	Address = {Dallas, Texas},
	Author = {F. Chamroukhi},
	Booktitle = {Proceedings of the International Joint Conference on Neural Networks (IJCNN), IEEE},
	Month = {August},
	Pages = {1--8},
	Title = {Robust {EM} algorithm for model-based curve clustering},
	Year = {2013}}

@article{FigueiredoUnsupervisedlearningMixtures,
	Author = {M\'ario A. T. Figueiredo and Anil K. Jain},
	Journal = {IEEE Transactions Pattern Analysis and Machine Intelligence},
	Pages = {381--396},
	Title = {Unsupervised Learning of Finite Mixture Models},
	Volume = {24},
	Year = {2000}}

@article{Robust-EM-GMM,
	Address = {New York, NY, USA},
	Author = {Yang, Miin-Shen and Lai, Chien-Yo and Lin, Chih-Ying},
	Issn = {0031-3203},
	Journal = {Pattern Recognition},
	Keywords = {Cluster analysis, EM algorithm, Gaussian mixture model, Initialization, Number of clusters, Robust EM},
	Month = nov,
	Number = {11},
	Numpages = {12},
	Pages = {3950--3961},
	Publisher = {Elsevier Science Inc.},
	Title = {{A robust EM clustering algorithm for Gaussian mixture models}},
	Volume = {45},
	Year = {2012}}

@phdthesis{Chamroukhi2010,
	Address = {Compi\`egne, France},
	Author = {Chamroukhi, F.},
	School = {Universit\'e de Technologie de Compi\`egne},
	Title = {Hidden process regression for curve modeling, classification and tracking},
	Type = {Ph.{D}. Thesis},
	Year = {2010}}

@inproceedings{chamroukhi-ijcnn2012,
	Adress = {Brisbane, Australia},
	Author = {Chamroukhi, F. and Glotin, H.},
	Booktitle = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
	Month = {June},
	Title = {Mixture model-based functional discriminant analysis for curve classification},
	Year = {2012}}

@inproceedings{Chamroukhi2012,
	Address = {Bruges, Belgium},
	Author = {F. Chamroukhi and H. Glotin and C. Rabouy},
	Booktitle = {Proceedings of XXth European Symposium on Artificial Neural Networks ESANN},
	Month = {April},
	Pages = {281-286},
	Title = {Functional Mixture Discriminant Analysis with hidden process regression for curve classification},
	Year = {2012}}

@article{chamroukhi_fmda_neucomp2013,
	Author = {Chamroukhi, F. and Glotin, H. and Sam{\'e}, A.},
	Journal = {Neurocomputing},
	Pages = {153-163},
	Title = {Model-based functional mixture discriminant analysis with hidden process regression for curve classification},
	Volume = {112},
	Year = {2013}}

@inproceedings{Chamroukhi2008,
	Address = {Derby, UK},
	Author = {Chamroukhi, F. and Sam\'e, A. and Aknin, P.},
	Booktitle = {The 4th IET International Conference on Railway Condition Monitoring RCM},
	Issn = {0537-9989},
	Month = {June},
	Pages = {1-4},
	Title = {Switch mechanism diagnosis using a pattern recognition approach},
	Year = {2008}}

@inproceedings{Chamroukhi2011,
	Author = {Chamroukhi, F. and Sam\'e, A. and P. Aknin and Govaert, G.},
	Booktitle = {Proceedings of the IEEE International Joint Conference on Neural Networks (IJCNN)},
	Pages = {2814-2821},
	Title = {{Model-based clustering with Hidden Markov Model regression for time series with regime changes}},
	Year = {2011}}

@inproceedings{Chamroukhi-sfds-2012,
	Author = {Chamroukhi, F. and Sam\'e, A. and Govaert, G. and P. Aknin},
	Booktitle = {44\`emes Journ\'ees de Statistique de la SFdS},
	Title = {Classification automatique de donn\'ees temporelles en classes ordonn\'ees},
	Year = {2012}}

@article{chamroukhi-et-al-rnti2011,
	Author = {Chamroukhi, F. and Sam\'e, A. and Govaert, G. and Aknin, P.},
	Journal = {Revue des Nouvelles Technologies de l'Information (RNTI)},
	Month = {Jan},
	Pages = {15--32},
	Title = {Mod\`ele \`a processus latent et algorithme {EM} pour la r\'egression non lin\'eaire},
	Volume = {S1},
	Year = {2011}}

@article{Chamroukhi2011a,
	Author = {Chamroukhi, F. and Sam\'e, A. and Govaert, G. and Aknin, P.},
	Journal = {Revue des Nouvelles Technologies de l'Information (RNTI)},
	Month = {Jan},
	Pages = {15--32},
	Title = {Mod\`ele \`a processus latent et algorithme {EM} pour la r\'egression non lin\'eaire},
	Volume = {S1},
	Year = {2011}}

@article{chamroukhi-et-al-neucomp2013b,
	Author = {F. Chamroukhi and D. Trabelsi and S. Mohammed and L. Oukhellou and Y. Amirat},
	Journal = {Neurocomputing},
	Month = {November},
	Pages = {633--644},
	Publisher = {Elsevier},
	Title = {Joint segmentation of multivariate time series with hidden process regression for human activity recognition},
	Volume = {120},
	Year = {2013}}

@article{Chamroukhi-NN2009,
	Address = {Oxford, UK, UK},
	Author = {Chamroukhi, F. and Sam\'{e}, A. and Govaert, G. and Aknin, P.},
	Date-Added = {2014-10-22 20:08:41 +0000},
	Date-Modified = {2014-10-22 20:08:41 +0000},
	Journal = {Neural Networks},
	Note = {\href{http://chamroukhi.univ-tln.fr/papers/Chamroukhi_Neural_Networks_2009.pdf}{PDF}},
	Number = {5-6},
	Pages = {593--602},
	Publisher = {Elsevier Science Ltd.},
	Title = {Time series modeling by a regression approach based on a latent process},
	Volume = {22},
	Year = {2009}}

@inproceedings{Chemudugunta2006,
	Author = {Chemudugunta, C. and Smyth, P. and Steyvers, M.},
	Booktitle = {Advances in Neural Information Processing Systems},
	Title = {Modeling General and Specific Aspects of Documents with a Probabilistic Topic Model},
	Year = {2006}}

@inproceedings{Chemudugunta2006a,
	Author = {Chemudugunta, C. and Smyth, P. and Steyvers, M.},
	Booktitle = {Advances in Neural Information Processing Systems},
	Title = {Modeling General and Specific Aspects of Documents with a Probabilistic Topic Model},
	Year = {2006}}

@inproceedings{Chemudugunta2006b,
	Author = {Chemudugunta, C. and Smyth, P. and Steyvers, M.},
	Booktitle = {Advances in Neural Information Processing Systems},
	Title = {Modeling General and Specific Aspects of Documents with a Probabilistic Topic Model},
	Year = {2006}}

@inproceedings{Chemudugunta2006c,
	Author = {Chemudugunta, C. and Smyth, P. and Steyvers, M.},
	Booktitle = {Advances in Neural Information Processing Systems},
	Title = {Modeling General and Specific Aspects of Documents with a Probabilistic Topic Model},
	Year = {2006}}

@article{Chen1999,
	Address = {Oxford, UK, UK},
	Author = {Chen, K. and Xu, L. and Chi, H.},
	Journal = {Neural Networks},
	Number = {9},
	Pages = {1229-1252},
	Publisher = {Elsevier Science Ltd.},
	Title = {Improved learning algorithms for mixture of experts in multiclass classification},
	Volume = {12},
	Year = {1999}}

@article{comon94,
	Author = {Comon, P.},
	Journal = {Signal Processing},
	Note = {Special issue on Higher-Order Statistics},
	Number = {3},
	Pages = {287--314},
	Title = {Independent {C}omponent {A}nalysis, a new concept~?},
	Volume = {36},
	Year = {1994}}

@article{cover,
	Author = {Cover, T.M.},
	Journal = {IEEE Transactions on Electronic Computers},
	Pages = {326--334},
	Title = {Geometrical and Statistical properties of systems of linear inequalities with applications in pattern recognition},
	Volume = {14},
	Year = {1965}}

@article{Cover1965,
	Author = {Cover, T.M.},
	Journal = {IEEE Transactions on Electronic Computers},
	Pages = {326--334},
	Title = {Geometrical and Statistical properties of systems of linear inequalities with applications in pattern recognition},
	Volume = {14},
	Year = {1965}}

@book{Cover2006,
	Author = {T. M. Cover and Joy A. T.},
	Edition = {Second Edition},
	Publisher = {Wiley Interscience},
	Title = {Elements of Information Theory},
	Year = {2006}}

@book{Cover2006a,
	Author = {T. M. Cover and Joy A. T.},
	Edition = {Second Edition},
	Publisher = {Wiley Interscience},
	Title = {Elements of Information Theory},
	Year = {2006}}

@book{Cover2006b,
	Author = {T. M. Cover and Joy A. T.},
	Edition = {Second Edition},
	Publisher = {Wiley Interscience},
	Title = {Elements of Information Theory},
	Year = {2006}}

@book{Cover2006c,
	Author = {T. M. Cover and Joy A. T.},
	Edition = {Second Edition},
	Publisher = {Wiley Interscience},
	Title = {Elements of Information Theory},
	Year = {2006}}

@article{Cox2003,
	Author = {Cox, T. F. and Cox, M. A. A. and Raton, B.},
	Journal = {Technometrics},
	Keywords = {multidimensional, scaling},
	Month = {May},
	Number = {2},
	Pages = {182},
	Title = {Multidimensional Scaling},
	Volume = {45},
	Year = {2003}}

@article{Dabo-Niang2007,
	Author = {Sophie Dabo-Niang and Fr{\'e}d{\'e}ric Ferraty and Philippe Vieu},
	Journal = {Computational Statistics \& Data Analysis},
	Number = {10},
	Pages = {4878 - 4890},
	Title = {On the using of modal curves for radar waveforms classification},
	Volume = {51},
	Year = {2007}}

@phdthesis{Dang1998,
	Author = {Dang, M. V.},
	School = {Universit{\'e} de Technologie de Compi\`{e}gne},
	Title = {Classification de Donn{\'e}es Spatiales : Mod\`{e}les Probabilistes et Crit\`{e}res de Partitionnement},
	Year = {1998}}

@article{Day1969,
	Author = {N. Day},
	Journal = {Biometrica},
	Pages = {463-474},
	Title = {Estimation of components of a mixture of normal distribution},
	Volume = {56},
	Year = {1969}}

@article{EstimationCompforMND1969,
	Author = {N. Day},
	Journal = {Biometrica},
	Pages = {463-474},
	Title = {Estimation of components of a mixture of normal distribution},
	Volume = {56},
	Year = {1969}}

@book{DeGroot1970,
	Author = {M. DeGroot},
	Publisher = {Wiley Classics Library},
	Title = {Optimal Statistics Decisions},
	Year = {1970}}

@book{OSDGroot-Book1970,
	Author = {M. DeGroot},
	Publisher = {Wiley Classics Library},
	Title = {Optimal Statistics Decisions},
	Year = {1970}}

@article{Delaigle2012a,
	Author = {Delaigle, A. and Hall, P. and Bathia, N.},
	Journal = {Biometrika},
	Optnumber = {2},
	Optpages = {299-313},
	Optvolume = {99},
	Title = {Componentwise classification and clustering of functional data},
	Year = {2012}}

@article{Delaigle2012b,
	Author = {Delaigle, A. and Hall, P. and Bathia, N.},
	Journal = {Biometrika},
	Number = {2},
	Pages = {299-313},
	Title = {Componentwise classification and clustering of functional data},
	Volume = {99},
	Year = {2012}}

@article{Delaigle2012c,
	Author = {Delaigle, A. and Hall, P. and Bathia, N.},
	Journal = {Biometrika},
	Number = {2},
	Pages = {299-313},
	Title = {Componentwise classification and clustering of functional data},
	Volume = {99},
	Year = {2012}}

@article{Dempster1977,
	Author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
	Journal = {Journal of The Royal Statistical Society, B},
	Pages = {1-38},
	Title = {Maximum likelihood from incomplete data via the {EM} algorithm},
	Volume = {39(1)},
	Year = {1977}}

@inproceedings{Derrode2006a,
	Address = {Toulouse},
	Author = {Derrode, S. Benyoussef, L. and W. Pieczynski},
	Booktitle = {ICASSP},
	Month = {May},
	Pages = {15--19},
	Title = {Contextual estimation of hidden Markov chains with application to image segmentation},
	Year = {2006}}

@inproceedings{Derrode2006b,
	Address = {Toulouse},
	Author = {Derrode, S. Benyoussef, L. and W. Pieczynski},
	Booktitle = {ICASSP},
	Month = {May},
	Pages = {15--19},
	Title = {Contextual estimation of hidden Markov chains with application to image segmentation},
	Year = {2006}}

@inproceedings{Derrode2006c,
	Address = {Toulouse},
	Author = {Derrode, S. Benyoussef, L. and W. Pieczynski},
	Booktitle = {ICASSP},
	Month = {May},
	Pages = {15--19},
	Title = {Contextual estimation of hidden Markov chains with application to image segmentation},
	Year = {2006}}

@inproceedings{Derrode2006d,
	Address = {Toulouse},
	Author = {Derrode, S. Benyoussef, L. and W. Pieczynski},
	Booktitle = {ICASSP},
	Month = {May},
	Pages = {15--19},
	Title = {Contextual estimation of hidden Markov chains with application to image segmentation},
	Year = {2006}}

@article{Diebold1994,
	Address = {Oxford},
	Author = {Diebold, F.X. and Lee, J.-H. and Weinbach, G.},
	Editor = {C.W.J. Granger and G. Mizon, eds.},
	Journal = {Nonstationary Time Series Analysis and Cointegration. (Advanced Texts in Econometrics)},
	Pages = {283--302},
	Publisher = {Oxford University Press},
	Title = {Regime Switching with Time-Varying Transition Probabilities},
	Year = {1994}}

@book{Dubuisson1990,
	Author = {Dubuisson, B.},
	Publisher = {Herm\`es},
	Serie = {Trait\'e des nouvelles thechnologies},
	Title = {Diagnostic et Reconnaissance des formes},
	Year = {1990}}

@book{Duda1973,
	Author = {Duda, R. O. and Hart, P. E.},
	Howpublished = {Hardcover},
	Publisher = {John Wiley \& Sons Inc},
	Title = {Pattern Classification and Scene Analysis},
	Year = {1973}}

@book{Duda1973a,
	Author = {Duda, R. O. and Hart, P. E.},
	Howpublished = {Hardcover},
	Publisher = {John Wiley \& Sons Inc},
	Title = {Pattern Classification and Scene Analysis},
	Year = {1973}}

@book{Duda1973b,
	Author = {Duda, R. O. and Hart, P. E.},
	Howpublished = {Hardcover},
	Publisher = {John Wiley \& Sons Inc},
	Title = {Pattern Classification and Scene Analysis},
	Year = {1973}}

@book{Duda1973c,
	Author = {Duda, R. O. and Hart, P. E.},
	Howpublished = {Hardcover},
	Publisher = {John Wiley \& Sons Inc},
	Title = {Pattern Classification and Scene Analysis},
	Year = {1973}}

@article{Fearnhead-2006,
	Author = {Fearnhead, Paul},
	Journal = {Statistics and Computing},
	Title = {Exact and efficient {B}ayesian inference for multiple changepoint problems},
	Year = {2006}}

@article{Fearnhead-2007-jstor,
	Author = {P. Fearnhead and Z. Liu},
	Journal = {Journal of the Royal Statistical Society, Series B},
	Title = {{O}nline {I}nference for {M}ultiple {C}hangepoint {P}roblems},
	Year = {2007}}

@article{Fearnhead2007,
	Author = {P. Fearnhead and Z. Liu},
	Journal = {Journal of the Royal Statistical Society, Series B},
	Pages = {589--605},
	Title = {{O}nline {I}nference for {M}ultiple {C}hangepoint {P}roblems},
	Volume = {69},
	Year = {2007}}

@article{Ferrari-Trecate2002,
	Author = {Ferrari-Trecate, G. and Muselli, M. and Liberati, D. and Morari, M.},
	Journal = {Automatica},
	Pages = {205-217},
	Title = {A clustering technique for the identification of piecewise affine and hybrid systems},
	Volume = {39},
	Year = {2002}}

@article{Ferraty2003,
	Author = {F. Ferraty and P. Vieu},
	Date-Modified = {2015-08-21 21:41:18 +0000},
	Journal = {Computational Statistics {\&} Data Analysis},
	Number = {1-2},
	Pages = {161-173},
	Title = {Curves discrimination: a nonparametric functional approach},
	Volume = {44},
	Year = {2003}}

@article{Fisher1958,
	Author = {Fisher, W. D.},
	Journal = {Journal of the American Statistical Association},
	Pages = {789-798},
	Title = {On grouping for maximum homogeneity},
	Volume = {53},
	Year = {1958}}

@article{Forney1973,
	Abstract = {The Viterbi algorithm (VA) is a recursive optimal solution to the problem of estimating the state sequence of a discrete-time finite-state Markov process observed in memoryless noise. Many problems in areas such as digital communications can be cast in this form. This paper gives a tutorial exposition of the algorithm and of how it is implemented and analyzed. Applications to date are reviewed. Increasing use of the algorithm in a widening variety of areas is foreseen.},
	Author = {Forney, G. D.},
	Journal = {Proceedings of the IEEE},
	Number = {3},
	Pages = {268--278},
	Title = {The viterbi algorithm},
	Volume = {61},
	Year = {1973}}

@article{Fort2003,
	Author = {G. Fort and E. Moulines},
	Journal = {Annals Statistics},
	Number = {4},
	Pages = {1220--1259},
	Title = {Convergence of the Monte Carlo expectation maximization for curved exponential families},
	Volume = {31},
	Year = {2003}}

@article{Fox2011,
	Author = {Emily B. Fox and Erik B. Sudderth and Michael I. Jordan and Alan S. Willsky},
	Journal = {IEEE Transactions on Signal Processing},
	Number = {4},
	Pages = {1569-1585},
	Title = {Bayesian Nonparametric Inference of Switching Dynamic Linear Models},
	Volume = {59},
	Year = {2011}}

@inproceedings{Fox2008,
	Author = {Emily B. Fox and Erik B. Sudderth and Michael I. Jordan and Alan S. Willsky},
	Booktitle = {NIPS},
	Pages = {457-464},
	Title = {Nonparametric Bayesian Learning of Switching Linear Dynamical Systems},
	Year = {2008}}

@article{FraleyAndRaftery-2007,
	Author = {Fraley, C. and Raftery, A. E.},
	Date-Modified = {2015-08-21 21:49:26 +0000},
	Journal = {Journal of Classification},
	Number = {2},
	Pages = {155--181},
	Title = {Bayesian Regularization for Normal Mixture Estimation and Model-Based Clustering},
	Volume = {24},
	Year = {2007}}

@techreport{FraleyAndRaftery-2005,
	Author = {Chris Fraley and Adrian E. Raftery},
	Date-Modified = {2015-08-21 21:48:53 +0000},
	Institution = {Departament of Statistics, Box 354322, University of Washington Seattle, WA 98195-4322 USA},
	Month = {August},
	Number = {486},
	Title = {Bayesian Regularization for Normal Mixture Estimation and Model-Based Clustering},
	Year = {2005}}

@techreport{Fridman1993,
	Author = {Fridman, M.},
	Institution = {Institute of mathematics, University of Minnesota},
	Title = {Hidden Markov Model Regression},
	Year = {1993}}

@inproceedings{GaffneyANDsmythNIPS2004,
	Author = {S. J. Gaffney and P. Smyth},
	Booktitle = {In Advances in Neural Information Processing Systems (NIPS)},
	Date-Modified = {2015-08-21 21:53:18 +0000},
	Title = {Joint probabilistic curve clustering and alignment},
	Year = {2004}}

@article{Gauchi2010,
	Author = {Gauchi,J-P and Vila, J-P and Coroller, L},
	Journal = {Communications in Statistics - Simulation and Computation},
	Pages = {322--334},
	Title = {New prediction interval and band in the nonlinear regression model : applications to predictive modeling in foods},
	Volume = {39},
	Year = {2010}}

@article{Gauvain1994d,
	Author = {Jean-luc Gauvain and Chin-hui Lee},
	Journal = {IEEE Transactions on Speech and Audio Processing},
	Number = {1},
	Pages = {291--298},
	Title = {Maximum A Posteriori Estimation for Multivariate Gaussian Mixture Observations of Markov Chains},
	Volume = {2},
	Year = {1994}}

@article{Gauvain1994,
	Author = {J-L Gauvain and C-H Lee},
	Journal = {IEEE Transactions on Speech and Audio Processing},
	Month = {April},
	Number = {2},
	Pages = {291-298},
	Title = {Maximum a Posteriori Estimation for Multivariate Gaussian Mixture Observations of Markov Chains},
	Volume = {2},
	Year = {1994}}

@article{Gauvain1992,
	Author = {J-L Gauvain and C-H Lee},
	Journal = {Speech Communication},
	Number = {2-3},
	Pages = {205-213},
	Title = {Bayesian learning for hidden Markov model with Gaussian mixture state observation densities},
	Volume = {11},
	Year = {1992}}

@article{Gelfand2002,
	Author = {Gelfand, A. and Kottas, A.},
	Journal = {Journal of Computational and Graphical Statistics},
	Pages = {289-305},
	Title = {A computational approach for full nonparametric bayesian inference under dirichlet process mixture model},
	Volume = {11},
	Year = {2002}}

@article{Green1984,
	Author = {Green, P.},
	Journal = {Journal of The Royal Statistical Society, B},
	Number = {2},
	Pages = {149-192},
	Title = {Iteratively Reweighted Least Squares for Maximum Likelihood Estimation, and some robust and resistant alternatives},
	Volume = {46},
	Year = {1984}}

@article{Harrison2003,
	Author = {Harrison, L. and Penny, W. D. and Friston, K.},
	Journal = {Neuroimage},
	Month = {August},
	Number = {4},
	Pages = {1477--1491},
	Title = {Multivariate autoregressive modeling of fMRI time series.},
	Volume = {19},
	Year = {2003}}

@article{HarrissonANDpenny2003,
	Author = {Harrison, L. and Penny, W. D. and Friston, K.},
	Journal = {Neuroimage},
	Month = {August},
	Number = {4},
	Pages = {1477--1491},
	Title = {Multivariate autoregressive modeling of fMRI time series.},
	Volume = {19},
	Year = {2003}}

@article{Hastie1996,
	Author = {Hastie, T. and Tibshirani, R.},
	Journal = {Journal of the Royal Statistical Society, B},
	Pages = {155-176},
	Title = {{Discriminant Analysis by Gaussian Mixtures}},
	Volume = {58},
	Year = {1996}}

@article{Hastie1996a,
	Author = {Hastie, T. and Tibshirani, R.},
	Journal = {Journal of the Royal Statistical Society, B},
	Pages = {155-176},
	Title = {Discriminant Analysis by Gaussian Mixtures},
	Volume = {58},
	Year = {1996}}

@book{Hastie2010,
	Author = {Hastie, T. and Tibshirani, R. and Friedman, J.},
	Edition = {Second Edition},
	Month = {January},
	Publisher = {Springer},
	Address={New York},
	Series = {Springer Series in Statistics},
	Title = {The Elements of Statistical Learning, Second Edition: Data Mining, Inference, and Prediction},
	Year = {2010}}

@book{Hjort2010,
	Author = {Hjort, N. and Holmes, C. and Muller, P. and Waller, S. G.},
	Publisher = {Cambrige University Press},
	Title = {Bayesian Non Parametrics: Principles and practice},
	Year = {2010}}

@book{polya-urn-models-mahmoud,
	Author = {Hosam, M.},
	Pages = {308},
	Publisher = {CRC Press / Chapman and Hall},
	Title = {Polya Urn Models},
	Year = {209}}

@book{applied-logreg-book,
	Author = {Hosmer, D. W. and Lemeshow, S.},
	Howpublished = {Hardcover},
	Keywords = {logistic regression, statistics},
	Month = {September},
	Publisher = {{Wiley-Interscience Publication}},
	Title = {Applied logistic regression (Wiley Series in probability and statistics)},
	Year = {2000}}

@article{Hotelling1933,
	Author = {Hotelling, H.},
	Journal = {Journal of Educational Psychology},
	Pages = {417--441},
	Title = {Analysis of a complex of statistical variables into principal components},
	Volume = {24},
	Year = {1933}}

@inproceedings{Hugueney2009,
	Address = {Bruges, Belgium},
	Author = {Hugueney, B. and H\'ebrail, G. and Lechevallier, Y. and Rossi, F.},
	Booktitle = {Proceedings of XVIIth European Symposium on Artificial Neural Networks (ESANN 2009)},
	Month = {April},
	Pages = {281--286},
	Title = {Simultaneous Clustering and Segmentation for Functional Data},
	Year = {2009}}

@book{Hyvaerinen2001,
	Author = {Hyv\"{a}rinen, A.},
	Citeulike-Article-Id = {819596},
	Keywords = {ica},
	Posted-At = {2006-08-28 09:06:01},
	Priority = {0},
	Publisher = {Wiley},
	Title = {Independant Component Analysis},
	Year = {2001}}

@book{hyvarinen01,
	Author = {Hyv\"{a}rinen, A.},
	Citeulike-Article-Id = {819596},
	Keywords = {ica},
	Posted-At = {2006-08-28 09:06:01},
	Priority = {0},
	Publisher = {Wiley},
	Title = {Independant Component Analysis},
	Year = {2001}}

@article{Hyvaerinen1999,
	Author = {Hyv\"{a}rinen, A.},
	Journal = {IEEE, Transaction on Neural Networks},
	Number = {3},
	Title = {Fast and Robust fixed point algorithms for independant component analysis},
	Volume = {10},
	Year = {1999}}

@article{hyvarinen99,
	Author = {Hyv\"{a}rinen, A.},
	Journal = {IEEE, Transaction on Neural Networks},
	Number = {3},
	Title = {Fast and Robust fixed point algorithms for independant component analysis},
	Volume = {10},
	Year = {1999}}

@inproceedings{Isermann2004,
	Author = {Isermann, R.},
	Booktitle = {In Proceedings of the 16th IFAC Symposium on Automatic Control in Aerospace, St},
	Pages = {71--85},
	Title = {Model-based fault detection and diagnosis: status and applications},
	Year = {2004}}

@inproceedings{Isermann2004a,
	Author = {Isermann, R.},
	Booktitle = {In Proceedings of the 16th IFAC Symposium on Automatic Control in Aerospace, St},
	Pages = {71--85},
	Title = {Model-based fault detection and diagnosis: status and applications},
	Year = {2004}}

@inproceedings{Isermann2004b,
	Author = {Isermann, R.},
	Booktitle = {In Proceedings of the 16th IFAC Symposium on Automatic Control in Aerospace, St},
	Pages = {71--85},
	Title = {Model-based fault detection and diagnosis: status and applications},
	Year = {2004}}

@inproceedings{Isermann2004c,
	Author = {Isermann, R.},
	Booktitle = {In Proceedings of the 16th IFAC Symposium on Automatic Control in Aerospace, St},
	Pages = {71--85},
	Title = {Model-based fault detection and diagnosis: status and applications},
	Year = {2004}}

@article{Isermann1984,
	Author = {Isermann, R.},
	Journal = {Automatica},
	Number = {4},
	Pages = {387--404},
	Title = {Process fault detection based on modeling and estimation methods},
	Volume = {20},
	Year = {1984}}

 @article{ShiMT05-Hierarchical-GPR,
	Author = {Shi, J. Q. and  Murray-Smith, R. and Titterington, D. M.},
	Journal = {Statistics and Computing},
	Number = {1},
	Pages = {31-41},
	Title = {Hierarchical Gaussian process mixtures for regression},
	Volume = {15},
	Year = {2005}}

@article{James2002,
	Author = {G. M. James},
	Journal = {Journal of the Royal Statistical Society Series B},
	Pages = {411--432},
	Title = {Generalized Linear Models with Functional Predictor Variables},
	Volume = {64},
	Year = {2002}}

@article{James2001,
	Author = {G. M. James and T. J. Hastie},
	Journal = {Journal of the Royal Statistical Society Series B},
	Pages = {533--550},
	Title = {Functional Linear Discriminant Analysis for Irregularly Sampled Curves},
	Volume = {63},
	Year = {2001}}

@article{James2003,
	Author = {James, G. M. and Sugar, C.},
	Journal = {Journal of the American Statistical Association},
	Number = {462},
	Optpages = {397-408},
	Title = {Clustering for Sparsely Sampled Functional Data},
	Volume = {98},
	Year = {2003}}

@book{Jebara2003,
	Address = {Norwell, MA, USA},
	Author = {Jebara, T.},
	Publisher = {Kluwer Academic Publishers},
	Title = {Machine Learning: Discriminative and Generative (Kluwer International Series in Engineering and Computer Science)},
	Year = {2003}}

@phdthesis{Jebara2001,
	Author = {Jebara, T.},
	School = {Media Laboratory, MIT},
	Title = {Discriminative, {G}enerative and {I}mitative learning},
	Type = {PhD thesis},
	Year = {2001}}

@inproceedings{Jebara1998,
	Author = {Jebara, T. and Pentland, A.},
	Booktitle = {Advances in Neural Information Processing Systems 11},
	Title = {Maximum conditional likelihood via bound maximization and the {CEM} algorithm},
	Year = {1998}}

@inproceedings{Jebara1998a,
	Author = {Jebara, T. and Pentland, A.},
	Booktitle = {Advances in Neural Information Processing Systems 11},
	Title = {Maximum conditional likelihood via bound maximization and the {CEM} algorithm},
	Year = {1998}}

@inproceedings{Jebara1998b,
	Author = {Jebara, T. and Pentland, A.},
	Booktitle = {Advances in Neural Information Processing Systems 11},
	Title = {Maximum conditional likelihood via bound maximization and the {CEM} algorithm},
	Year = {1998}}

@inproceedings{Jebara1998c,
	Author = {Jebara, T. and Pentland, A.},
	Booktitle = {Advances in Neural Information Processing Systems 11},
	Title = {Maximum conditional likelihood via bound maximization and the {CEM} algorithm},
	Year = {1998}}

@inproceedings{Jebara2007,
	Author = {Jebara, T. and Song, Y. and Thadani, K.},
	Booktitle = {Machine Learning: ECML 2007},
	Pages = {164--175},
	Title = {Spectral Clustering and Embedding with Hidden Markov Models},
	Year = {2007}}

@article{Jensen1906,
	Author = {Jensen, J. L. W. V.},
	Journal = {Acta Mathematica},
	Number = {1},
	Pages = {175-193},
	Title = {Sur les fonctions convexes et les in\'egalit\'es entre les valeurs moyennes},
	Volume = {30},
	Year = {1906}}

@incollection{Joachims1998,
	Author = {Thorsten Joachims},
	Booktitle = {Advances in Kernel Methods -- Support Vector Learning},
	Editor = {Bernhard Sch\"{o}lkopf and Chris Burges and Alex Smola},
	Pages = {169--185},
	Publisher = {MIT Press},
	Title = {Making large-scale SVM learning practical},
	Year = {1998}}

@article{Jordan1995,
	Author = {Jordan, M. I. and Xu, L.},
	Journal = {Neural Networks},
	Number = {9},
	Pages = {1409-1431},
	Title = {Convergence results for the {EM} approach to mixtures of experts architectures},
	Volume = {8},
	Year = {1995}}

@article{Jordan1999,
	Author = {Jordan, M. I. and Ghahramani, Z. and Jaakkola, T. and Saul, L. K.},
	Journal = {Machine Learning},
	Pages = {183-233},
	Title = {An introduction to variational methods for graphical models},
	Volume = {37},
	Year = {1999}}

@article{Juang1986,
	Author = {B.-H. Juang and S. E. Levinson and M. M. Sondhi},
	Journal = {IEEE Transactions on Information Theory},
	Month = {March},
	Number = {2},
	Pages = {307-309},
	Title = {Maximum likelihood estimation for multivariate mixture observations of Markov chains},
	Volume = {32},
	Year = {1986}}

@inproceedings{Juang1985d,
	Address = {Brighton, England},
	Author = {B.-H. Juang and S. E. Levinson and M. M. Sondhi},
	Booktitle = {IEEE International Symposium on Information Theory},
	Month = {June},
	Title = {Maximum likelihood estimation for mixture multivariate stochastic observations of Markov chains},
	Year = {1985}}

@inproceedings{Juang1985e,
	Address = {Brighton, England},
	Author = {B.-H. Juang and S. E. Levinson and M. M. Sondhi},
	Booktitle = {IEEE International Symposium on Information Theory},
	Month = {June},
	Title = {Maximum likelihood estimation for mixture multivariate stochastic observations of Markov chains},
	Year = {1985}}

@inproceedings{Juang1985f,
	Address = {Brighton, England},
	Author = {B.-H. Juang and S. E. Levinson and M. M. Sondhi},
	Booktitle = {IEEE International Symposium on Information Theory},
	Month = {June},
	Title = {Maximum likelihood estimation for mixture multivariate stochastic observations of Markov chains},
	Year = {1985}}

@inproceedings{Juang1985g,
	Address = {Brighton, England},
	Author = {B.-H. Juang and S. E. Levinson and M. M. Sondhi},
	Booktitle = {IEEE International Symposium on Information Theory},
	Month = {June},
	Title = {Maximum likelihood estimation for mixture multivariate stochastic observations of Markov chains},
	Year = {1985}}

@article{Juang1985,
	Author = {B.-H. Juang and L. R. Rabiner},
	Journal = {IEEE Transactions on Acoustics, Speech and Signal Processing},
	Month = {December},
	Number = {6},
	Pages = {1404-1413},
	Title = {Mixture autoregressive hidden Markov models for speech signals},
	Volume = {33},
	Year = {1985}}

@phdthesis{Kaski1997,
	Abstract = {Finding structures in vast multidimensional data sets, be they measurement data, statistics, or textual documents, is difficult and time-consuming. Interesting, novel relations between the data items may be hidden in the data. The selforganizing map (SOM) algorithm of Kohonen can be used to aid the exploration: the structures in the data sets can be illustrated on special map displays. In this work, the methodology of using SOMs for exploratory data analysis or data mining is reviewed and...},
	Author = {Kaski, Samuel},
	Keywords = {self-organizing kohonen maps, neural network},
	Publisher = {The Finnish Academy of Technology},
	School = {Acta Polytechnica Scandinavica, Mathematics, Computing and Management in Engineering},
	Title = {Data Exploration Using Self-Organizing Maps},
	Year = {1997}}

@article{Kenny1990,
	Author = {Kenny, P. and Lennig,M. and Mermelstein P.},
	Journal = {IEEE Transactions on Acoustics, Speech and Signal Processing},
	Number = {2},
	Pages = {220--225,},
	Title = {A linear predictive HMM for vector-valued observations with applications to speech recognition},
	Volume = {38},
	Year = {1990}}

@inproceedings{Kivinen2007,
	Author = {J. J. Kivinen and E. B. Sudderth and M. I. Jordan},
	Booktitle = {ICIP (3)},
	Pages = {121-124},
	Title = {Image Denoising with Nonparametric Hidden Markov Trees},
	Year = {2007}}

@book{Kohonen2001,
	Author = {Kohonen, T.},
	Edition = {third Edition},
	Keywords = {neural-nets, som},
	Publisher = {Springer},
	Series = {Information Sciences},
	Title = {Self-Organizing Maps},
	Year = {2001}}

@book{Kohonen1989,
	Author = {Kohonen, T.},
	Publisher = {Springer-Verlag New York, Inc. New York, NY, USA},
	Title = {Self-organization and associative memory},
	Year = {1989}}

@book{Kohonen1989a,
	Author = {Kohonen, T.},
	Publisher = {Springer-Verlag New York, Inc. New York, NY, USA},
	Title = {Self-organization and associative memory},
	Year = {1989}}

@book{Kohonen1989b,
	Author = {Kohonen, T.},
	Publisher = {Springer-Verlag New York, Inc. New York, NY, USA},
	Title = {Self-organization and associative memory},
	Year = {1989}}

@book{Kohonen1989c,
	Author = {Kohonen, T.},
	Publisher = {Springer-Verlag New York, Inc. New York, NY, USA},
	Title = {Self-organization and associative memory},
	Year = {1989}}

@article{Kohonen1982,
	Abstract = {This work contains a theoretical study and computer simulations of a new self-organizing process. The principal discovery is that in a simple network of adaptive physical elements which receives signals from a primary event space, the signal representations are automatically mapped onto a set of output responses in such a way that the responses acquire the same topological order as that of the primary events. In other words, a principle has been discovered which facilitates the automatic formation of topologically correct maps of features of observable events. The basic self-organizing system is a one- or two-dimensional array of processing units resembling a network of threshold-logic units, and characterized by short-range lateral feedback between neighbouring units. Several types of computer simulations are used to demonstrate the ordering process as well as the conditions under which it fails.},
	Author = {Kohonen, Teuvo},
	Journal = {Biological Cybernetics},
	Keywords = {neuroscience, self-organization, som},
	Month = {January},
	Number = {1},
	Pages = {59--69},
	Posted-At = {2009-02-27 16:21:30},
	Priority = {5},
	Title = {Self-organized formation of topologically correct feature maps},
	Volume = {43},
	Year = {1982}}

@article{Kohonen2000,
	Abstract = {Describes the implementation of a system that is able to organize vast document collections according to textual similarities. It is based on the self-organizing map (SOM) algorithm. As the feature vectors for the documents statistical representations of their vocabularies are used. The main goal in our work has been to scale up the SOM algorithm to be able to deal with large amounts of high-dimensional data. In a practical experiment we mapped 6840568 patent abstracts onto a 1002240-node SOM. As the feature vectors we used 500-dimensional vectors of stochastic figures obtained as random projections of weighted word histograms},
	Author = {Kohonen, T. and Kaski, S. and Lagus, K. and Salojarvi, J. and Honkela, J. and Paatero, V. and Saarela, A.},
	Journal = {IEEE Transactions on Neural Networks},
	Number = {3},
	Pages = {574-585},
	Title = {Self organization of a massive document collection},
	Volume = {11},
	Year = {2000}}

@article{Kong1991,
	Abstract = {A comparison is made of a differential-competitive-learning (DCL) system with two supervised competitive-learning (SCL) systems for centroid estimation and for phoneme recognition. DCL provides a form of unsupervised adaptive vector quantization. Standard stochastic competitive-learning systems learn only if neurons win a competition for activation induced by randomly sampled patterns. DCL systems learn only if the competing neurons change their competitive signal. Signal-velocity information provides unsupervised local reinforcement during learning. The sign of the neuronal signal derivative rewards winners and punishes losers. Standard competitive learning ignores instantaneous win-rate information. Synaptic fan-in vectors adaptively quantize the randomly sampled pattern space into nearest-neighbor decision classes. More generally, the synaptic-vector distribution estimates the unknown sampled probability density function p(x). Simulations showed that unsupervised DCL-trained synaptic vectors converged to class centroids at least as fast as, and wandered less about these centroids than, SCL-trained synaptic vectors did. Simulations on a small set of English phonemes favored DCL over SCL for classification accuracy.},
	Author = {Kong, S. and Kosko, B.},
	Journal = {IEEE Transactions on Neural Networks},
	Number = {1},
	Pages = {118-124},
	Title = {Differential competitive learning for centroid estimation and phoneme recognition},
	Volume = {2},
	Year = {1991}}

@article{Krishnapuram2005,
	Author = {Krishnapuram, B. and Carin, L. and Figueiredo, M.A.T. and Hartemink,A.J.},
	Journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	Number = {6},
	Pages = {957-968},
	Title = {Sparse multinomial logistic regression: fast algorithms and generalization bounds},
	Volume = {27},
	Year = {2005}}

@article{JordanBK2012,
	Author = {Brian Kulis and Michael I. Jordan},
	Journal = {CoRR},
	Title = {Revisiting k-means: New Algorithms via Bayesian Nonparametrics},
	Volume = {abs/1111.0352},
	Year = {2011}}

@article{Kulis2011,
	Author = {Brian Kulis and Michael I. Jordan},
	Journal = {CoRR},
	Title = {Revisiting k-means: New Algorithms via Bayesian Nonparametrics},
	Volume = {abs/1111.0352},
	Year = {2011}}

@article{Lazaro-Gredilla2010,
	Author = {L\'{a}zaro-Gredilla, Miguel and Qui\~{n}onero-Candela, Joaquin and Rasmussen, Carl Edward and Figueiras-Vidal, An\'{\i}bal R.},
	Journal = {Journal of Machine Learning Research},
	Pages = {1865--1881},
	Title = {Sparse Spectrum Gaussian Process Regression},
	Volume = {11},
	Year = {2010}}

@article{Lange1995,
	Author = {Lange, K.},
	Journal = {Journal of the Royal Statistical Society, B},
	Pages = {425--437.},
	Title = {A gradient algorithm locally equivalent to the {EM} algorithm},
	Volume = {57},
	Year = {1995}}

@article{Lechevalier1990,
	Author = {Lechevalier, Y.},
	Journal = {Technical report, French National Institute for Research in Computer Science and Control (INRIA)},
	Title = {Optimal clustering on ordered set},
	Year = {1990}}

@article{smith-roberts-93,
	Author = {Smith A. F. M. and Roberts, G. O.},
	Journal = {Royal Statistical Society},
	Pages = {3-23},
	Series = {B,55},
	Title = {Bayesian Computation via the Gibbs sampler and related Markov chain Monte Carlo methods},
	Year = {1993}}

@inproceedings{MacQueen1967,
	Author = {MacQueen, J. B.},
	Booktitle = {Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability},
	Pages = {281--297},
	Title = {Some methods for classification and analysis of multivariate observations},
	Year = {1967}}

@article{Makarenkov1999,
	Author = {Makarenkov, V. and Legendre, P},
	Journal = {Math\'ematiques, informatique et sciences humaines},
	Title = {Une m\'ethode d'analyse canonique non-lin\'eaire et son application \`a des donn\'ees biologiques},
	Year = {1999}}

@article{McGee1970,
	Author = {McGee, V. E. and Carleton, W. T.},
	Journal = {Journal of the American Statistical Association},
	Pages = {1109-1124},
	Title = {Piecewise regression},
	Volume = {65},
	Year = {1970}}

@book{McLachlan1988,
	Author = {G. J. McLachlan and K. E. Basford},
	Publisher = {Marcel Dekker, New York},
	Title = {Mixture Models: Inference and Applications to Clustering},
	Year = {1988}}

@book{McLachlan1997,
	Author = {McLachlan, G. J. and Krishnan, T.},
	Publisher = {New York: Wiley},
	Title = {The EM algorithm and extensions},
	Year = {1997}}

@inproceedings{Meila1996,
	Author = {Meila, M. and Jordan, M. I.},
	Booktitle = {Advances in Neural Information Processing Systems 8},
	Pages = {1003--1009},
	Publisher = {MIT Press},
	Title = {Learning fine motion by Markov mixtures of experts},
	Year = {1996}}

@article{Meng1993,
	Author = {Meng, X. L. and Rubin, D. B.},
	Journal = {Biometrika},
	Keywords = {cem},
	Number = {2},
	Pages = {267--278},
	Title = {Maximum likelihood estimation via the {ECM} algorithm: A general framework},
	Volume = {80},
	Year = {1993}}

@article{Mingoti2006,
	Author = {Mingoti, Sueli A. and Lima, Joab O.},
	Journal = {European Journal of Operational Research},
	Month = {November},
	Number = {3},
	Pages = {1742-1759},
	Title = {Comparing SOM neural network with Fuzzy c-means, K-means and traditional hierarchical clustering algorithms},
	Volume = {174},
	Year = {2006}}

@article{Mingoti2006a,
	Author = {Mingoti, Sueli A. and Lima, Joab O.},
	Journal = {European Journal of Operational Research},
	Month = {November},
	Number = {3},
	Pages = {1742-1759},
	Title = {Comparing SOM neural network with Fuzzy c-means, K-means and traditional hierarchical clustering algorithms},
	Volume = {174},
	Year = {2006}}

@article{Mingoti2006b,
	Author = {Mingoti, Sueli A. and Lima, Joab O.},
	Journal = {European Journal of Operational Research},
	Month = {November},
	Number = {3},
	Pages = {1742-1759},
	Title = {Comparing SOM neural network with Fuzzy c-means, K-means and traditional hierarchical clustering algorithms},
	Volume = {174},
	Year = {2006}}

@article{Mingoti2006c,
	Author = {Mingoti, Sueli A. and Lima, Joab O.},
	Journal = {European Journal of Operational Research},
	Month = {November},
	Number = {3},
	Pages = {1742-1759},
	Title = {Comparing SOM neural network with Fuzzy c-means, K-means and traditional hierarchical clustering algorithms},
	Volume = {174},
	Year = {2006}}

@book{Mitchell1997,
	Address = {New York},
	Author = {Mitchell, T. M.},
	Keywords = {machine learning},
	Publisher = {McGraw-Hill},
	Title = {Machine Learning},
	Year = {1997}}

@article{Mongillo2008,
	Author = {Mongillo, G. and Deneve, S.},
	Journal = {Neural computation},
	Month = {July},
	Number = {7},
	Pages = {1706--1716},
	Title = {Online learning with hidden markov models},
	Volume = {20},
	Year = {2008}}

@phdthesis{Muri1997,
	Author = {Muri, F.},
	School = {Universit{\'e} Paris Descartes, Paris V},
	Title = {Comparaison d'algorithmes d'identification de cha\^{i}nes de Markov cach{\'e}es et application \`a la d{\'e}tection de r{\'e}gions homog\`{e}nes dans les s{\'e}quences ADN},
	Year = {1997}}

@phdthesis{Murphy2002,
	Author = {Murphy, K. P.},
	School = {UC Berkeley, Computer Science Division},
	Title = {Dynamic Bayesian Networks: Representation, Inference and Learning},
	Year = {2002}}

@techreport{Neal93-MCMC,
	Author = {Neal, R. M.},
	Institution = {Dept. of Computer Science, University of Toronto},
	Keywords = {MCMC, Gibbs, bayesian},
	Number = {CRG-TR-93-1},
	Pages = {144},
	Title = {{Probabilistic Inference Using Markov Chain Monte Carlo Methods}},
	Year = {1993}}

@article{meng_and_rubin_ECM_93,
	Author = {Meng, X. L. and Rubin, D. B.},
	Journal = {Biometrika},
	Keywords = {cem},
	Number = {2},
	Pages = {267--278},
	Title = {Maximum likelihood estimation via the {ECM} algorithm: A general framework},
	Volume = {80},
	Year = {1993}}

@article{Diabetes_dataset79,
	Author = {Reaven, G.M. and Miller, R.G.},
	Journal = {Diabetologia},
	Keywords = {Chemical diabetes; multidimensional analysis; insulin resistance; insulin secretion; glucose intolerance; diabetes mellitus},
	Number = {1},
	Pages = {17-24},
	Publisher = {Springer-Verlag},
	Title = {An attempt to define the nature of chemical diabetes using a multidimensional analysis},
	Volume = {16},
	Year = {1979}}

@article{banfield_and_raftery_93,
	Abstract = {The classification maximum likelihood approach is sufficiently general to encompass many current clustering algorithms, including those based on the sum of squares criterion and on the criterion of Friedman and Rubin (1967, Journal of the American Statistical Association 62, 1159-1178). However, as currently implemented, it does not allow the specification of which features (orientation, size, and shape) are to be common to all clusters and which may differ between clusters. Also, it is restricted to Gaussian distributions and it does not allow for noise. We propose ways of overcoming these limitations. A reparameterization of the covariance matrix allows us to specify that some, but not all, features be the same for all clusters. A practical framework for non-Gaussian clustering is outlined, and a means of incorporating noise in the form of a Poisson process is described. An approximate Bayesian method for choosing the number of clusters is given. The performance of the proposed methods is studied by simulation, with encouraging results. The methods are applied to the analysis of a data set arising in the study of diabetes, and the results seem better than those of previous analyses. A magnetic resonance image (MRI) of the brain is also analyzed, and the methods appear successful in extracting the main features of anatomical interest. The methods described here have been implemented in both Fortran and S-PLUS versions, and the software is freely available through StatLib.},
	Author = {Banfield, J. D. and Raftery, A. E.},
	Journal = {Biometrics},
	Number = {3},
	Pages = {803--821},
	Title = {{M}odel-{B}ased {G}aussian and {N}on-{G}aussian {C}lustering},
	Volume = {49},
	Year = {1993}}

@inproceedings{Wood06-non-parametric-UAI,
	Author = {Wood, F. and Griffiths, Thomas L. and Ghahramani, Z.},
	Booktitle = {UAI},
	Title = {{A} {N}on-{P}arametric {B}ayesian {M}ethod for {I}nferring {H}idden {C}auses.},
	Year = {2006}}

@book{rabiner_book,
	Address = {Upper Saddle River, NJ, USA},
	Author = {Rabiner, L. and Juang, B.-H.},
	Publisher = {Prentice-Hall, Inc.},
	Title = {Fundamentals of speech recognition},
	Year = {1993}}

@article{Lewis94estimatingbayesFactor,
	Author = {Steven M. Lewis and Adrian E. Raftery},
	Journal = {Journal of the American Statistical Association},
	Pages = {648--655},
	Title = {{E}stimating {B}ayes {F}actors via {P}osterior {S}imulation with the {L}aplace-{M}etropolis {E}stimator},
	Volume = {92},
	Year = {1994}}

@article{Jiang_and_tanner_IEEEinft_99,
	Author = {Wenxin Jiang and M. A. Tanner},
	Journal = {IEEE Transactions on Information Theory},
	Pages = {1005-1013},
	Title = {On the Asymptotic Normality of Hierarchical Mixtures-of-Experts for Generalized Linear Models},
	Volume = {46},
	Year = {1999}}

@article{PenalizedMLEUnivNMD2000,
	Author = {A. Ridolfi and J. Idier},
	Journal = {Bayesian Inference and Maximum Entropy Methods},
	Month = {July},
	Pages = {229-237},
	Title = {Penalized maximum likelihood estimation for univariate normal mixture distribution},
	Year = {2000}}

@article{Ridolfi2000,
	Author = {A. Ridolfi and J. Idier},
	Journal = {Bayesian Inference and Maximum Entropy Methods},
	Month = {July},
	Pages = {229-237},
	Title = {Penalized maximum likelihood estimation for univariate normal mixture distribution},
	Year = {2000}}

@article{jordan_and_xu_1995,
	Author = {Jordan, M. I. and Xu, L.},
	Journal = {Neural Networks},
	Number = {9},
	Pages = {1409-1431},
	Title = {Convergence results for the {EM} approach to mixtures of experts architectures},
	Volume = {8},
	Year = {1995}}

@article{North2000,
	Address = {Washington, DC, USA},
	Author = {North, B. and Blake, A. and Isard, M. and Rittscher, J.},
	Journal = {IEEE Transactions Pattern Analysis and Machine Intelligence},
	Number = {9},
	Pages = {1016--1034},
	Publisher = {IEEE Computer Society},
	Title = {Learning and Classification of Complex Dynamics},
	Volume = {22},
	Year = {2000}}

@phdthesis{Nowl1991,
	Author = {S.J. Nowlan},
	School = {Carnegie Mellon University, Pittsburgh},
	Title = {Soft competitive adaptation: Neural Network Learning Algorithms based on Fitting Statistical Mixtures},
	Year = {1991}}

@inproceedings{chamroukhi-ICMLA2009,
	Address = {USA},
	Author = {Onanena, R. and Chamroukhi, F. and Oukhellou, L. and Candusso, D. and Aknin, P. and Hissel, D.},
	Booktitle = {Machine Learning and Applications, Eighth International Conference on},
	Pages = {632-637},
	Publisher = {IEEE Computer Society},
	Title = {Supervised learning of a regression model based on latent process. Application to the estimation of fuel cell lifetime},
	Year = {2009}}

@inproceedings{Onanena2009,
	Address = {USA},
	Author = {Onanena, R. and Chamroukhi, F. and Oukhellou, L. and Candusso, D. and Aknin, P. and Hissel, D.},
	Booktitle = {Machine Learning and Applications, Fourth International Conference on},
	Pages = {632-637},
	Publisher = {IEEE Computer Society},
	Title = {Supervised learning of a regression model based on latent process. Application to the estimation of fuel cell lifetime},
	Year = {2009}}

@article{Ormonenti-IEEENN-98,
	Author = {Ormoneit, D. and Tresp, V.},
	Journal = {IEEE Transactions on Neural Networks},
	Number = {4},
	Pages = {639--650},
	Title = {Averaging, maximum penalized likelihood and Bayesian estimation for improving Gaussian mixture probability density estimates},
	Volume = {9},
	Year = {1998}}

@phdthesis{Oukhellou1997,
	Author = {Oukhellou, L.},
	School = {Universit\'e PARIS-SUD, Centre d'ORSAY},
	Title = {Param\'etrisation et Classification de Signaux en Contr\^ole Non Destructif. Application \`a la Reconnaissance des D\'efauts de Rails par Courants de Foucault},
	Type = {Th\`ese de doctorat},
	Year = {1997}}

@article{Oukhellou2008,
	Author = {Oukhellou, L. and C\^ome, E. and Bouillaut, L. and Aknin, P.},
	Journal = {Transportation Research, Part C},
	Pages = {755-767},
	Title = {Combined use of sensor data and structural information resumed by Bayesian network. Application to a railway diagnosis-aid scheme},
	Volume = {16},
	Year = {2008}}

@article{Pearson1901,
	Author = {Pearson, K.},
	Journal = {Philosophical Magazine},
	Number = {6},
	Pages = {559--572},
	Title = {On Lines and Planes of Closest Fit to Systems of Points in Space},
	Volume = {2},
	Year = {1901}}

@book{bayes_non_param_princ_practic2010,
	Editor = {Hjort, N. and Holmes, C. and Muller, P. and Waller, S. G.},
	Publisher = {Cambrige University Press},
	Title = {{B}ayesian {N}on {P}arametrics: {P}rinciples and practice},
	Year = {2010}}

@book{bayesdataanalysis_2003,
	Author = {Andrew Gelman, John B. Carlin, Hal S. Stern and Donald B. Rubin},
	Publisher = {Chapman and Hall/CRC},
	Title = {Bayesian Data Analysis},
	Year = {2003}}

@inbook{HDP_HMM_Teh2008,
	Abstract = {{Hierarchical modeling is a fundamental concept in Bayesian statistics. The basic idea is that parameters are endowed with distributions which may themselves introduce new parameters, and this construction recurses. In this review we discuss the role of hierarchical modeling in Bayesian non-parametrics, focusing on models in which the infinite-dimensional parameters are treated hierarchically. For example, we consider a model in which the base measure for a Dirichlet process is itself treated as a draw from another Dirichlet process. This yields a natural recursion that we refer to as a hierarchical Dirichlet process. We also discuss hierarchies based on the Pitman-Yor process and on completely random processes. We demonstrate the value of these hierarchical constructions in a wide range of practical applications, in problems in computational biology, computer vision and natural language processing.}},
	Address = {Cambridge, UK},
	Author = {Teh, Yee W. and Jordan, Michael},
	Booktitle = {Bayesian Nonparametrics: Principles and Practice},
	Keywords = {prob, stat, survey},
	Publisher = {Cambridge University Press},
	Title = {{Hierarchical Bayesian Nonparametric Models with Applications}},
	Year = 2010}

@article{Garland2011,
	Abstract = {Summary Cultural transmission, the social learning of information or behaviors from conspecifics [1{\^a}5], is believed to occur in a number of groups of animals, including primates [1, 6{\^a}9], cetaceans [4, 10, 11], and birds [3, 12, 13]. Cultural traits can be passed vertically (from parents to offspring), obliquely (from the previous generation via a nonparent model to younger individuals), or horizontally (between unrelated individuals from similar age classes or within generations) [4]. Male humpback whales (Megaptera novaeangliae) have a highly stereotyped, repetitive, and progressively evolving vocal sexual display or {\^a}song{\^a} [14{\^a}17] that functions in sexual selection (through mate attraction and/or male social sorting) [18{\^a}20]. All males within a population conform to the current version of the display (song type), and similarities may exist among the songs of populations within an ocean basin [16, 17, 21]. Here we present a striking pattern of horizontal transmission: multiple song types spread rapidly and repeatedly in a unidirectional manner, like cultural ripples, eastward through the populations in the western and central South Pacific over an 11-year period. This is the first documentation of a repeated, dynamic cultural change occurring across multiple populations at such a large geographic scale. },
	Author = {Ellen C. Garland and Anne W. Goldizen and Melinda L. Rekdahl and Rochelle Constantine and Claire Garrigue and Nan Daeschler Hauser and M. Michael Poole and Jooke Robbins and Michael J. Noad},
	Issn = {0960-9822},
	Journal = {Current Biology},
	Number = {8},
	Pages = {687 - 691},
	Title = {Dynamic Horizontal Cultural Transmission of Humpback Whale Song at the Ocean Basin Scale},
	Volume = {21},
	Year = {2011}}

@article{Antoniak_MixDP,
	Author = {Antoniak, C. E.},
	Journal = {The Annals of Statistics},
	Keywords = {bayes, bio-assay, dirchlet-process, discrimination, empirical-bayes, mixing-distribution, nonparametric, random-measures},
	Number = {6},
	Pages = {1152--1174},
	Priority = {5},
	Title = {Mixtures of {D}irichlet Processes with Applications to {B}ayesian Nonparametric Problems},
	Volume = {2},
	Year = {1974}}

@inproceedings{ihmm,
	Author = {Matthew J. Beal and Zoubin Ghahramani and Carl E. Rasmussen},
	Booktitle = {Machine Learning},
	Pages = {29--245},
	Publisher = {MIT Press},
	Title = {The Infinite Hidden Markov Model},
	Year = {2002}}

@conference{van2008beam,
	Author = {Van Gael, J. and Saatci, Y. and Teh, Y.W. and Ghahramani, Z.},
	Booktitle = {Proceedings of the 25th international conference on Machine learning},
	Organization = {ACM New York, NY, USA},
	Pages = {1088--1095},
	Timestamp = {2014.04.24},
	Title = {{Beam sampling for the infinite hidden Markov model}},
	Year = {2008}}

@inproceedings{fox_hdphmm_state_persistence_2008,
	Abstract = {{The hierarchical Dirichlet process hidden Markov model (HDP-HMM) is a flexible, nonparametric model which allows state spaces of unknown size to be learned from data. We demonstrate some limitations of the original HDP-HMM formulation (Teh et al., 2006), and propose a sticky extension which allows more robust learning of smoothly varying dynamics. Using DP mixtures, this formulation also allows learning of more complex, multimodal emission distributions. We further develop a sampling algorithm that employs a truncated approximation of the DP to jointly resample the full state sequence, greatly improving mixing rates. Via extensive experiments with synthetic data and the NIST speaker diarization database, we demonstrate the advantages of our sticky extension, and the utility of the HDP-HMM in real-world applications.}},
	Address = {New York, NY, USA},
	Author = {Fox, Emily B. and Sudderth, Erik B. and Jordan, Michael I. and Willsky, Alan S.},
	Booktitle = {ICML 2008: Proceedings of the 25th international conference on Machine learning},
	Location = {Helsinki, Finland},
	Pages = {312--319},
	Publisher = {ACM},
	Title = {{An HDP-HMM for systems with state persistence}},
	Year = {2008}}

@article{McLachlan1987,
	Author = {G. J. McLachlan},
	Journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	Number = {3},
	Pages = {318-324},
	Title = {On Bootstrapping the Likelihood Ratio Test Stastistic for the Number of Components in a Normal Mixture},
	Volume = {36},
	Year = {1978}}

@article{Nguyen2016approxMoE,
author = {Nguyen, Hien D. and Lloyd-Jones, Luke R. and McLachlan, Geoffrey J.},
title = {A Universal Approximation Theorem for Mixture-of-Experts Models},
journal = {Neural Computation},
volume = {28},
number = {12},
pages = {2585-2593},
year = {2016},
doi = {10.1162/NECO\_a\_00892},
abstract = { The mixture-of-experts (MoE) model is a popular neural network architecture for nonlinear regression and classification. The class of MoE mean functions is known to be uniformly convergent to any unknown target function, assuming that the target function is from a Sobolev space that is sufficiently differentiable and that the domain of estimation is a compact unit hypercube. We provide an alternative result, which shows that the class of MoE mean functions is dense in the class of all continuous functions over arbitrary compact domains of estimation. Our result can be viewed as a universal approximation theorem for MoE models. The theorem we present allows MoE users to be confident in applying such models for estimation when data arise from nonlinear and nondifferentiable generative processes. }
}


@article{Nguyen2014-MoLE,
	Author = {Hien D. Nguyen and Geoffrey J. McLachlan},
	Doi = {http://dx.doi.org/10.1016/j.csda.2014.10.016},
	volume = {93},
	Journal = {Computational Statistics \& Data Analysis},
	Keywords = {Robust regression},
	Pages = {177--191},
	Title = {Laplace mixture of linear experts},
	Year = {2016}
	}
@article{Spearman1904,
	Author = {Spearman, C.},
	Journal = {American Journal of psychology},
	Pages = {201-293},
	Title = {General intelligence, objectively determined and measured},
	Volume = {15},
	Year = {1904}}

@article{Rabiner86anintroductionHMM,
	Author = {L. R. Rabiner and B. H. Juang},
	Date-Modified = {2015-08-21 21:12:06 +0000},
	Journal = {IEEE ASSP Magazine},
	Title = {An introduction to hidden {M}arkov models},
	Year = {1986}}

@article{Pace2010,
	Author = {Pace, Federica and Benard, Frederic and Glotin, Herve and Adam, Olivier and White, Paul},
	Journal = {Applied Acoustics},
	Number = {11},
	Pages = {1107 - 1112},
	Title = {Subunit definition and analysis for humpback whale call classification},
	Volume = {71},
	Year = {2010}}

@article{Rand1971,
	Author = {Rand, W.M.},
	Journal = {Journal of the American Statistical Association},
	Keywords = {cluster clustering criteria evaluation index rand},
	Number = 336,
	Pages = {846-850},
	Title = {Objective criteria for the evaluation of clustering methods},
	Volume = 66,
	Year = 1971}

@article{Sangalli2013,
	Author = {L.M. Sangalli and J.O. Ramsay and T.O. Ramsay},
	Journal = {Journal of the Royal Statistical Society (Series B)},
	Pages = {681--703},
	Title = {Spatial spline regression models},
	Volume = {75},
	Year = {2013}}

@incollection{Ramsay2011,
	Author = {J.O. Ramsay and T.O. Ramsay and L.M. Sangalli},
	Booktitle = {Recent Advances in Functional Data Analysis and Related Topics},
	Editor = {F. Ferraty},
	Pages = {269--275},
	Publisher = {Springer},
	Title = {Spatial functional data analysis},
	Year = {2011}}

@book{McLachlanEM2008,
	Author = {McLachlan, G. J. and Krishnan, T.},
	Edition = {Second},
	Publisher = {New York: Wiley},
	Title = {The EM algorithm and extensions},
	Year = {2008}}

 

@article{NgEtAll2006,
	Author = {S. K. Ng and G. J. McLachlan and K. Wang adn L. Ben-Tovim Jones and S.-W. Ng},
	Journal = {Bioinformatics},
	Number = {14},
	Pages = {1745--1752},
	Title = {A Mixture model with random-effects components for clustering correlated gene-expression profiles},
	Volume = {22},
	Year = {2006}}

@article{LecunMnist,
	Author = {LeCun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	Doi = {10.1109/5.726791},
	Issn = {0018-9219},
	Journal = {Proceedings of the IEEE},
	Keywords = {backpropagation;convolution;multilayer perceptrons;optical character recognition;2D shape variability;GTN;back-propagation;cheque reading;complex decision surface synthesis;convolutional neural network character recognizers;document recognition;document recognition systems;field extraction;gradient based learning technique;gradient-based learning;graph transformer networks;handwritten character recognition;handwritten digit recognition task;high-dimensional patterns;language modeling;multilayer neural networks;multimodule systems;performance measure minimization;segmentation recognition;Character recognition;Feature extraction;Hidden Markov models;Machine learning;Multi-layer neural network;Neural networks;Optical character recognition software;Optical computing;Pattern recognition;Principal component analysis},
	Month = {Nov},
	Number = {11},
	Pages = {2278-2324},
	Title = {Gradient-based learning applied to document recognition},
	Volume = {86},
	Year = {1998},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/5.726791}}

@article{LenkANDDeSarbo2000,
	Author = {Lenk, P. and DeSarbo, W.},
	Journal = {Psychometrika},
	Number = {1},
	Pages = {93-119},
	Title = {Bayesian inference for finite mixtures of generalized linear models with random effects},
	Volume = {65},
	Year = {2000}}

@incollection{Marin2005Bayes-modeling-inference-mixtures,
	Abstract = {Abstract only:
Today{'}s data analysts and modellers are in the luxurious position of being able to more closely describe, estimate, predict and infer about complex systems of interest, thanks to ever more powerful computational methods but also wider ranges of modelling distributions. Mixture models constitute a fascinating illustration of these aspects: while within a parametric family, they offer malleable approximations in non-parametric settings; although based on standard distributions, they pose highly complex computational challenges; and they are both easy to constrain to meet identifiability requirements and fall within the class of ill-posed problems. They also provide an endless benchmark for assessing new techniques, from the EM algorithm to reversible jump methodology. In particular, they exemplify the formidable opportunity provided by new computational technologies like Markov chain Monte Carlo (MCMC) algorithms. It is no coincidence that the Gibbs sampling algorithm for the estimation of mixtures was proposed before (Tanner and Wong 1987) and immediately after (Diebolt and Robert 1990c) the seminal paper of Gelfand and Smith (1990): before MCMC was popularised, there simply was no satisfactory approach to the computation of Bayes estimators for mixtures of distributions, even though older importance sampling algorithms were later discovered to apply to the simulation of posterior distributions of mixture parameters (Casella et al. 2002).

Bayesian approaches to mixture modelling have attracted great interest among researchers and practitioners alike. The Bayesian paradigm (Berger 1985, Besag et al. 1995, Robert 2001, see, e.g.,) allows for probability statements to be made directly about the unknown parameters, prior or expert opinion to be included in the analysis, and hierarchical descriptions of both local-scale and global features of the model. This framework also allows the complicated structure of a mixture model to be decomposed into a set of simpler structures through the use of hidden or latent variables. When the
number of components is unknown, it can well be argued that the Bayesian paradigm is the only sensible approach to its estimation (Richardson and Green 1997).

This chapter aims to introduce the reader to the construction, prior modelling, estimation and evaluation of mixture distributions in a Bayesian paradigm. We will show that mixture distributions provide a flexible, parametricframework for statistical modelling and analysis. Focus is on methods rather than advanced examples, in the hope that an understanding of the practical aspects of such modelling can be carried into many disciplines. It also stresses implementation via specific MCMC algorithms that
can be easily reproduced by the reader. In Section 1.2, we detail some basic properties of mixtures, along with two different motivations. Section 1.3 points out the fundamental difficulty in doing inference with such objects, along with a discussion about prior modelling, which is more restrictive than usual, and the constructions of estimators, which also is more involved than the standard posterior mean solution. Section 1.4 describes the completion and non-completion MCMC algorithms that can be used
for the approximation to the posterior distribution on mixture parameters, followed by an extension of this analysis in Section 1.5 to the case in which the number of components is unknown and may be estimated by Green's (1995) reversible jump algorithm and Stephens' 2000 birth-and-death procedure.

Section 1.6 gives some pointers to related models and problems like mixtures of regressions (or conditional mixtures) and hidden Markov models (or dependent mixtures), as well as Dirichlet priors.},
	Author = {Jean-Michel Marin and Kerrie L. Mengersen and Christian Robert},
	Booktitle = {Handbook of Statistics: Volume 25},
	Date-Modified = {2015-08-21 22:14:08 +0000},
	Editor = {D. Dey and C.R. Rao},
	Keywords = {Bayesian statistics, mixture models, Monte Carlo methods},
	Publisher = {Elsevier},
	Title = {Bayesian modelling and inference on mixtures of distributions},
	Year = {2005},
	Bdsk-Url-1 = {http://eprints.qut.edu.au/901/}}

@article{Malfait2003,
	Author = {Malfait, N. and Ramsay, J. O.},
	Citeulike-Article-Id = {4219768},
	Journal = {The Canadian Journal of Statistics},
	Keywords = {functional, historical, linear, model},
	Number = {2},
	Publisher = {Wiley-Blackwell Hoboken},
	Title = {{The historical functional linear model}},
	Volume = {31},
	Year = {2003}}

@article{Giacofci2012,
	Author = {Giacofci, M. and Lambert-Lacroix, S. and Marot, G. and Picard, F.},
	Journal = {Biometrics},
	Number = {1},
	Pages = {31-40},
	Title = {Wavelet-Based Clustering for Mixed-Effects Functional Models in High Dimension},
	Volume = {69},
	Year = {2013}}

@article{biernacki_etal_startingEM_CSDA03,
	Author = {Biernacki, C. and Celeux, G. and Govaert, G.},
	Journal = {Computational Statistics and Data Analysis},
	Pages = {561-575},
	Title = {Choosing Starting Values for the {EM} Algorithm for Getting the Highest Likelihood in Multivariate Gaussian Mixture Models},
	Volume = {41},
	Year = {2003}}

@article{celeux_et_diebolt_SEM_85,
	Author = {Celeux, G. and Diebolt, J.},
	Journal = {Computational Statistics Quarterly},
	Number = {1},
	Pages = {73-82},
	Title = {The {SEM} algorithm a probabilistic teacher algorithm derived from the {EM} algorithm for the mixture problem},
	Volume = {2},
	Year = {1985}}

@book{hastieTibshiraniFreidman_book_2009,
	Author = {Hastie, T. and Tibshirani, R. and Friedman, J.},
	Edition = {Second Edition},
	Month = {January},
	Publisher = {Springer},
	Series = {Springer Series in Statistics},
	Title = {The Elements of Statistical Learning, Second Edition: Data Mining, Inference, and Prediction},
	Year = {2010}}

@book{ruppert_etal_semiparametricregression,
	Author = {Ruppert, D. Wand, M.P. and Carroll, R.J.},
	Publisher = {Cambridge University Press},
	Title = {Semiparametric Regression},
	Year = {2003}}

@article{Pedregal2004,
	Author = {Pedregal, D. J. and Garc\'ia, F. P. and Schmid, F.},
	Journal = {Reliability Engineering and System Safety},
	Pages = {103--110},
	Title = {RCM$^2$ predictive maintenance of railway systems based on unobserved components models},
	Volume = {83},
	Year = {2004}}

@article{picardetal2007,
	Author = {Picard, F. and Robin, S. and Lebarbier, E. and Daudin, J. J.},
	Journal = {Biometrics},
	Month = {September},
	Number = {3},
	Pages = {758--766},
	Title = {A Segmentation/Clustering Model for the Analysis of Array CGH Data},
	Volume = {63},
	Year = {2007}}

@techreport{Pitman2002,
	Author = {Pitman, J.},
	Institution = {Dept. of Statistics. UC, Berkeley},
	Number = {621},
	Publisher = {Notes for Saint Flour Summer School.},
	Series = {Lecture Notes in Mathematics},
	Title = {Combinatorial stochastic processes},
	Year = {2002}}

@techreport{Pitman2002a,
	Author = {Pitman, J.},
	Institution = {Dept. of Statistics. UC, Berkeley},
	Number = {621},
	Publisher = {Notes for Saint Flour Summer School.},
	Series = {Lecture Notes in Mathematics},
	Title = {Combinatorial stochastic processes},
	Year = {2002}}

@article{Pitman1995,
	Added-At = {2010-01-15T19:24:45.000+0100},
	Author = {Pitman, J.},
	Doi = {10.1007/BF01213386},
	Fjournal = {Probability Theory and Related Fields},
	Interhash = {5230941904836593bfc7893faecaf33a},
	Intrahash = {6554be9f832ad38bfa01ada9df147869},
	Issn = {0178-8051},
	Journal = {Probab. Theory Related Fields},
	Mrclass = {60G09 (60C05)},
	Mrnumber = {MR1337249 (96e:60059)},
	Mrreviewer = {Ma{\l}gorzata Majsnerowska},
	Number = {2},
	Pages = {145--158},
	Timestamp = {2010-01-15T19:24:45.000+0100},
	Title = {Exchangeable and partially exchangeable random partitions},
	Volume = {102},
	Year = {1995},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/BF01213386}}

@article{Quandt1978,
	Author = {Quandt, R. E. and Ramsey, J. B.},
	Journal = {Journal of the American Statistical Association},
	Pages = {730--738},
	Title = {Estimating mixtures of normal distributions and switching regressions},
	Volume = {73},
	Year = {1978}}

@inproceedings{Jackson2007-DMPGP,
	Author = {Edmund Jackson and Manuel Davy and Arnaud Doucet and William J. Fitzgerald},
	Booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP},
	Pages = {1077--1080},
	Title = {Bayesian Unsupervised Signal Classification by Dirichlet Process Mixtures of Gaussian Processes},
	Year = {2007}}

@article{Davy-2010,
	Author = {Davy, M. and Tourneret, J.Y.},
	Journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	Number = {10},
	Pages = {1781--1794},
	Title = {Generative Supervised Classification Using Dirichlet Process Priors},
	Volume = {32},
	Year = {2010}}

@inproceedings{Rabaoui2012,
	Author = {Asma Rabaoui and Hachem Kadri and Manuel Davy},
	Booktitle = {ICASSP},
	Pages = {3381-3384},
	Title = {Nonparametric Bayesian supervised classification of functional data},
	Year = {2012}}

@incollection{Teh2010a,
	Author = {Y. W. Teh},
	Booktitle = {Encyclopedia of Machine Learning},
	Publisher = {pringer},
	Title = {Dirichlet Processes},
	Year = {2010}}

@book{Rabiner1993,
	Address = {Upper Saddle River, NJ, USA},
	Author = {Rabiner, L. and Juang, B.-H.},
	Publisher = {Prentice-Hall, Inc.},
	Title = {Fundamentals of speech recognition},
	Year = {1993}}

@article{Rabiner1989,
	Author = {Rabiner, L. R.},
	Journal = {Proceedings of the IEEE},
	Number = {2},
	Pages = {257-286},
	Title = {A tutorial on hidden {M}arkov models and selected applications in speech recognition},
	Volume = {77},
	Year = {1989}}

@article{Ramsay1991,
	Abstract = {Multivariate data analysis permits the study of observations which are finite sets of numbers, but modern data collection situations can involve data, or the processes giving rise to them, which are functions. Functional data analysis involves infinite dimensional processes and/or data. The paper shows how the theory of L-splines can support generalizations of linear modelling and principal components analysis to samples drawn from random functions. Spline smoothing rests on a partition of a function space into two orthogonal subspaces, one of which contains the obvious or structural components of variation among a set of observed functions, and the other of which contains residual components. This partitioning is achieved through the use of a linear differential operator, and we show how the theory of polynomial splines can be applied more generally with an arbitrary operator and associated boundary constraints. These data analysis tools are illustrated by a study of variation in temperature-precipitation patterns among some Canadian weather-stations.},
	Author = {Ramsay, J. O. and Dalzell, C. J.},
	Journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	Keywords = {fda},
	Number = {3},
	Pages = {539--572},
	Posted-At = {2010-08-13 13:05:59},
	Priority = {2},
	Publisher = {Blackwell Publishing for the Royal Statistical Society},
	Title = {Some Tools for Functional Data Analysis},
	Volume = {53},
	Year = {1991}}

@book{Rasmussen2006,
	Author = {C. E. Rasmussen and C. K. I. Williams},
	Publisher = {Cambridge, MA: The MIT Press},
	Title = {Gaussian Processes for Machine Learning},
	Year = {2006}}

@article{Ridolfi2000,
	Author = {A. Ridolfi and J. Idier},
	Journal = {Bayesian Inference and Maximum Entropy Methods},
	Month = {July},
	Pages = {229-237},
	Title = {Penalized maximum likelihood estimation for univariate normal mixture distribution},
	Year = {2000}}

@article{Ridolfi1999,
	Author = {A. Ridolfi and J. Idier},
	Journal = {Actes 17e coll. GRETSI},
	Month = {September},
	Pages = {259-262},
	Title = {Penalized maximum likelihood estimation for univariate normal mixture distribution},
	Year = {1999}}

@book{Rosenblatt1962,
	Author = {Rosenblatt, F.},
	Publisher = {Spartan},
	Title = {Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms.},
	Year = {1962}}

@article{Rossi2006,
	Author = {Rossi, F. and Conan-Guez, B.},
	Journal = {Neural Processing Letters},
	Month = {February},
	Number = {1},
	Pages = {55--70},
	Title = {Theoretical Properties of Projection Based Multilayer Perceptrons with Functional Inputs},
	Volume = {23},
	Year = {2006}}

@article{Rossi2005,
	Author = {Rossi, F. and Conan-Guez, B.},
	Journal = {Revue de Statistique Appliqu\'ee},
	Number = {4},
	Pages = {5--30},
	Title = {Un mod{\`e}le neuronal pour la r{\'e}gression et la discrimination sur donn{\'e}es fonctionnelles},
	Volume = {LIII},
	Year = {2005}}

@article{Rossi2005d,
	Author = {Rossi, F. and Conan-Guez, B.},
	Journal = {Neural Networks},
	Month = {January},
	Number = {1},
	Pages = {45--60},
	Title = {Functional Multi-Layer Perceptron: a nonlinear tool for functional data analysis},
	Volume = {18},
	Year = {2005}}

@article{Rossi2005e,
	Author = {Rossi, F. and Conan-Guez, B.},
	Journal = {Neural Networks},
	Month = {January},
	Number = {1},
	Pages = {45--60},
	Title = {Functional Multi-Layer Perceptron: a nonlinear tool for functional data analysis},
	Volume = {18},
	Year = {2005}}

@article{Rossi2005f,
	Author = {Rossi, F. and Conan-Guez, B.},
	Journal = {Neural Networks},
	Month = {January},
	Number = {1},
	Pages = {45--60},
	Title = {Functional Multi-Layer Perceptron: a nonlinear tool for functional data analysis},
	Volume = {18},
	Year = {2005}}

@article{Rossi2005g,
	Author = {Rossi, F. and Conan-Guez, B.},
	Journal = {Neural Networks},
	Month = {January},
	Number = {1},
	Pages = {45--60},
	Title = {Functional Multi-Layer Perceptron: a nonlinear tool for functional data analysis},
	Volume = {18},
	Year = {2005}}

@inproceedings{Roweis1998,
	Author = {Roweis, S.},
	Booktitle = {Proceedings of the 11th Conference on Advances in Neural Information Processing Systems (NIPS)},
	Citeulike-Article-Id = {710827},
	Editor = {Jordan, M. I. and Kearns, M. J. and Solla, S. A.},
	Keywords = {em, pca},
	Posted-At = {2006-06-26 09:57:01},
	Priority = {0},
	Publisher = {MIT Press},
	Title = {{EM} Algorithms for {PCA} and {SPCA}},
	Volume = {10},
	Year = {1998}}

@book{Ruppert2003,
	Author = {Ruppert, D. Wand, M.P. and Carroll, R.J.},
	Publisher = {Cambridge University Press},
	Title = {Semiparametric Regression},
	Year = {2003}}

@article{Same2011,
	Author = {Sam{\'e}, A. and Chamroukhi, F. and Govaert, G. and Aknin, P.},
	Journal = {Advances in Data Analysis and Classification},
	Number = {4},
	Pages = {1-21},
	Publisher = {Springer Berlin / Heidelberg},
	Title = {Model-based clustering and segmentation of time series with changes in regime},
	Volume = {5},
	Year = {2011}}

@article{same-chamroukhi-Adac,
	Author = {Sam{\'e}, A. and Chamroukhi, F. and Govaert, G. and Aknin, P.},
	Issn = {1862-5347},
	Journal = {Advances in Data Analysis and Classification},
	Pages = {1-21},
	Publisher = {Springer Berlin / Heidelberg},
	Title = {Model-based clustering and segmentation of time series with changes in regime},
	Year = {2011}}

@article{same-etal-onlineCEM2007,
	Author = {Sam{\'e}, A. and Ambroise, C. and Govaert, G.},
	Journal = {Statistics and Computing},
	Number = {3},
	Pages = {209-218},
	Title = {An online classification {EM} algorithm based on the mixture model},
	Volume = {17},
	Year = {2007}}

@article{Sato2000,
	Author = {Sato, M-A. and Ishii, S.},
	Citeulike-Article-Id = {2587790},
	Citeulike-Linkout-0 = {http://neco.mitpress.org/cgi/content/abstract/12/2/407},
	Citeulike-Linkout-1 = {http://view.ncbi.nlm.nih.gov/pubmed/10636949},
	Citeulike-Linkout-2 = {http://www.hubmed.org/display.cgi?uids=10636949},
	Day = {1},
	Journal = {Neural Computation},
	Keywords = {bayesian, graphical-models, stochastic-approximation},
	Month = {February},
	Number = {2},
	Pages = {407--432},
	Posted-At = {2008-03-25 22:45:45},
	Priority = {2},
	Title = {On-line {EM} Algorithm for the {N}ormalized {G}aussian {N}etwork},
	Volume = {12},
	Year = {2000}}

@phdthesis{Schoelkopf1997,
	Author = {Bernhard Sch\"{o}lkopf},
	Note = {Published by: R. Oldenbourg Verlag, Munich},
	School = {Technischen Universit\"{a}t Berlin},
	Title = {Support Vector Learning},
	Year = {1997}}

@incollection{Schoelkopf1998,
	Author = {Bernhard Sch\"{o}lkopf and Chris Burges and Alex Smola},
	Booktitle = {Advances in Kernel Methods -- Support Vector Learning},
	Editor = {Bernhard Sch\"{o}lkopf and Chris Burges and Alex Smola},
	Pages = {1--22},
	Publisher = {MIT Press},
	Title = {Introduction to Support Vector Learning},
	Year = {1998}}

@book{Schoelkopf2001,
	Author = {Sch\"{o}lkopf, Bernhard and Smola, Alexander J.},
	Edition = {1st},
	Keywords = {kernel-method, learning-theory, machine-learning, pattern-recognition},
	Publisher = {The MIT Press},
	Title = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond (Adaptive Computation and Machine Learning)},
	Year = {2001}}

@article{Schwarz1978,
	Author = {Schwarz, G.},
	Journal = {Annals of Statistics},
	Pages = {461-464},
	Title = {Estimating the dimension of a model},
	Volume = {6},
	Year = {1978}}

@article{Scott1981,
	Author = {A. J. Scott and M. J. Symons},
	Journal = {Biometrics},
	Pages = {35--43},
	Title = {Clustering Criteria and Multivariate Normal Mixtures},
	Volume = {37},
	Year = {1981}}

@article{Symons81,
	Author = {A. J. Scott and M. J. Symons},
	Journal = {Biometrics},
	Pages = {35--43},
	Title = {Clustering Criteria and Multivariate Normal Mixtures},
	Volume = {37},
	Year = {1981}}

@article{Scott1971,
	Author = {A. J. Scott and M. J. Symons},
	Journal = {Biometrics},
	Pages = {387--397},
	Title = {Clustering methods based on likelihood ratio criteria},
	Volume = {27},
	Year = {1971}}

@book{Shawe-Taylor2004,
	Author = {Shawe-Taylor, J. and Cristianini, N.},
	Howpublished = {Hardcover},
	Keywords = {pattern analysis, kernel methods, support vector machines},
	Month = {June},
	Publisher = {Cambridge University Press},
	Title = {Kernel Methods for Pattern Analysis},
	Year = {2004}}

@article{Shi-etal-GPR,
	Author = {J.Q. Shi and B. Wang and R. Murray-Smith and D.M. Titterington},
	Journal = {Biometrics},
	Pages = {714-723},
	Title = {Gaussianprocess functional regression modeling for batch data},
	Volume = {63},
	Year = {2007}}

@article{Shi2007,
	Author = {J.Q. Shi and B. Wang and R. Murray-Smith and D.M. Titterington},
	Journal = {Biometrics},
	Pages = {714-723},
	Title = {Gaussianprocess functional regression modeling for batch data},
	Volume = {63},
	Year = {2007}}

@book{Shi2011,
	Author = {J. Q. Shi and T. Choi},
	Publisher = {Chapman \& Hall/CRC Press},
	Title = {Gaussian Process Regression Analysis for Functional Data},
	Year = {2011}}

@book{ShiGPR-Book2011,
	Author = {J. Q. Shi and T. Choi},
	Publisher = {Chapman \& Hall/CRC Press},
	Title = {Gaussian Process Regression Analysis for Functional Data},
	Year = {2011}}

@article{Shi2008,
	Author = {Shi, J. Q. and Wang, B.},
	Date = {2008-08-08},
	Journal = {Statistics and Computing},
	Number = {3},
	Pages = {267-283},
	Title = {Curve prediction and clustering with mixtures of Gaussian process functional regression models.},
	Volume = {18},
	Year = {2008}}

@article{Sin1995,
	Author = {Sin, B. and Kim, J. H.},
	Journal = {Signal Processing},
	Number = {1},
	Pages = {31--46},
	Publisher = {Elsevier North-Holland, Inc.},
	Title = {Nonstationary hidden Markov model},
	Volume = {46},
	Year = {1995}}

@inproceedings{Smyth1996,
	Author = {P. Smyth},
	Booktitle = {Advances in NIPS},
	Pages = {648-654},
	Title = {Clustering Sequences with Hidden Markov Models},
	Year = {1996}}

@inproceedings{Smyth1996a,
	Author = {P. Smyth},
	Booktitle = {Advances in NIPS},
	Pages = {648-654},
	Title = {Clustering Sequences with Hidden Markov Models},
	Year = {1996}}

@inproceedings{Smyth1996b,
	Author = {P. Smyth},
	Booktitle = {Advances in Neural Information Processing Systems 9, NIPS},
	Pages = {648-654},
	Title = {Clustering Sequences with Hidden Markov Models},
	Year = {1996}}

@inproceedings{Smyth1996c,
	Author = {P. Smyth},
	Booktitle = {Advances in Neural Information Processing Systems 9, NIPS},
	Pages = {648-654},
	Title = {Clustering Sequences with Hidden Markov Models},
	Year = {1996}}

@inproceedings{Smyth1996d,
	Author = {P. Smyth},
	Booktitle = {Advances in Neural Information Processing Systems 9, NIPS},
	Pages = {648--654},
	Title = {Clustering Sequences with Hidden Markov Models},
	Year = {1996}}

@inproceedings{Smyth1996e,
	Author = {P. Smyth},
	Booktitle = {Advances in Neural Information Processing Systems 9, NIPS},
	Pages = {648--654},
	Title = {Clustering Sequences with Hidden Markov Models},
	Year = {1996}}

@inproceedings{Smyth1996f,
	Author = {P. Smyth},
	Booktitle = {Advances in Neural Information Processing Systems 9, NIPS},
	Pages = {648--654},
	Title = {Clustering Sequences with Hidden Markov Models},
	Year = {1996}}

@inproceedings{Smyth1996g,
	Author = {P. Smyth},
	Booktitle = {Advances in Neural Information Processing Systems 9, NIPS},
	Pages = {648--654},
	Title = {Clustering Sequences with Hidden Markov Models},
	Year = {1996}}

@inproceedings{Smyth1993,
	Author = {Smyth, P.},
	Booktitle = {Neural Networks for Signal Processing},
	Journal = {Neural Networks for Signal Processing [1993] III. Proceedings of the 1993 IEEE-SP Workshop},
	Pages = {582--592},
	Title = {Hidden Markov models and neural networks for fault detection in dynamic systems},
	Year = {1993}}

@inproceedings{Smyth1993a,
	Author = {Smyth, P.},
	Booktitle = {Neural Networks for Signal Processing},
	Journal = {Neural Networks for Signal Processing [1993] III. Proceedings of the 1993 IEEE-SP Workshop},
	Pages = {582--592},
	Title = {Hidden Markov models and neural networks for fault detection in dynamic systems},
	Year = {1993}}

@inproceedings{Smyth1993b,
	Author = {Smyth, P.},
	Booktitle = {Neural Networks for Signal Processing},
	Journal = {Neural Networks for Signal Processing [1993] III. Proceedings of the 1993 IEEE-SP Workshop},
	Pages = {582--592},
	Title = {Hidden Markov models and neural networks for fault detection in dynamic systems},
	Year = {1993}}

@inproceedings{Smyth1993c,
	Author = {Smyth, P.},
	Booktitle = {Neural Networks for Signal Processing},
	Journal = {Neural Networks for Signal Processing [1993] III. Proceedings of the 1993 IEEE-SP Workshop},
	Pages = {582--592},
	Title = {Hidden Markov models and neural networks for fault detection in dynamic systems},
	Year = {1993}}

@inproceedings{Smyth1994,
	Author = {P. Smyth and U. M. Fayyad and M. C. Burl and P. Perona and P. Baldi},
	Booktitle = {NIPS},
	Pages = {1085-1092},
	Title = {Inferring Ground Truth from Subjective Labelling of Venus Images},
	Year = {1994}}

@inproceedings{Smyth1994a,
	Author = {P. Smyth and U. M. Fayyad and M. C. Burl and P. Perona and P. Baldi},
	Booktitle = {NIPS},
	Pages = {1085-1092},
	Title = {Inferring Ground Truth from Subjective Labelling of Venus Images},
	Year = {1994}}

@inproceedings{Smyth1994b,
	Author = {P. Smyth and U. M. Fayyad and M. C. Burl and P. Perona and P. Baldi},
	Booktitle = {NIPS},
	Pages = {1085-1092},
	Title = {Inferring Ground Truth from Subjective Labelling of Venus Images},
	Year = {1994}}

@inproceedings{Smyth1994c,
	Author = {P. Smyth and U. M. Fayyad and M. C. Burl and P. Perona and P. Baldi},
	Booktitle = {NIPS},
	Pages = {1085-1092},
	Title = {Inferring Ground Truth from Subjective Labelling of Venus Images},
	Year = {1994}}

@techreport{snoussi-djafari-degeneracy2005,
	Author = {Snoussi, Hichem and Mohammad-Djafari, Ali},
	Institution = {University of Technology of Troyes ISTIT/M2S},
	Title = {Degeneracy and Likelihood Penalization in Multivariate Gaussian Mixture Models},
	Year = {2005}}

@article{snoussi-djafari-penalized-likelihood2001,
	Author =  {Snoussi, Hichem and Mohammad-Djafari, Ali},
	Booktitle = {Bayesian Inference and Maximum Entropy Methods, B. Fry (Ed.), AIP Proceedings},
	Month = {august},
	Pages = {36--46},
	Title =  {{Penalized maximum likelihood for multivariate Gaussian mixture}},
	Year = {2001}}
 

@phdthesis{SudderthPhD2006,
	Address = {Cambridge, MA, USA},
	Advisor = {Freeman, William T. and Willsky, Alan S.},
	Author = {Sudderth, Erik B.},
	School = {Massachusetts Institute of Technology},
	Title = {Graphical models for visual object recognition and tracking},
	Year = {2006}}

@book{Thurstone1947,
	Author = {Thurstone, L. L.},
	Publisher = {University of Chicago Press},
	Title = {Multiple Factor Analysis},
	Year = {1947}}

@book{thusrstone47,
	Author = {Thurstone, L. L.},
	Publisher = {University of Chicago Press},
	Title = {Multiple Factor Analysis},
	Year = {1947}}

@article{Tipping1999,
	Author = {Tipping, M. E. and Bishop, C.},
	Journal = {Journal of the Royal Statistical Society, Series B},
	Keywords = {em, pca},
	Pages = {611--622},
	Title = {Probabilistic principal component analysis},
	Volume = {61},
	Year = {1999}}

@article{tipping99,
	Author = {Tipping, M. E. and Bishop, C.},
	Journal = {Journal of the Royal Statistical Society, Series B},
	Keywords = {em, pca},
	Pages = {611--622},
	Title = {Probabilistic principal component analysis},
	Volume = {61},
	Year = {1999}}

@techreport{Tipping1997,
	Author = {Tipping, M. E. and Bishop, C.},
	Institution = {Neural Comuting Research Group, Aston University},
	Number = {NCRG/97/010},
	Title = {Probabilistic principal component analysis},
	Year = {1997}}

@techreport{tipping97,
	Author = {Tipping, M. E. and Bishop, C.},
	Institution = {Neural Comuting Research Group, Aston University},
	Number = {NCRG/97/010},
	Title = {Probabilistic principal component analysis},
	Year = {1997}}

@book{Titterington1985,
	Author = {Titterington, D. and Smith, A. and Makov, U.},
	Publisher = {John Wiley \& Sons},
	Title = {Statistical Analysis of Finite Mixture Distributions},
	Year = {1985}}

@article{Titterington1984,
	Author = {Titterington, D. M.},
	Journal = {Journal of the Royal Statistical Society},
	Number = {2},
	Pages = {257--267},
	Serie = {B},
	Title = {Recursive parameter estimation using incomplete data},
	Volume = {46},
	Year = {1984}}

@book{Tomassone1992,
	Address = {Paris},
	Author = {R. Tomassone and E. Lesquoy and C. Millier},
	Language = {\Fr},
	Publisher = {Masson},
	Title = {La R{\'e}gression nouveaux regards sur une ancienne m{\'e}thode statistique},
	Year = {1992}}

@book{Tomassone1992a,
	Address = {Paris},
	Author = {R. Tomassone and E. Lesquoy and C. Millier},
	Language = {\Fr},
	Publisher = {Masson},
	Title = {La R{\'e}gression nouveaux regards sur une ancienne m{\'e}thode statistique},
	Year = {1992}}

@book{Tomassone1992b,
	Address = {Paris},
	Author = {R. Tomassone and E. Lesquoy and C. Millier},
	Language = {\Fr},
	Publisher = {Masson},
	Title = {La R{\'e}gression nouveaux regards sur une ancienne m{\'e}thode statistique},
	Year = {1992}}

@book{Tomassone1992c,
	Address = {Paris},
	Author = {R. Tomassone and E. Lesquoy and C. Millier},
	Language = {\Fr},
	Publisher = {Masson},
	Title = {La R{\'e}gression nouveaux regards sur une ancienne m{\'e}thode statistique},
	Year = {1992}}

@article{chamroukhi-ieeeTASE2013,
	Author = {Trabelsi, Dorra and Mohammed, Samer and Chamroukhi, Faicel and Oukhellou, Latifa and Amirat, Yacine},
	Journal = {IEEE Transactions on Automation Science and Engineering},
	Number = {10},
	Pages = {829--335},
	Title = {An unsupervised approach for automatic activity recognition based on Hidden Markov Model Regression},
	Volume = {3},
	Year = {2013}}

@inproceedings{chamroukhi-esann2012-2,
	Address = {Bruges, Belgium},
	Author = {D. Trabelsi and S. Mohammed and F. Chamroukhi and L. Oukhellou and Y. Amirat},
	Booktitle = {Proceedings of XXth European Symposium on Artificial Neural Networks ESANN},
	Month = {April},
	Pages = {417--422},
	Title = {Supervised and unsupervised classification approaches for human activity recognition using body-mounted sensors},
	Year = {2012}}

@inproceedings{Trabelsi2012,
	Address = {Bruges, Belgium},
	Author = {D. Trabelsi and S. Mohammed and F. Chamroukhi and L. Oukhellou and Y. Amirat},
	Booktitle = {Proceedings of XXth European Symposium on Artificial Neural Networks ESANN},
	Month = {April},
	Pages = {417--422},
	Title = {Supervised and unsupervised classification approaches for human activity recognition using body-mounted sensors},
	Year = {2012}}

@inproceedings{Ultsch1990,
	Abstract = {In this paper we describe experiments with self organizing feature maps that have been implemented on a transputer system. The use of feature maps for clustering is investigated and it is shown that naive application of Kohonen's algorithm, although preserving the topology of the input data, is not able to capture clusters. A new method, called U-matrix, is proposed which is capable to classify correctly all examples. First experiments with medical data of high dimensionality show a high correlation with expert clustering of data.},
	Author = {Ultsch, A. and Siemon, H. P.},
	Booktitle = {Proceedings of International Neural Networks Conference (INNC)},
	Pages = {305-308},
	Publisher = {Kluwer Academic Press},
	Title = {Kohonen's Self Organizing Feature Maps for Exploratory Data Analysis},
	Year = {1990}}

@inproceedings{Ultsch1990a,
	Abstract = {In this paper we describe experiments with self organizing feature maps that have been implemented on a transputer system. The use of feature maps for clustering is investigated and it is shown that naive application of Kohonen's algorithm, although preserving the topology of the input data, is not able to capture clusters. A new method, called U-matrix, is proposed which is capable to classify correctly all examples. First experiments with medical data of high dimensionality show a high correlation with expert clustering of data.},
	Author = {Ultsch, A. and Siemon, H. P.},
	Booktitle = {Proceedings of International Neural Networks Conference (INNC)},
	Pages = {305-308},
	Publisher = {Kluwer Academic Press},
	Title = {Kohonen's Self Organizing Feature Maps for Exploratory Data Analysis},
	Year = {1990}}

@inproceedings{Ultsch1990b,
	Abstract = {In this paper we describe experiments with self organizing feature maps that have been implemented on a transputer system. The use of feature maps for clustering is investigated and it is shown that naive application of Kohonen's algorithm, although preserving the topology of the input data, is not able to capture clusters. A new method, called U-matrix, is proposed which is capable to classify correctly all examples. First experiments with medical data of high dimensionality show a high correlation with expert clustering of data.},
	Author = {Ultsch, A. and Siemon, H. P.},
	Booktitle = {Proceedings of International Neural Networks Conference (INNC)},
	Pages = {305-308},
	Publisher = {Kluwer Academic Press},
	Title = {Kohonen's Self Organizing Feature Maps for Exploratory Data Analysis},
	Year = {1990}}

@inproceedings{Ultsch1990c,
	Abstract = {In this paper we describe experiments with self organizing feature maps that have been implemented on a transputer system. The use of feature maps for clustering is investigated and it is shown that naive application of Kohonen's algorithm, although preserving the topology of the input data, is not able to capture clusters. A new method, called U-matrix, is proposed which is capable to classify correctly all examples. First experiments with medical data of high dimensionality show a high correlation with expert clustering of data.},
	Author = {Ultsch, A. and Siemon, H. P.},
	Booktitle = {Proceedings of International Neural Networks Conference (INNC)},
	Pages = {305-308},
	Publisher = {Kluwer Academic Press},
	Title = {Kohonen's Self Organizing Feature Maps for Exploratory Data Analysis},
	Year = {1990}}

@book{Vapnik1999,
	Author = {Vapnik, V. N.},
	Citeulike-Article-Id = {115106},
	Howpublished = {Hardcover},
	Keywords = {learning theory},
	Publisher = {Springer},
	Title = {The Nature of Statistical Learning Theory (Information Science and Statistics)},
	Year = {1999}}

@book{Vapnik1998,
	Author = {Vapnik, V. N.},
	Howpublished = {Hardcover},
	Keywords = {learning theory},
	Publisher = {John Wiley \& Sons},
	Title = {Statistical Learning Theory},
	Year = {1998}}

@misc{Vapnik1974,
	Author = {Vapnik, V. N. and Chervonenkis, V.},
	Title = {Teoriya Raspoznavaniya Obrazov: Statisticheskie Problemy Obucheniya (Theory of Pattern Recognition: Statistical Problems of Learning)},
	Year = {1974}}

@article{Viterbi1967,
	Author = {Viterbi, A. J.},
	Journal = {IEEE Transactions on Information Theory},
	Number = {2},
	Pages = {260--269},
	Title = {Error bounds for convolutional codes and an asymptotically optimum decoding algorithm},
	Volume = {13},
	Year = {1967}}

@book{Wahba1990,
	Address = {Philadelphia, PA},
	Author = {Wahba, G.},
	Keywords = {regression, spline regression},
	Publisher = {Society for Industrial and Applied Mathematics (SIAM)},
	Series = {CBMS-NSF Regional Conference Series in Applied Mathematics},
	Title = {Spline models for observational data},
	Volume = {59},
	Year = {1990}}

@phdthesis{Waterhouse1997,
	Author = {Waterhouse, S. R.},
	School = {Department of Engineering, Cambridge University},
	Title = {Classification and regression using Mixtures of Experts},
	Year = {1997}}

@article{Wong2001,
	Author = {Wong, C.S. and Li, W.K.},
	Journal = {Biometrika},
	Number = {3},
	Pages = {833--846},
	Title = {On a logistic mixture autoregressive model},
	Volume = {88},
	Year = {2001}}

@article{Teh2006-JASA,
	Author = {Y. W. Teh and M. I. Jordan and M. J. Beal and D. M. Blei},
	Journal = {Journal of the American Statistical Association},
	Pages = {1566-1581},
	Title = {Hierarchical Dirichlet processes},
	Volume = {101},
	Year = {2006}}

@article{Wu1983,
	Abstract = {Two convergence aspects of the EM algorithm are studied: (i) does the EM algorithm find a local maximum or a stationary value of the (incomplete-data) likelihood function? (ii) does the sequence of parameter estimates generated by EM converge? Several convergence results are obtained under conditions that are applicable to many practical situations. Two useful special cases are: (a) if the unobserved complete-data specification can be described by a curved exponential family with compact parameter space, all the limit points of any EM sequence are stationary points of the likelihood function; (b) if the likelihood function is unimodal and a certain differentiability condition is satisfied, then any EM sequence converges to the unique maximum likelihood estimate. A list of key properties of the algorithm is included.},
	Author = {Wu, C. F. Jeff},
	Journal = {The Annals of Statistics},
	Keywords = {em-algorithm, learning},
	Number = {1},
	Pages = {95--103},
	Publisher = {Institute of Mathematical Statistics},
	Title = {On the Convergence Properties of the EM Algorithm},
	Volume = {11},
	Year = {1983}}

@article{Xiong2004,
	Author = {Xiong, Yimin and Yeung, Dit-Yan},
	Journal = {Pattern Recognition},
	Number = {8},
	Pages = {1675-1689},
	Title = {Time series clustering with ARMA mixtures.},
	Volume = {37},
	Year = {2004}}

@book{jutten07a,
	Editor = {Jutten, C. and Comon, P.},
	Publisher = {Herm{\`e}s},
	Serie = {Traitement du Signal et des Images},
	Title = {S\'eparation de source 1, concepts de base et analyse en composantes ind\'ependantes},
	Year = {2007}}

@book{jutten07b,
	Editor = {Jutten, C. and Comon, P.},
	Publisher = {Herm{\`e}s},
	Serie = {Traitement du Signal et des Images},
	Title = {S{\'e}paration de source 2, au-del\`{a} de l'aveugle et application},
	Year = {2007}}

@book{Jutten2007,
	Editor = {Jutten, C. and Comon, P.},
	Publisher = {Herm{\`e}s},
	Serie = {Traitement du Signal et des Images},
	Title = {S\'eparation de source 1, concepts de base et analyse en composantes ind\'ependantes},
	Year = {2007}}

@book{Jutten2007a,
	Editor = {Jutten, C. and Comon, P.},
	Publisher = {Herm\'es},
	Serie = {Traitement du Signal et des Images},
	Title = {S\'eparation de source 2, au-del\`{a} de l'aveugle et application},
	Year = {2007}}

@book{roberts01,
	Editor = {Roberts, S. and Everson, R.},
	Publisher = {Cambridge Univeristy Press},
	Title = {Indepedent Component Analysis, Principles and Practices},
	Year = {2001}}

@book{Roberts2001,
	Editor = {Roberts, S. and Everson, R.},
	Publisher = {Cambridge Univeristy Press},
	Title = {Indepedent Component Analysis, Principles and Practices},
	Year = {2001}}

@misc{UCI,
	Author = {K. Bache and M. Lichman},
	Institution = {University of California, Irvine, School of Information and Computer Sciences},
	Title = {{UCI} Machine Learning Repository},
	Url = {http://archive.ics.uci.edu/ml},
	Year = {2013},
	Bdsk-Url-1 = {http://archive.ics.uci.edu/ml}}

@article{BiernackiL14,
	Author = {Christophe Biernacki and Alexandre Lourme},
	Journal = {Statistics and Computing},
	Number = {6},
	Pages = {953--969},
	Title = {Stable and visualizable Gaussian parsimonious clustering models},
	Volume = {24},
	Year = {2014}}

@book{Govaert-Nadif-Co-ClusteingBook,
	Author = {G{\'e}rard Govaert and Mohamed Nadif},
	Month = {November},
	Note = {256 pages},
	Publisher = {Wiley-ISTE},
	Series = {Computer engineering series},
	Title = {Co-Clustering},
	Year = {2013}}

@article{sethuramanDP1994,
	Author = {Sethuraman, J.},
	Date-Modified = {2015-08-21 20:32:12 +0000},
	Journal = {Statistica Sinica},
	Pages = {639--650},
	Title = {{A constructive definition of Dirichlet priors}},
	Volume = {4},
	Year = {1994}}

@article{Ishwaren-dp2002,
	Author = {Ishwaren, H. and Zarepour, M.},
	Journal = {Canadian Journal of Statistics},
	Pages = {269-283},
	Title = {{E}xact and {A}pproximate {R}epresentations for the {S}um {D}irichlet {P}rocess},
	Volume = {30},
	Year = {2002}}

@article{BlackwellandMacQueen73,
	Added-At = {2011-05-09T23:10:52.000+0200},
	Author = {Blackwell, D. and MacQueen, J.},
	Journal = {The Annals of Statistics},
	Keywords = {imported},
	Pages = {353-355},
	Title = {{F}erguson {D}istributions {V}ia {P}olya {U}rn {S}chemes},
	Volume = 1,
	Year = 1973}

@article{celeux1996stochastic,
	Author = {Celeux, Gilles and Chauveau, Didier and Diebolt, Jean},
	Journal = {Journal of Statistical Computation and Simulation},
	Number = {4},
	Pages = {287--314},
	Publisher = {Gordon and Breach Science Publishers},
	Title = {Stochastic versions of the EM algorithm: an experimental study in the mixture case},
	Volume = {55},
	Year = {1996}}

@article{AIC,
	Author = {Akaike, H.},
	Journal = {IEEE Transactions on Automatic Control},
	Number = {6},
	Pages = {716--723},
	Title = {A new look at the statistical model identification},
	Volume = {19},
	Year = {1974}}

@book{Alan-matrix-book,
	Address = {Philadelphia, PA},
	Author = {Alan, J. L.},
	Publisher = {Society for Industrial and Applied Mathematics (SIAM)},
	Title = {Matrix Analysis for Scientists and Engineers},
	Year = {2005}}

@article{anderson91,
	Author = {Anderson, J.},
	Journal = {Psychological Review},
	Pages = {409-429},
	Title = {The adaptive nature of human categorization},
	Volume = {98},
	Year = {1991}}

@book{bayesdataanalysis-2006,
	Author = {Andrew Gelman and John Carlin and Hal Stern and David Dunson and Aki Vehtari and Donald Rubin},
	Publisher = {Chapman and Hall/CRC},
	Title = {Bayesian Data Analysis},
	Year = {2003}}

@book{Antoniadis1992,
	Author = {A. Antoniadis and J. Berruyer and R. Carmona},
	Publisher = {Economica},
	Title = {Rgression non linaire et applications},
	Year = {1992}}

@article{Antoniak74-DPM,
	Abstract = {{A random process called the Dirichlet process whose sample functions are almost surely probability measures has been proposed by Ferguson as an approach to analyzing nonparametric problems from a Bayesian viewpoint. An important result obtained by Ferguson in this approach is that if observations are made on a random variable whose distribution is a random sample function of a Dirichlet process, then the conditional distribution of the random measure can be easily calculated, and is again a Dirichlet process. This paper extends Ferguson's result to cases where the random measure is a mixing distribution for a parameter which determines the distribution from which observations are made. The conditional distribution of the random measure, given the observations, is no longer that of a simple Dirichlet process, but can be described as being a mixture of Dirichlet processes. This paper gives a formal definition for these mixtures and develops several theorems about their properties, the most important of which is a closure property for such mixtures. Formulas for computing the conditional distribution are derived and applications to problems in bio-assay, discrimination, regression, and mixing distributions are given.}},
	Author = {Antoniak, Charles E.},
	Journal = {The Annals of Statistics},
	Keywords = {bayesian, dirichletprocess, nonparametrics},
	Number = {6},
	Pages = {1152--1174},
	Priority = {3},
	Publisher = {Institute of Mathematical Statistics},
	Title = {{Mixtures of Dirichlet Processes with Applications to Bayesian Nonparametric Problems}},
	Volume = {2},
	Year = {1974}}

@article{sujeevanPR2009,
	Author = {S. Aseervatham and Y. Bennani},
	Journal = {Pattern Recognition},
	Number = {9},
	Pages = {2067-2076},
	Title = {Semi-structured document categorization with a semantic kernel},
	Volume = {42},
	Year = {2009}}

@article{bachJMLR2006,
	Author = {F. R. Bach and M. I. Jordan},
	Journal = {Journal of Machine Learning Research},
	Pages = {1963-2001},
	Title = {Learning Spectral Clustering, With Application To Speech Separation},
	Volume = {7},
	Year = {2006}}

@article{baldi-and-chauvin-1994,
	Address = {Cambridge, MA, USA},
	Author = {Baldi, P. and Chauvin, Y.},
	Journal = {Neural Computation},
	Number = {2},
	Pages = {307--318},
	Publisher = {MIT Press},
	Title = {Smooth on-line learning algorithms for hidden Markov models},
	Volume = {6},
	Year = {1994}}

@article{banfield-and-raftery-93,
	Abstract = {The classification maximum likelihood approach is sufficiently general to encompass many current clustering algorithms, including those based on the sum of squares criterion and on the criterion of Friedman and Rubin (1967, Journal of the American Statistical Association 62, 1159-1178). However, as currently implemented, it does not allow the specification of which features (orientation, size, and shape) are to be common to all clusters and which may differ between clusters. Also, it is restricted to Gaussian distributions and it does not allow for noise. We propose ways of overcoming these limitations. A reparameterization of the covariance matrix allows us to specify that some, but not all, features be the same for all clusters. A practical framework for non-Gaussian clustering is outlined, and a means of incorporating noise in the form of a Poisson process is described. An approximate Bayesian method for choosing the number of clusters is given. The performance of the proposed methods is studied by simulation, with encouraging results. The methods are applied to the analysis of a data set arising in the study of diabetes, and the results seem better than those of previous analyses. A magnetic resonance image (MRI) of the brain is also analyzed, and the methods appear successful in extracting the main features of anatomical interest. The methods described here have been implemented in both Fortran and S-PLUS versions, and the software is freely available through StatLib.},
	Author = {Banfield, J. D. and Raftery, A. E.},
	Journal = {Biometrics},
	Number = {3},
	Pages = {803--821},
	Title = {{M}odel-{B}ased {G}aussian and {N}on-{G}aussian {C}lustering},
	Volume = {49},
	Year = {1993}}

@article{BaumWelch,
	Author = {Baum, L.E. and Petrie, T. and Soules, G. and Weiss, N.},
	Journal = {Annals of Mathematical Statistics},
	Pages = {164-171},
	Title = {A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains},
	Volume = {41},
	Year = {1970}}

@article{BenboudjemaP07,
	Author = {D. Benboudjema and W. Pieczynski},
	Bibsource = {DBLP, http://dblp.uni-trier.de},
	Ee = {http://doi.ieeecomputersociety.org/10.1109/TPAMI.2007.1059},
	Journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	Number = {8},
	Pages = {1367-1378},
	Title = {Unsupervised Statistical Segmentation of Nonstationary Images Using Triplet Markov Fields},
	Volume = {29},
	Year = {2007}}

@article{bengio96-IOHMMs,
	Author = {Bengio, Y. and Frasconi, P.},
	Journal = {IEEE Transactions on Neural Networks},
	Keywords = {iohmm},
	Number = {5},
	Title = {{I}nput {O}utput {HMM}'s for Sequences Processing},
	Volume = {7},
	Year = {1996}}

@inproceedings{bengio95-IOHMMs,
	Author = {Bengio, Y. and Frasconi, P.},
	Booktitle = {Advances in Neural Information Processing Systems},
	Keywords = {discriminative, em, hmm, temporal},
	Pages = {427-434},
	Title = {An input output HMM architecture},
	Volume = {7},
	Year = {1995}}

@book{bennani-book-NN,
	Author = {Bennani, Y.},
	Publisher = {Herm\`es},
	Title = {Apprentissage Connexionniste},
	Year = {2006}}

@phdthesis{bensmail-phd95,
	Author = {Bensmail, Halima},
	Date-Modified = {2015-08-21 21:20:13 +0000},
	Pages = {175},
	School = {Universit\'e Paris 6},
	Title = {Mod{\`e}les de r{\'e}gularisation en discrimination et classification bay{\'e}sienne},
	Year = {1995}}

@article{Bensmail-model-based-clust97,
	Address = {Hingham, MA, USA},
	Author = {Bensmail, H. and Celeux, G. and Raftery, A. E. and Robert, C. P.},
	Journal = {Statistics and Computing},
	Keywords = {Bayes factor, Gaussian mixture, Gibbs sampler, eigenvalue decomposition},
	Number = {1},
	Numpages = {10},
	Pages = {1--10},
	Publisher = {Kluwer Academic Publishers},
	Title = {Inference in model-based cluster analysis},
	Volume = {7},
	Year = {1997}}

@article{Bensmail-and-meluman-MBC-03,
	Author = {Bensmail, H. and Meulman, Jacqueline J.},
	Journal = {Journal of Classification},
	Number = {1},
	Pages = {049--076},
	Title = {{M}odel-based {C}lustering with {N}oise: {B}ayesian {I}nference and {E}stimation},
	Volume = {20},
	Year = {2003}}

@article{Bezdek-fuzzy-Kmeans,
	Author = {Bezdek, J. C.},
	Journal = {Journal of Mathematical Biology},
	Pages = {57-71},
	Title = {Numerical taxonomy with fuzzy sets},
	Volume = {A},
	Year = {1974}}

@article{Bicego2009,
	Author = {Manuele Bicego and Mario A.T. Figueiredo},
	Journal = {Pattern Recognition},
	Keywords = {Soft clustering},
	Number = {1},
	Pages = {27-32},
	Title = {Soft clustering using weighted one-class support vector machines},
	Volume = {42},
	Year = {2009}}

@article{biernacki-etal-startingEM-CSDA03,
	Author = {Biernacki, C. and Celeux, G. and Govaert, G.},
	Journal = {Computational Statistics and Data Analysis},
	Pages = {561-575},
	Title = {Choosing Starting Values for the {EM} Algorithm for Getting the Highest Likelihood in Multivariate Gaussian Mixture Models},
	Volume = {41},
	Year = {2003}}

@article{ICL,
	Author = {Biernacki, C. and Celeux, G. and Govaert, G},
	Journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	Number = {7},
	Pages = {719--725},
	Title = {Assessing a mixture model for clustering with the integrated completed likelihood},
	Volume = {22},
	Year = {2000}}

@article{biernacki-et-al-improvement-NEC,
	Author = {Biernacki, C. and G. Celeux and Govaert, G.},
	Journal = {Pattern Recognition Letters},
	Number = {3},
	Pages = {267-272},
	Title = {An improvement of the NEC criterion for assessing the number of clusters in a mixture model},
	Volume = {20},
	Year = {1999}}

@book{bishopPRML,
	Address = {U K},
	Author = {C. M. Bishop},
	Publisher = {Springer Verlag},
	Title = {Pattern recognition and machine learning},
	Year = {2006}}

@book{bishopNNPR,
	Author = {Bishop, C. M.},
	Publisher = {Oxford University Press, USA},
	Title = {Neural Networks for Pattern Recognition},
	Year = {1995}}

@article{bishop-etal-GTM,
	Abstract = {Accepted for publication in Neural Computation. Latent variable models represent the probability density of data in a space of several dimensions in terms of a smaller number of latent, or hidden, variables. A familiar example is factor analysis which is based on a linear transformations between the latent space and the data space. In this paper we introduce a form of non-linear latent variable model called the Generative Topographic Mapping for which the parameters of the model can be determined using the {EM} algorithm. GTM provides a principled alternative to the widely used Self-Organizing Map (SOM) of Kohonen (1982), and overcomes most of the significant limitations of the SOM. We demonstrate the performance of the GTM algorithm on a toy problem and on simulated data from flow diagnostics for a multi-phase oil pipeline. GTM: The Generative Topographic Mapping 2},
	Author = {Bishop, C. M. and Williams, C. K. I.},
	Comment = {This paper develops a more general approach to factor models where instead of assuming that each observation is a linear combination of factors, it assumes an arbitrary function y(x;W) controlled by a set of parameters W that maps the L-dimensional latent space to some L-dimensional manifold in observation space. In the case where y is a linear function and we assume both a gaussian prior on x and a gaussian error distribution p(t|x), this reduces to standard linear factor analysis. However, for arbitrary y, this can be approximated by a constrained gaussian mixture model as follows: we take a regular lattice in latent space, and project each point under y into observation space. Then our gaussian error term gives us a set of gaussians around these projected points; each observation can be associated with one of them. This is a constrained mixture model because the centers of the gaussians cannot move independently but instead are determined by y; thus, it can be solved by {EM} for sufficiently tractable functions y. In the case where y takes the form of a generalized linear regression model y(x;W) = W\phi(x), the M-step becomes a set of linear equations.},
	Journal = {Neural Computation},
	Keywords = {pmf},
	Pages = {215--234},
	Posted-At = {2008-10-17 17:43:21},
	Priority = {2},
	Title = {GTM: The generative topographic mapping},
	Volume = {10},
	Year = {1998}}

@article{variational-dpm-blei-jordan2006,
	Author = {Blei, David M. and Jordan, Michael I.},
	Journal = {Bayesian Analysis},
	Keywords = {Dirichletprocess Variationalmethods},
	Number = {1},
	Pages = {121--144},
	Title = {{V}ariational {I}nference for {D}irichlet {P}rocess {M}ixtures},
	Volume = {1},
	Year = {2006}}

@phdthesis{bordes10thesis,
	Address = {Computer Science Laboratory of Paris 6 (LIP6)},
	Author = {Bordes, A.},
	Month = {February},
	School = {Universit\'e Pierre et Marie Curie},
	Title = {New Algorithms for Large-Scale Support Vector Machines},
	Year = {2010}}

@incollection{bottou-mlss-2004,
	Address = {Berlin},
	Author = {Bottou, L.},
	Booktitle = {Advanced Lectures on Machine Learning},
	Editor = {Bousquet, O. and Von Luxburg, U.},
	Pages = {146-168},
	Publisher = {Springer Verlag},
	Series = {Lecture Notes in Artificial Intelligence, LNAI~3176},
	Title = {Stochastic Learning},
	Year = {2004}}

@incollection{bottou-98x,
	Address = {Cambridge, UK},
	Author = {Bottou, L.},
	Booktitle = {Online Learning and Neural Networks},
	Editor = {Saad, David},
	Publisher = {Cambridge University Press},
	Title = {Online Algorithms and Stochastic Approximations},
	Year = {1998}}

@inproceedings{Bottou95-Kmeans-convergence,
	Author = {L. Bottou and Y. Bengio},
	Booktitle = {Advances in Neural Information Processing Systems 7},
	Pages = {585--592},
	Publisher = {MIT Press},
	Title = {Convergence Properties of the K-Means Algorithms},
	Year = {1995}}

@article{BouguilaZ07,
	Author = {Bouguila, Nizar and Ziou, Djemel},
	Journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
	Number = {10},
	Pages = {1716-1731},
	Title = {High-Dimensional Unsupervised Selection and Estimation of a Finite Generalized Dirichlet Mixture Model Based on Minimum Message Length.},
	Volume = {29},
	Year = {2007}}

@article{review-charles-bouveron-mbc-2013,
	Author = {C. Bouveyron and C. Brunet},
	Journal = {Computational Statistics and Data Analysis},
	Title = {Model-based clustering of high-dimensional data : A review},
	Year = {2012}}

@book{convex-optimization-boyd-and-vandenberghe,
	Author = {Boyd, S. and Vandenberghe, L.},
	Publisher = {Cambridge University Press},
	Title = {Convex Optimization},
	Url = {http://www.stanford.edu/~boyd/cvxbook/},
	Year = {2004},
	Bdsk-Url-1 = {http://www.stanford.edu/~boyd/cvxbook/}}

@phdthesis{come2009,
	Author = {C\^ome, E.},
	School = {Universit\'e de Technologie de Compi\`egne},
	Title = {Apprentissage de mod\`eles g\'en\'eratifs pour le diagnostic de syst\`emes complexes avec labellisation douce et contraintes spatiales},
	Type = {Th\`ese de doctorat},
	Year = {2009}}

@article{come-etal-PR-09,
	Author = {C\^ome, E. and Oukhellou, L. and Den{\oe}ux, T. and Aknin, P.},
	Journal = {Pattern recognition},
	Pages = {334-348},
	Title = {Learning form partially supervised data using mixture models and belief functions},
	Volume = {42},
	Year = {2009}}

@article{Caillol97,
	Author = {H. Caillol and W. Pieczynski and A. Hillion},
	Bibsource = {DBLP, http://dblp.uni-trier.de},
	Ee = {http://dx.doi.org/10.1109/83.557353},
	Journal = {IEEE Transactions on Image Processing},
	Number = {3},
	Pages = {425-440},
	Title = {Estimation of fuzzy Gaussian mixture and unsupervised statistical image segmentation},
	Volume = {6},
	Year = {1997}}

@article{cappe-EM-HMM-2009,
	Archiveprefix = {arXiv},
	Author = {Capp\'{e}, O.},
	Journal = {preprint},
	Month = {August},
	Title = {Online EM Algorithm for Hidden Markov Models},
	Url = {http://arxiv.org/abs/0908.2359},
	Year = {2009},
	Bdsk-Url-1 = {http://arxiv.org/abs/0908.2359}}

@article{cappe-and-moulines-online-EM-JRSS2009,
	Author = {Capp\'{e}, O. and Moulines, E.},
	Journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	Month = {June},
	Number = {3},
	Pages = {593--613},
	Title = {On-line expectation-maximization algorithm for latent data models},
	Volume = {71},
	Year = {2009}}

@book{cappe-book,
	Author = {Capp\'{e}, O. and Moulines, E. and Ryd{\'e}n, T.},
	Month = {July},
	Publisher = {Springer},
	Series = {Springer Series in Statistics},
	Title = {Inference in Hidden Markov Models},
	Year = {2005}}

@article{Carvalho-and-tanner-CSDA2007-mix-poisson,
	Address = {Amsterdam, The Netherlands, The Netherlands},
	Author = {Carvalho, A. X. and Tanner, M. A.},
	Journal = {Computational Statistics and Data Analysis},
	Number = {11},
	Pages = {5266--5294},
	Publisher = {Elsevier Science Publishers B. V.},
	Title = {Modelling nonlinear count time series with local mixtures of Poisson autoregressions},
	Volume = {51},
	Year = {2007}}

@article{celeux-et-al-SEM-SEM-MCEM-96,
	Abstract = {We compare three different stochastic versions of the {EM} algorithm: The Stochastic {EM} algorithm ({SEM}), the Simulated Annealing {EM} algorithm ({SAEM}) and the Monte Carlo {EM} algorithm ({MCEM}). We focus particularly on the mixture of distributions problem In this context, we investigate the practical behaviour of these algorithms through intensive Monte Carlo numerical simulations and a real data study. We show that, for some particular mixture situations, the {SEM} algorithm is almost always preferable to the {EM} and simulated annealing versions {SAEM} and {MCEM}. For some severely overlapping mixtures, however, none of these algorithms can be confidently used. Then, {SEM} can be used as an efficient data exploratory tool for locating significant maxima of the likelihood function. In the real data case, we show that the {SEM} stationary distribution provides a contrasted view of the loglikelihood by emphasizing sensible maxima.},
	Author = {Celeux, G. and Chauveau, D. and Diebolt, J.},
	Journal = {Journal of statistical computation and simulation},
	Number = {4},
	Pages = {287-314},
	Title = {Stochastic versions of the {EM} algorithm: an experimental study in the mixture case},
	Volume = {55},
	Year = {1996}}

@article{Diebolt-and-Robert-1994,
	Author = {Diebolt, Jean and Robert, Christian P.},
	Journal = {Journal of the Royal Statistical Society. Series B},
	Number = {2},
	Pages = {363--375},
	Publisher = {Blackwell Publishing for the Royal Statistical Society},
	Title = {{Estimation of Finite Mixture Distributions through Bayesian Sampling}},
	Volume = {56},
	Year = {1994}}

@article{celeux-et-diebolt-SAEM-92,
	Author = {Celeux, G. and Diebolt, J.},
	Journal = {Stochastics and Stochastics Reports},
	Pages = {119-134},
	Title = {A Stochastic Approximation Type {EM} Algorithm for the Mixture Problem},
	Volume = {41},
	Year = {1992}}

@article{celeux-and-govaert-parsimoniousGMM-95,
	Author = {Celeux, G. and Govaert, G.},
	Journal = {Pattern Recognition},
	Number = {5},
	Pages = {781-793},
	Title = {{G}aussian {P}arsimonious {C}lustering {M}odels.},
	Volume = {28},
	Year = {1995}}

@article{celeux-and-gilda-NEC-96,
	Abstract = {In this paper, we consider an entropy criterion to estimate the number of clusters arising from a mixture model. This criterion is derived from a relation linking the likelihood and the classification likelihood of a mixture. Its performance is investigated through Monte Carlo experiments, and it shows favorable results compared to other classical criteria.},
	Author = {Celeux, G. and Soromenho, G.},
	Journal = {Journal of Classification},
	Month = {September},
	Number = {2},
	Pages = {195-212},
	Title = {An entropy criterion for assessing the number of clusters in a mixture model},
	Volume = {13},
	Year = {1996}}

@article{chamroukhi-esann2012,
	Adress = {Bruges, Belgium},
	Author = {Chamroukhi, F. and Glotin, H. and Rabouy, C.},
	Journal = {Proceedings of XXth European Symposium on Artificial Neural Networks (ESANN)},
	Month = {april},
	Title = {Functional Mixture Discriminant Analysis with hidden process regression for curve classification},
	Year = {2012}}

@article{chamroukhi-et-al-neurocomputing2010,
	Author = {Chamroukhi, F. and Sam\'{e}, A. and Govaert, G. and Aknin, P.},
	Journal = {Neurocomputing},
	Month = {March},
	Number = {7-9},
	Pages = {1210--1221},
	Title = {A hidden process regression model for functional data description. Application to curve discrimination},
	Volume = {73},
	Year = {2010}}

@article{chamroukhi-et-al-NN2009,
	Address = {Oxford, UK, UK},
	Author = {Chamroukhi, F. and Sam\'{e}, A. and Govaert, G. and Aknin, P.},
	Journal = {Neural Networks},
	Number = {5-6},
	Pages = {593--602},
	Publisher = {Elsevier Science Ltd.},
	Title = {Time series modeling by a regression approach based on a latent process},
	Volume = {22},
	Year = {2009}}

@article{chamroukhi-rnti,
	Author = {Chamroukhi, F. and Sam\'e, A. and Govaert, G. and Aknin, P.},
	Journal = {RNTI, Revue des Nouvelles Technologies de l'Information},
	Title = {Mod\`ele \`a processus latent et algorithme {EM} pour la r\'egression non lin\'eaire, to be published},
	Year = {2010}}

@article{chamroukhi-et-al-ESANN2009,
	Author = {Chamroukhi, F. and Sam\'e, A. and Govaert, G. and Aknin, P.},
	Journal = {Proceedings of XVIIth European Symposium on Artificial Neural Networks (ESANN)},
	Pages = {503-508},
	Title = {A regression model with a hidden logistic process for signal parameterization},
	Year = {2009}}

@inproceedings{chamroukhiIJCNN2009,
	Author = {Chamroukhi, F. and Sam\'e, A. and Govaert, G. and Aknin, P.},
	Booktitle = {International Joint Conference on Neural Networks (IJCNN)},
	Title = {A regression model with a hidden logistic process for feature extraction from time series},
	Year = {2009}}

@article{Chib1995,
	Author = {Chib, S.},
	Journal = {Journal of the American Statistical Association},
	Number = {432},
	Pages = {1313--1321},
	Priority = {0},
	Title = {{M}arginal likelihood from the {G}ibbs output},
	Volume = {90},
	Year = {1995}}

@book{cover-elements-2006,
	Author = {T. M. Cover and Joy A. T.},
	Edition = {Second Edition},
	Publisher = {Wiley Interscience},
	Title = {Elements of Information Theory},
	Year = {2006}}

@article{cox-mds,
	Author = {Cox, T. F. and Cox, M. A. A. and Raton, B.},
	Journal = {Technometrics},
	Keywords = {multidimensional, scaling},
	Month = {May},
	Number = {2},
	Pages = {182},
	Title = {Multidimensional Scaling},
	Volume = {45},
	Year = {2003}}

@article{DaboNiang2007,
	Author = {Sophie Dabo-Niang and Fr{\'e}d{\'e}ric Ferraty and Philippe Vieu},
	Journal = {Computational Statistics \& Data Analysis},
	Keywords = {Curves classification},
	Number = {10},
	Pages = {4878 - 4890},
	Title = {On the using of modal curves for radar waveforms classification},
	Volume = {51},
	Year = {2007}}

@phdthesis{debiolles07,
	Author = {Debiolles, A.},
	School = {Universit\`e de Technologie de Compi\`egne},
	Title = {Diagnostic de syst\`emes complexes base de mod\`ele interne, reconnaissance des formes et fusion d'informations. Application au diagnostic des Cricuits de Voie ferroviaires},
	Type = {Th\`ese de doctorat},
	Year = {2007}}

@book{deboor1978,
	Author = {C. de Boor},
	Publisher = {Springer-Verlag},
	Title = {A Practical Guide to Splines},
	Year = {1978}}

@article{Delaigle2012,
	Author = {Delaigle, A. and Hall, P. and Bathia, N.},
	Journal = {Biometrika},
	Number = {2},
	Pages = {299-313},
	Title = {Componentwise classification and clustering of functional data},
	Volume = {99},
	Year = {2012}}

@article{dlr,
	Author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
	Journal = {Journal of The Royal Statistical Society, B},
	Pages = {1-38},
	Title = {Maximum likelihood from incomplete data via the {EM} algorithm},
	Volume = {39(1)},
	Year = {1977}}

@inproceedings{Derrode2006,
	Address = {Toulouse},
	Author = {Derrode, S. Benyoussef, L. and W. Pieczynski},
	Booktitle = {ICASSP},
	Month = {May},
	Pages = {15--19},
	Title = {Contextual estimation of hidden Markov chains with application to image segmentation},
	Year = {2006}}

@article{diebold-et-al-non-hom-HMM-1994,
	Address = {Oxford},
	Author = {Diebold, F.X. and Lee, J.-H. and Weinbach, G.},
	Editor = {C.W.J. Granger and G. Mizon, eds.},
	Journal = {Nonstationary Time Series Analysis and Cointegration. (Advanced Texts in Econometrics)},
	Pages = {283--302},
	Publisher = {Oxford University Press},
	Title = {Regime Switching with Time-Varying Transition Probabilities},
	Year = {1994}}

@article{donat-etal-neucom-2010,
	Author = {Donat, Roland and Leray, Philippe and Bouillaut, Laurent and Aknin, Patrice},
	Journal = {Neurocomputing},
	Number = {4-6},
	Pages = {570-577},
	Title = {A dynamic Bayesian network to represent discrete duration models},
	Volume = {73},
	Year = {2010}}

@article{douc-moulines-ryden-2002,
	Author = {Douc, R. and Moulines, E. and Ryden, T.},
	Journal = {Annals of Statistics},
	Number = {5},
	Pages = {2254--2304},
	Title = {Asymptotic properties of the maximum likelihood estimator in autoregressive models with Markov regime},
	Volume = {32},
	Year = {2004}}

@book{dubuisson90,
	Author = {Dubuisson, B.},
	Publisher = {Herm\`es},
	Serie = {Trait\'e des nouvelles thechnologies},
	Title = {Diagnostic et Reconnaissance des formes},
	Year = {1990}}

@book{duda-and-hart-book,
	Author = {Duda, R. O. and Hart, P. E.},
	Howpublished = {Hardcover},
	Publisher = {John Wiley \& Sons Inc},
	Title = {Pattern Classification and Scene Analysis},
	Year = {1973}}

@article{DP-Ferguson73,
	Abstract = {{The Bayesian approach to statistical problems, though fruitful in many ways, has been rather unsuccessful in treating nonparametric problems. This is due primarily to the difficulty in finding workable prior distributions on the parameter space, which in nonparametric ploblems is taken to be a set of probability distributions on a given sample space. There are two desirable properties of a prior distribution for nonparametric problems. (I) The support of the prior distribution should be large--with respect to some suitable topology on the space of probability distributions on the sample space. (II) Posterior distributions given a sample of observations from the true probability distribution should be manageable analytically. These properties are antagonistic in the sense that one may be obtained at the expense of the other. This paper presents a class of prior distributions, called Dirichlet process priors, broad in the sense of (I), for which (II) is realized, and for which treatment of many nonparametric statistical problems may be carried out, yielding results that are comparable to the classical theory. In Section 2, we review the properties of the Dirichlet distribution needed for the description of the Dirichlet process given in Section 3. Briefly, this process may be described as follows. Let X be a space and A a σ-field of subsets, and let α be a finite non-null measure on (X, A). Then a stochastic process P indexed by elements A of A, is said to be a Dirichlet process on (X, A) with parameter α if for any measurable partition (A1, ⋯, Ak) of X, the random vector (P(A1), ⋯, P(Ak)) has a Dirichlet distribution with parameter (α(A1), ⋯, α(Ak)). P may be considered a random probability measure on (X, A), The main theorem states that if P is a Dirichlet process on (X, A) with parameter α, and if X1, ⋯, Xn is a sample from P, then the posterior distribution of P given X1, ⋯, Xn is also a Dirichlet process on (X, A) with a parameter α + ∑n 1 δxi , where δx denotes the measure giving mass one to the point x. In Section 4, an alternative definition of the Dirichlet process is given. This definition exhibits a version of the Dirichlet process that gives probability one to the set of discrete probability measures on (X, A). This is in contrast to Dubins and Freedman [2], whose methods for choosing a distribution function on the interval [0, 1] lead with probability one to singular continuous distributions. Methods of choosing a distribution function on [0, 1] that with probability one is absolutely continuous have been described by Kraft [7]. The general method of choosing a distribution function on [0, 1], described in Section 2 of Kraft and van Eeden [10], can of course be used to define the Dirichlet process on [0, 1]. Special mention must be made of the papers of Freedman and Fabius. Freedman [5] defines a notion of tailfree for a distribution on the set of all probability measures on a countable space X. For a tailfree prior, posterior distribution given a sample from the true probability measure may be fairly easily computed. Fabius [3] extends the notion of tailfree to the case where X is the unit interval [0, 1], but it is clear his extension may be made to cover quite general X. With such an extension, the Dirichlet process would be a special case of a tailfree distribution for which the posterior distribution has a particularly simple form. There are disadvantages to the fact that P chosen by a Dirichlet process is discrete with probability one. These appear mainly because in sampling from a P chosen by a Dirichlet process, we expect eventually to see one observation exactly equal to another. For example, consider the goodness-of-fit problem of testing the hypothesis H0 that a distribution on the interval [0, 1] is uniform. If on the alternative hypothesis we place a Dirichlet process prior with parameter α itself a uniform measure on [0, 1], and if we are given a sample of size n ≥ 2, the only nontrivial nonrandomized Bayes rule is to reject H0 if and only if two or more of the observations are exactly equal. This is really a test of the hypothesis that a distribution is continuous against the hypothesis that it is discrete. Thus, there is still a need for a prior that chooses a continuous distribution with probability one and yet satisfies properties (I) and (II). Some applications in which the possible doubling up of the values of the observations plays no essential role are presented in Section 5. These include the estimation of a distribution function, of a mean, of quantiles, of a variance and of a covariance. A two-sample problem is considered in which the Mann-Whitney statistic, equivalent to the rank-sum statistic, appears naturally. A decision theoretic upper tolerance limit for a quantile is also treated. Finally, a hypothesis testing problem concerning a quantile is shown to yield the sign test. In each of these problems, useful ways of combining prior information with the statistical observations appear. Other applications exist. In his Ph. D. dissertation [1], Charles Antoniak finds a need to consider mixtures of Dirichlet processes. He treats several problems, including the estimation of a mixing distribution, bio-assay, empirical Bayes problems, and discrimination problems.}},
	Author = {Ferguson, Thomas S.},
	Date-Modified = {2015-08-21 21:38:48 +0000},
	Journal = {The Annals of Statistics},
	Number = {2},
	Pages = {209--230},
	Publisher = {Institute of Mathematical Statistics},
	Title = {{A Bayesian Analysis of Some Nonparametric Problems}},
	Volume = {1},
	Year = {1973}}

@inproceedings{ferrari1,
	Author = {Ferrari-Trecate, G. and Muselli, M.},
	Booktitle = {International Conference on Artificial Neural Networks (ICANN)},
	Pages = {28-30},
	Title = {A new learning method for piecewise linear regression},
	Year = {2002}}

@article{ferrari2,
	Author = {Ferrari-Trecate, G. and Muselli, M. and Liberati, D. and Morari, M.},
	Journal = {Automatica},
	Pages = {205-217},
	Title = {A clustering technique for the identification of piecewise affine and hybrid systems},
	Volume = {39},
	Year = {2002}}

@article{fisher,
	Author = {Fisher, W. D.},
	Journal = {Journal of the American Statistical Association},
	Pages = {789-798},
	Title = {On grouping for maximum homogeneity},
	Volume = {53},
	Year = {1958}}

@article{flury84,
	Author = {B. W. Flury},
	Journal = {JASA},
	Pages = {892-897},
	Title = {Common principal components in k gorups},
	Volume = {79},
	Year = {1984}}

@article{flury-gautschi86,
	Author = {B. W. Flury and W. Gautschi},
	Journal = {SIAM J. Scientific Statist. Comput.},
	Pages = {169-184},
	Title = {An algorithm for simulations orthogonal transformation of several positive definite symetric matrices to nearly diagonal form},
	Volume = {7},
	Year = {1986}}

@article{forney-algo-viterbi,
	Abstract = {The Viterbi algorithm (VA) is a recursive optimal solution to the problem of estimating the state sequence of a discrete-time finite-state Markov process observed in memoryless noise. Many problems in areas such as digital communications can be cast in this form. This paper gives a tutorial exposition of the algorithm and of how it is implemented and analyzed. Applications to date are reviewed. Increasing use of the algorithm in a widening variety of areas is foreseen.},
	Author = {Forney, G. D.},
	Journal = {Proceedings of the IEEE},
	Number = {3},
	Pages = {268--278},
	Title = {The viterbi algorithm},
	Volume = {61},
	Year = {1973}}

@article{fort-convergence-MCEM,
	Author = {G. Fort and E. Moulines},
	Journal = {Annals Statistics},
	Number = {4},
	Pages = {1220--1259},
	Title = {Convergence of the Monte Carlo expectation maximization for curved exponential families},
	Volume = {31},
	Year = {2003}}

@article{foulley2002,
	Author = {Foulley, JL.},
	Journal = {Journal de la Soci{\'e}t{\'e} Francaise de Statistique},
	Number = {(3-4)},
	Pages = {57-109},
	Title = {Algorithme {EM} : Th{\'e}orie et application au mod{\`e}le mixte},
	Volume = {143},
	Year = {2002}}

@phdthesis{Fox2009,
	Address = {Cambridge, MA},
	Author = {Fox, E.B.},
	School = {MIT},
	Title = {{B}ayesian {N}onparametric {L}earning of {C}omplex {D}ynamical {P}henomena},
	Type = {PhD. Thesis},
	Year = {2009}}

@article{Fraley1998,
	Abstract = {We consider the problem of determining the structure of clustered data, without prior knowledge of the number of clusters or any other information about their composition. Data are represented by a mixture model in which each component corresponds to a different cluster. Models with varying geometric properties are obtained through Gaussian components with different parametrizations and cross-cluster constraints. Noise and outliers can be modelled by adding a Poisson process component. Partitions are determined by the expectation-maximization (EM) algorithm for maximum likelihood, with initial values from agglomerative hierarchical clustering. Models are compared using an approximation to the Bayes factor based on the Bayesian information criterion (BIC); unlike significance tests, this allows comparison of more than two models at the same time, and removes the restriction that the models compared be nested. The problems of determining the number of clusters and the clustering method are solved simultaneously by choosing the best model. Moreover, the EM result provides a measure of uncertainty about the associated classification of each data point. Examples are given, showing that this approach can give performance that is much better than standard procedures, which often fail to identify groups that are either overlapping or of varying sizes and shapes. 10.1093/comjnl/41.8.578},
	Author = {Fraley, C. and Raftery, A. E.},
	Day = {1},
	Journal = {The Computer Journal},
	Keywords = {cluster-analysis, model-based-clustering},
	Month = {August},
	Number = {8},
	Pages = {578--588},
	Posted-At = {2008-03-23 23:00:08},
	Priority = {2},
	Title = {How Many Clusters? Which Clustering Method? Answers Via Model-Based Cluster Analysis},
	Address={Ozdoes},
	Volume = {41},
	Year = {1998}}

@inproceedings{Gaffney99trajectoryclustering,
	Author = {Scott Gaffney and Padhraic Smyth},
	Booktitle = {Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining},
	Pages = {63--72},
	Publisher = {ACM Press},
	Title = {Trajectory Clustering with Mixtures of Regression Models},
	Year = {1999}}

@phdthesis{GaffneyThesis,
	Author = {S. J. Gaffney},
	School = {Department of Computer Science, University of California, Irvine},
	Title = {Probabilistic Curve-Aligned Clustering and Prediction with Regression Mixture Models},
	Year = {2004}}

@article{Gauchi-vila-coroller,
	Author = {Gauchi,J-P and Vila, J-P and Coroller, L},
	Journal = {Communications in Statistics - Simulation and Computation},
	Pages = {322--334},
	Title = {New prediction interval and band in the nonlinear regression model : applications to predictive modeling in foods},
	Volume = {39},
	Year = {2010}}

@article{Geman-GIBBS:1984,
	Address = {Washington, DC, USA},
	Author = {Geman, Stuart and Geman, Donald},
	Journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
	Keywords = {Annealing, Gibbs distribution, MAP estimate, Markov random field, image restoration, line process, relaxation, scene modeling, spatial degradation},
	Month = nov,
	Number = {6},
	Numpages = {21},
	Pages = {721--741},
	Publisher = {IEEE Computer Society},
	Title = {{S}tochastic {R}elaxation, {G}ibbs {D}istributions, and the {B}ayesian {R}estoration of {I}mages},
	Volume = {6},
	Year = {1984}}

@article{gauvain-max94,
	Author = {Jean-luc Gauvain and Chin-hui Lee},
	Journal = {IEEE Transactions on Speech and Audio Processing},
	Number = {1},
	Pages = {291--298},
	Title = {Maximum A Posteriori Estimation for Multivariate Gaussian Mixture Observations of Markov Chains},
	Volume = {2},
	Year = {1994}}

@article{Gauvain94,
	Author = {J-L Gauvain and C-H Lee},
	Journal = {IEEE Transactions on Speech and Audio Processing},
	Month = {April},
	Number = {2},
	Pages = {291-298},
	Title = {Maximum a Posteriori Estimation for Multivariate Gaussian Mixture Observations of Markov Chains},
	Volume = {2},
	Year = {1994}}

@article{Gauvain92,
	Author = {J-L Gauvain and C-H Lee},
	Journal = {Speech Communication},
	Number = {2-3},
	Pages = {205-213},
	Title = {Bayesian learning for hidden Markov model with Gaussian mixture state observation densities},
	Volume = {11},
	Year = {1992}}

@article{Gelfand94,
	Abstract = {Model determination is a fundamental data analytic task. Here we consider the problem of choosing among a finite (without loss of generality we assume two) set of models. After briefly reviewing classical and Bayesian model choice strategies we present a general predictive density which includes all proposed Bayesian approaches that we are aware of. Using Laplace approximations we can conveniently assess and compare the asymptotic behaviour of these approaches. Concern regarding the accuracy of these approximations for small to moderate sample sizes encourages the use of Monte Carlo techniques to carry out exact calculations. A data set fitted with nested non-linear models enables comparisons between proposals and between exact and asymptotic values.},
	Author = {Gelfand, A. E. and Dey, D. K.},
	Journal = {Journal of the Royal Statistical Society. Series B},
	Keywords = {model, selection},
	Number = {3},
	Pages = {501--514},
	Priority = {2},
	Publisher = {Blackwell Publishing for the Royal Statistical Society},
	Title = {{B}ayesian {M}odel {C}hoice: {A}symptotics and {E}xact {C}alculations},
	Volume = {56},
	Year = {1994}}

@article{Green95reversiblejump,
	Author = {Peter J. Green},
	Journal = {Biometrika},
	Pages = {711--732},
	Title = {{R}eversible {J}ump {M}arkov {C}hain {M}onte {C}arlo {C}omputation and {B}ayesian {M}odel {D}etermination},
	Volume = {82},
	Year = {1995}}

@inproceedings{Gui-FMDA,
	Author = {Gui, J. and Li, H.},
	Booktitle = {Proc. Joint Stat. Meeting (Biometric Section)},
	Title = {Mixture functional discriminant analysis for gene function classification based on time course gene expression data},
	Year = {2003}}

@inproceedings{boser-guyon-vapnik-SVM-COLT,
	Author = {B.E. Boser I.M. Guyon and V.N. Vapnik},
	Booktitle = {Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory (COLT)},
	Editor = {In D. Haussler},
	Month = {July},
	Pages = {144--152},
	Publisher = {ACM Press},
	Title = {A training algorithm for optimal margin classifiers},
	Year = {1992}}

@book{HH-MonteCarloMethods,
	Address = {London},
	Author = {Hammersley, John Michael and Handscomb, David Christopher},
	Publisher = {Chapman and Hall},
	Series = {Monographs on statistics and applied probability},
	Title = {Monte Carlo methods},
	Year = {1964}}

@book{Hand-book-datasets,
	Author = {Hand, D. and Daly, F. and Lunn, A. D. and McConway, K. J. and E. Ostrowski},
	Optaddress = {London},
	Publisher = {Chapman and Hall.},
	Title = {A Handbook of Small Data Sets},
	Year = {1994}}

@book{hastieTibshiraniFreidman-book-2009,
	Author = {Hastie, T. and Tibshirani, R. and Friedman, J.},
	Edition = {Second Edition},
	Month = {January},
	Publisher = {Springer},
	Series = {Springer Series in Statistics},
	Title = {The Elements of Statistical Learning, Second Edition: Data Mining, Inference, and Prediction},
	Year = {2010}}

@book{bayes-non-param-princ-practic2010,
	Author = {Hjort, N. and Holmes, C. and Muller, P. and Waller, S. G.},
	Publisher = {Cambrige University Press},
	Title = {{B}ayesian {N}on {P}arametrics: {P}rinciples and practice},
	Year = {2010}}

@article{Hughes-et-al-nonhomHMM,
	Abstract = {A nonhomogeneous hidden Markov model is proposed for relating precipitation occurrences at multiple rain gauge stations to broad-scale atmospheric circulation patterns (the so-called ``downscaling problem"). We model a 15 year sequence of winter data from 30 rain stations in southwestern Australia. The first 10 years of data are used for model development and the remaining 5 years are used for model evaluation. The fitted model accurately reproduces the observed rainfall statistics in the reserved data despite a shift in atmospheric circulation (and, consequently, rainfall) between the two periods. The fitted model also provides some useful insights into the processes driving rainfall in this region.},
	Author = {Hughes, J. P. and Guttorp, P. and Charles, S. P.},
	Journal = {Applied Statistics},
	Keywords = {model, simulation, weather},
	Pages = {15--30},
	Title = {{A non-homogeneous hidden Markov model for precipitation occurrence}},
	Volume = {48},
	Year = {1999}}

@inproceedings{HugueneyEtAl:ESANN2009,
	Address = {Bruges, Belgium},
	Author = {Hugueney, B. and H\'ebrail, G. and Lechevallier, Y. and Rossi, F.},
	Booktitle = {Proceedings of XVIIth European Symposium on Artificial Neural Networks (ESANN)},
	Month = {April},
	Pages = {281--286},
	Title = {Simultaneous Clustering and Segmentation for Functional Data},
	Year = {2009}}

@inproceedings{Isermann04model-basedfault,
	Author = {Isermann, R.},
	Booktitle = {In Proceedings of the 16th IFAC Symposium on Automatic Control in Aerospace, St},
	Pages = {71--85},
	Title = {Model-based fault detection and diagnosis: status and applications},
	Year = {2004}}

@article{Isermann84,
	Author = {Isermann, R.},
	Journal = {Automatica},
	Number = {4},
	Pages = {387--404},
	Title = {Process fault detection based on modeling and estimation methods},
	Volume = {20},
	Year = {1984}}

@article{jacobsME,
	Author = {Jacobs, R. A. and Jordan, M. I. and Nowlan, S. J. and Hinton, G. E.},
	Journal = {Neural Computation},
	Number = {1},
	Pages = {79-87},
	Title = {Adaptive Mixtures of Local Experts},
	Volume = {3},
	Year = {1991}}

@article{garetjamesJRSS2002,
	Author = {G. M. James},
	Journal = {Journal of the Royal Statistical Society Series B},
	Pages = {411--432},
	Title = {Generalized Linear Models with Functional Predictor Variables},
	Volume = {64},
	Year = {2002}}

@article{garetjamesANDtrevorhastieJRSS2001,
	Author = {G. M. James and T. J. Hastie},
	Journal = {Journal of the Royal Statistical Society Series B},
	Pages = {533--550},
	Title = {{Functional Linear Discriminant Analysis for Irregularly Sampled Curves}},
	Volume = {63},
	Year = {2001}}

@article{garetjamesJASA2003,
	Author = {James, G. M. and Sugar, C.},
	Journal = {Journal of the American Statistical Association},
	Number = {462},
	Optpages = {397-408},
	Title = {Clustering for Sparsely Sampled Functional Data},
	Volume = {98},
	Year = {2003}}

@book{jebaraBook-generative-discriminative,
	Address = {Norwell, MA, USA},
	Author = {Jebara, T.},
	Publisher = {Kluwer Academic Publishers},
	Title = {Machine Learning: Discriminative and Generative (Kluwer International Series in Engineering and Computer Science)},
	Year = {2003}}

@phdthesis{jebara-tehsis-01,
	Author = {Jebara, T.},
	School = {Media Laboratory, MIT},
	Title = {Discriminative, {G}enerative and {I}mitative learning},
	Type = {PhD thesis},
	Year = {2001}}

@inproceedings{jebara-and-pen-CEM-algorithm-nips98,
	Author = {Jebara, T. and Pentland, A.},
	Booktitle = {Advances in Neural Information Processing Systems 11},
	Title = {Maximum conditional likelihood via bound maximization and the {CEM} algorithm},
	Year = {1998}}

@inproceedings{Jebara-MixHmms-07,
	Abstract = {Clustering has recently enjoyed progress via spectral methods which group data using only pairwise affinities and avoid parametric assumptions. While spectral clustering of vector inputs is straightforward, extensions to structured data or time-series data remain less explored. This paper proposes a clustering method for time-series data that couples non-parametric spectral clustering with parametric hidden Markov models (HMMs). HMMs add some beneficial structural and parametric assumptions such as Markov properties and hidden state variables which are useful for clustering. This article shows that using probabilistic pairwise kernel estimates between parametric models provides improved experimental results for unsupervised clustering and visualization of real and synthetic datasets. Results are compared with a fully parametric baseline method (a mixture of hidden Markov models) and a non-parametric baseline method (spectral clustering with non-parametric time-series kernels).},
	Author = {Jebara, T. and Song, Y. and Thadani, K.},
	Booktitle = {Machine Learning: ECML 2007},
	Pages = {164--175},
	Title = {Spectral Clustering and Embedding with Hidden Markov Models},
	Year = {2007}}

@book{Jeffreys61,
	Author = {Jeffreys, H.},
	Booktitle = {Theory of Probability},
	Description = {CCNLab BibTeX},
	Edition = {Third},
	Keywords = {stats},
	Publisher = {Oxford},
	Title = {Theory of Probability},
	Year = {1961}}

@article{jensen,
	Author = {Jensen, J. L. W. V.},
	Journal = {Acta Mathematica},
	Number = {1},
	Pages = {175-193},
	Publisher = {Springer Netherlands},
	Title = {Sur les fonctions convexes et les in\'egalit\'es entre les valeurs moyennes},
	Volume = {30},
	Year = {1906}}

@article{Jiang1999,
	Author = {Wenxin Jiang and M. A. Tanner},
	Journal = {IEEE Transactions on Information Theory},
	Pages = {1005--1013},
	Title = {On the Asymptotic Normality of Hierarchical Mixtures-of-Experts for Generalized Linear Models},
	Volume = {46},
	Year = {1999}}

@incollection{joachims98largescale-SVM,
	Author = {Thorsten Joachims},
	Booktitle = {Advances in Kernel Methods -- Support Vector Learning},
	Editor = {Bernhard Sch\"{o}lkopf and Chris Burges and Alex Smola},
	Pages = {169--185},
	Publisher = {MIT Press},
	Title = {Making large-scale SVM learning practical},
	Year = {1998}}

@book{joachim-book,
	Author = {T. Joachims},
	Publisher = {Kluwer/Springer},
	Title = {Learning to Classify Text Using Support Vector Machines -- Methods, Theory, and Algorithms},
	Year = {2002}}

@article{Jordan-HME-1994,
	Author = {Jordan, M. I. and Jacobs, R. A.},
	Date-Modified = {2015-08-21 22:02:40 +0000},
	Journal = {Neural Computation},
	Pages = {181-214},
	Title = {Hierarchical Mixtures of Experts and the {EM} algorithm},
	Volume = {6},
	Year = {1994}}

@article{vari-jordan99,
	Author = {Jordan, M. I. and Ghahramani, Z. and Jaakkola, T. and Saul, L. K.},
	Journal = {Machine Learning},
	Pages = {183-233},
	Title = {An introduction to variational methods for graphical models},
	Volume = {37},
	Year = {1999}}

@inproceedings{Juang-etal-85,
	Address = {Brighton, England},
	Author = {B.-H. Juang and S. E. Levinson and M. M. Sondhi},
	Booktitle = {IEEE International Symposium on Information Theory},
	Month = {June},
	Title = {Maximum likelihood estimation for mixture multivariate stochastic observations of Markov chains},
	Year = {1985}}

@inproceedings{Juang-et-rabiner-86,
	Author = {B.-H. Juang and L. R. Rabiner},
	Booktitle = {IEEE ICASSP Proceedings},
	Optaddress = {Tokyo, Japan},
	Optmonth = {April},
	Pages = {41-44},
	Title = {Mixture autoregressive hidden Markov models for speaker independent isolated word recognition},
	Year = {1986}}

@article{Juang-et-rabiner-85,
	Author = {B.-H. Juang and L. R. Rabiner},
	Journal = {IEEE Transactions on Acoustics, Speech and Signal Processing},
	Month = {December},
	Number = {6},
	Pages = {1404-1413},
	Title = {Mixture autoregressive hidden Markov models for speech signals},
	Volume = {33},
	Year = {1985}}

@phdthesis{Kaski1997som,
	Abstract = {Finding structures in vast multidimensional data sets, be they measurement data, statistics, or textual documents, is difficult and time-consuming. Interesting, novel relations between the data items may be hidden in the data. The selforganizing map (SOM) algorithm of Kohonen can be used to aid the exploration: the structures in the data sets can be illustrated on special map displays. In this work, the methodology of using SOMs for exploratory data analysis or data mining is reviewed and...},
	Author = {Kaski, Samuel},
	Keywords = {self-organizing kohonen maps, neural network},
	Publisher = {The Finnish Academy of Technology},
	School = {Acta Polytechnica Scandinavica, Mathematics, Computing and Management in Engineering},
	Title = {Data Exploration Using Self-Organizing Maps},
	Year = {1997}}

@article{Kenny-ARHMM,
	Author = {Kenny, P. and Lennig,M. and Mermelstein P.},
	Journal = {IEEE Transactions on Acoustics, Speech and Signal Processing},
	Number = {2},
	Pages = {220--225,},
	Title = {A linear predictive HMM for vector-valued observations with applications to speech recognition},
	Volume = {38},
	Year = {1990}}

@inproceedings{jordan-imagedenoising-2007,
	Author = {J. J. Kivinen and E. B. Sudderth and M. I. Jordan},
	Booktitle = {ICIP (3)},
	Pages = {121-124},
	Title = {Image Denoising with Nonparametric Hidden Markov Trees},
	Year = {2007}}

@book{kohonen-SOM,
	Author = {Kohonen, T.},
	Edition = {third Edition},
	Keywords = {neural-nets, som},
	Publisher = {Springer},
	Series = {Information Sciences},
	Title = {Self-Organizing Maps},
	Year = {2001}}

@book{kohonen-SOM-89,
	Author = {Kohonen, T.},
	Publisher = {Springer-Verlag New York, Inc. New York, NY, USA},
	Title = {Self-organization and associative memory},
	Year = {1989}}

@article{Kohonen82,
	Abstract = {This work contains a theoretical study and computer simulations of a new self-organizing process. The principal discovery is that in a simple network of adaptive physical elements which receives signals from a primary event space, the signal representations are automatically mapped onto a set of output responses in such a way that the responses acquire the same topological order as that of the primary events. In other words, a principle has been discovered which facilitates the automatic formation of topologically correct maps of features of observable events. The basic self-organizing system is a one- or two-dimensional array of processing units resembling a network of threshold-logic units, and characterized by short-range lateral feedback between neighbouring units. Several types of computer simulations are used to demonstrate the ordering process as well as the conditions under which it fails.},
	Author = {Kohonen, Teuvo},
	Journal = {Biological Cybernetics},
	Keywords = {neuroscience, self-organization, som},
	Month = {January},
	Number = {1},
	Pages = {59--69},
	Posted-At = {2009-02-27 16:21:30},
	Priority = {5},
	Title = {Self-organized formation of topologically correct feature maps},
	Volume = {43},
	Year = {1982}}

@article{kohonen-et-al-ieeeTNN2000,
	Abstract = {Describes the implementation of a system that is able to organize vast document collections according to textual similarities. It is based on the self-organizing map (SOM) algorithm. As the feature vectors for the documents statistical representations of their vocabularies are used. The main goal in our work has been to scale up the SOM algorithm to be able to deal with large amounts of high-dimensional data. In a practical experiment we mapped 6840568 patent abstracts onto a 1002240-node SOM. As the feature vectors we used 500-dimensional vectors of stochastic figures obtained as random projections of weighted word histograms},
	Author = {Kohonen, T. and Kaski, S. and Lagus, K. and Salojarvi, J. and Honkela, J. and Paatero, V. and Saarela, A.},
	Journal = {IEEE Transactions on Neural Networks},
	Number = {3},
	Pages = {574-585},
	Title = {Self organization of a massive document collection},
	Volume = {11},
	Year = {2000}}

@article{kong-competitive-learning-91,
	Abstract = {A comparison is made of a differential-competitive-learning (DCL) system with two supervised competitive-learning (SCL) systems for centroid estimation and for phoneme recognition. DCL provides a form of unsupervised adaptive vector quantization. Standard stochastic competitive-learning systems learn only if neurons win a competition for activation induced by randomly sampled patterns. DCL systems learn only if the competing neurons change their competitive signal. Signal-velocity information provides unsupervised local reinforcement during learning. The sign of the neuronal signal derivative rewards winners and punishes losers. Standard competitive learning ignores instantaneous win-rate information. Synaptic fan-in vectors adaptively quantize the randomly sampled pattern space into nearest-neighbor decision classes. More generally, the synaptic-vector distribution estimates the unknown sampled probability density function p(x). Simulations showed that unsupervised DCL-trained synaptic vectors converged to class centroids at least as fast as, and wandered less about these centroids than, SCL-trained synaptic vectors did. Simulations on a small set of English phonemes favored DCL over SCL for classification accuracy.},
	Author = {Kong, S. and Kosko, B.},
	Journal = {IEEE Transactions on Neural Networks},
	Number = {1},
	Pages = {118-124},
	Title = {Differential competitive learning for centroid estimation and phoneme recognition},
	Volume = {2},
	Year = {1991}}

@article{krishnapuram,
	Author = {Krishnapuram, B. and Carin, L. and Figueiredo, M.A.T. and Hartemink,A.J.},
	Journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	Number = {6},
	Pages = {957-968},
	Title = {Sparse multinomial logistic regression: fast algorithms and generalization bounds},
	Volume = {27},
	Year = {2005}}

@article{Lazaro-Gredilla:2010:SSG,
	Author = {L\'{a}zaro-Gredilla, Miguel and Qui\~{n}onero-Candela, Joaquin and Rasmussen, Carl Edward and Figueiras-Vidal, An\'{\i}bal R.},
	Journal = {Journal of Machine Learning Research},
	Pages = {1865--1881},
	Title = {Sparse Spectrum Gaussian Process Regression},
	Volume = {11},
	Year = {2010}}

@article{yveslechevalier90,
	Author = {Lechevalier, Y.},
	Journal = {Technical report, French National Institute for Research in Computer Science and Control (INRIA)},
	Title = {Optimal clustering on ordered set},
	Year = {1990}}

@article{liuANDyangFunctionalDataClustering,
	Author = {Liu, X. and Yang, M.C.K.},
	Journal = {Computational Statistics and Data Analysis},
	Number = {4},
	Pages = {1361--1376},
	Title = {Simultaneous curve registration and clustering for functional data},
	Volume = {53},
	Year = {2009}}

@article{Mongillo-and-Deneve-2008,
	Author = {Mongillo, G. and Deneve, S.},
	Journal = {Neural computation},
	Month = {July},
	Number = {7},
	Pages = {1706--1716},
	Title = {Online learning with hidden markov models},
	Volume = {20},
	Year = {2008}}

@misc{crp-intro-navaro-perfors,
	Author = {Navarro, D. and Perfors, A.},
	Institution = {University of Adelaide},
	Journal = {COMPSCI 3016: Computational Cognitive Science},
	Title = {The Chinese restaurant process}}

@article{Raftery-1996,
	Author = {Raftery, Adrian E.},
	Journal = {Biometrika},
	Pages = {251-266},
	Title = {{Approximate Bayes factors and accounting for model uncertainty in generalized linear models}},
	Volume = {83},
	Year = {1996}}

@article{NR-wlb,
	Abstract = {{We introduce the weighted likelihood bootstrap (WLB) as a way to simulate approximately from a posterior distribution. This method is often easy to implement, requiring only an algorithm for calculating the maximum likelihood estimator, such as iteratively reweighted least squares. In the generic weighting scheme, the WLB is first order correct under quite general conditions. Inaccuracies can be removed by using the WLB as a source of samples in the sampling-importance resampling (SIR) algorithm, which also allows incorporation of particular prior information. The SIR-adjusted WLB can be a competitive alternative to other integration methods in certain models. Asymptotic expansions elucidate the second-order properties of the WLB, which is a generalization of Rubin's Bayesian bootstrap. The calculation of approximate Bayes factors for model comparison is also considered. We note that, given a sample simulated from the posterior distribution, the required marginal likelihood may be simulation consistently estimated by the harmonic mean of the associated likelihood values; a modification of this estimator that avoids instability is also noted. These methods provide simple ways of calculating approximate Bayes factors and posterior model probabilities for a very wide class of models.}},
	Author = {Newton, Michael A. and Raftery, Adrian E.},
	Issn = {00359246},
	Journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	Number = {1},
	Pages = {3--48},
	Priority = {0},
	Publisher = {Blackwell Publishing for the Royal Statistical Society},
	Title = {{Approximate Bayesian Inference with the Weighted Likelihood Bootstrap}},
	Volume = {56},
	Year = {1994}}

@article{oukhellou-etal-TR-08,
	Author = {Oukhellou, L. and C\^ome, E. and Bouillaut, L. and Aknin, P.},
	Journal = {Transportation Research, Part C},
	Pages = {755-767},
	Title = {Combined use of sensor data and structural information resumed by Bayesian network. Application to a railway diagnosis-aid scheme},
	Volume = {16},
	Year = {2008}}

@article{federicapace-fredericbenard-herveglotin-olivieradam-paulwhite-2010,
	Author = {Pace, Federica and Benard, Frederic and Glotin, Herve and Adam, Olivier and White, Paul},
	Journal = {Applied Acoustics},
	Number = {11},
	Pages = {1107 - 1112},
	Title = {Subunit definition and analysis for humpback whale call classification},
	Volume = {71},
	Year = {2010}}

@article{Diego-etal-points-diagnosis-2004,
	Author = {Pedregal, D. J. and Garc\'ia, F. P. and Schmid, F.},
	Journal = {Reliability Engineering and System Safety},
	Pages = {103--110},
	Title = {RCM$^2$ predictive maintenance of railway systems based on unobserved components models},
	Volume = {83},
	Year = {2004}}

@article{pitman95,
	Added-At = {2010-01-15T19:24:45.000+0100},
	Author = {Pitman, J.},
	Fjournal = {Probability Theory and Related Fields},
	Interhash = {5230941904836593bfc7893faecaf33a},
	Intrahash = {6554be9f832ad38bfa01ada9df147869},
	Issn = {0178-8051},
	Journal = {Probab. Theory Related Fields},
	Mrclass = {60G09 (60C05)},
	Mrnumber = {MR1337249 (96e:60059)},
	Mrreviewer = {Ma{\l}gorzata Majsnerowska},
	Number = {2},
	Pages = {145--158},
	Timestamp = {2010-01-15T19:24:45.000+0100},
	Title = {Exchangeable and partially exchangeable random partitions},
	Volume = {102},
	Year = {1995}}

@article{quandt,
	Author = {Quandt, R. E. and Ramsey, J. B.},
	Journal = {Journal of the American Statistical Association},
	Number = {730-738},
	Title = {Estimating mixtures of normal distributions and switching regressions},
	Volume = {73},
	Year = {1978}}

@book{RamsayAndSilvermanFDA2005,
	Author = {Ramsay, J. O. and Silverman, B. W.},
	Date-Modified = {2015-08-21 22:21:34 +0000},
	Month = {June},
	Publisher = {Springer},
	Address={New York},
	Series = {Springer Series in Statistics},
	Title = {Functional Data Analysis},
	Year = {2005}}

@book{ramsayandsilvermanAppliedFDA2002,
	Author = {Ramsay, J. O. and Silverman, B. W.},
	Publisher = {Springer},
	Series = {Springer Series in Statistics},
	Title = {Applied Functional Data Analysis: Methods and Case Studies},
	Year = {2002}}

@article{Rasmussen2000,
	Adress = {Cambrige MA},
	Author = {Rasmussen, C.},
	Journal = {Advances in neuronal Information Processing Systems},
	Pages = {554-560},
	Publisher = {MIT Press},
	Title = {{T}he {I}nfinite {G}aussian {M}ixture {M}odel.},
	Volume = {10},
	Year = {2000}}

@book{RassmussenAndwillams-2006,
	Author = {C. E. Rasmussen and C. K. I. Williams},
	Publisher = {Cambridge, MA: The MIT Press},
	Title = {Gaussian Processes for Machine Learning},
	Year = {2006}}

@article{PenalizedMLEUnivNMD2000,
	Author = {A. Ridolfi and J. Idier},
	Journal = {Bayesian Inference and Maximum Entropy Methods},
	Month = {July},
	Pages = {229-237},
	Title = {Penalized maximum likelihood estimation for univariate normal mixture distribution},
	Year = {2000}}

@article{PenalizedMLEUnivNMD99,
	Author = {A. Ridolfi and J. Idier},
	Journal = {Actes 17e coll. GRETSI},
	Month = {September},
	Pages = {259-262},
	Title = {Penalized maximum likelihood estimation for univariate normal mixture distribution},
	Year = {1999}}

@book{bayesian-choice,
	Author = {Robert, Christian P.},
	Publisher = {Springer-Verlag},
	Title = {The {B}ayesian choice: a decision-theoretic motivation},
	Username = {jabreftest},
	Year = {1994}}

@book{rosenblatt-MLP-book,
	Author = {Rosenblatt, F.},
	Publisher = {Spartan},
	Title = {Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms.},
	Year = {1962}}

@article{RossiConanGuez06ProjectionNPL,
	Author = {Rossi, F. and Conan-Guez, B.},
	Journal = {Neural Processing Letters},
	Month = {February},
	Number = {1},
	Pages = {55--70},
	Title = {Theoretical Properties of Projection Based Multilayer Perceptrons with Functional Inputs},
	Volume = {23},
	Year = {2006}}

@article{RossiConanGuez05RSA,
	Author = {Rossi, F. and Conan-Guez, B.},
	Journal = {Revue de Statistique Appliqu\'ee},
	Number = {4},
	Pages = {5--30},
	Title = {Un mod{\`e}le neuronal pour la r{\'e}gression et la discrimination sur donn{\'e}es fonctionnelles},
	Volume = {LIII},
	Year = {2005}}

@article{chamroukhi-adac-2011,
	Author = {Sam{\'e}, A. and Chamroukhi, F. and Govaert, G. and Aknin, P.},
	Journal = {Advances in Data Analysis and Classification},
	Number = {4},
	Pages = {1-21},
	Publisher = {Springer Berlin / Heidelberg},
	Title = {Model-based clustering and segmentation of time series with changes in regime},
	Volume = {5},
	Year = {2011}}

@article{Gershman-Blei-tutoBNP-2012,
	Author = {Samuel, J. Gershman and Blei, David M.},
	Date-Modified = {2015-08-21 21:06:56 +0000},
	Journal = {Journal of Mathematical Psychology},
	Pages = {1-12},
	Title = {A tutorial on Bayesian non-parametric model},
	Volume = {56},
	Year = {2012}}

@incollection{scholkopf98-svm,
	Author = {Bernhard Sch\"{o}lkopf and Chris Burges and Alex Smola},
	Booktitle = {Advances in Kernel Methods -- Support Vector Learning},
	Editor = {Bernhard Sch\"{o}lkopf and Chris Burges and Alex Smola},
	Pages = {1--22},
	Publisher = {MIT Press},
	Title = {Introduction to Support Vector Learning},
	Year = {1998}}

@article{BIC,
	Author = {Schwarz, G.},
	Journal = {Annals of Statistics},
	Pages = {461-464},
	Title = {Estimating the dimension of a model},
	Volume = {6},
	Year = {1978}}

@article{ShiW08,
	Author = {Shi, J. Q. and Wang, B.},
	Date = {2008-08-08},
	Journal = {Statistics and Computing},
	Number = {3},
	Pages = {267-283},
	Title = {Curve prediction and clustering with mixtures of Gaussian process functional regression models.},
	Volume = {18},
	Year = {2008}}

@article{Bongkee-and-Kim-1995,
	Author = {Sin, B. and Kim, J. H.},
	Journal = {Signal Processing},
	Number = {1},
	Pages = {31--46},
	Publisher = {Elsevier North-Holland, Inc.},
	Title = {Nonstationary hidden Markov model},
	Volume = {46},
	Year = {1995}}

@misc{Smola99SVC-largemargin,
	Author = {Alexander J. Smola and Peter Bartlett and Bernhard Sch\"{o}lkopf and Dale Schuurmans (Eds.)},
	Title = {Advances in Large Margin Classifiers},
	Year = {1999}}

@book{wahbaSmoothingSplines,
	Address = {Philadelphia, PA},
	Author = {Wahba, G.},
	Keywords = {regression, spline regression},
	Publisher = {Society for Industrial and Applied Mathematics (SIAM)},
	Series = {CBMS-NSF Regional Conference Series in Applied Mathematics},
	Title = {Spline models for observational data},
	Volume = {59},
	Year = {1990}}

@article{wang-06-convergence-titterington,
	Abstract = {Titterington proposed a recursive parameter estimation algorithm for finite mixture models. However, due to the well known problem of singularities and multiple maximum, minimum and saddle points that are possible on the likelihood surfaces, convergence analysis has seldom been made in the past years. In this paper, under mild conditions, we show the global convergence of Titterington's recursive estimator and its MAP variant for mixture models of full regular exponential family.},
	Author = {Wang, S. and Zhao, Y.},
	Journal = {Statistics \& Probability Letters},
	Month = {December},
	Number = {18},
	Pages = {2001--2006},
	Title = {Almost sure convergence of Titterington's recursive estimator for mixture models},
	Volume = {76},
	Year = {2006}}

@article{Wood2008,
	Author = {Wood, F. and Black, M. J.},
	Journal = {Journal of Neuroscience Methods},
	Number = {1},
	Pages = {1--12},
	Title = {{A} nonparametric {B}ayesian alternative to spike sorting},
	Volume = {173},
	Year = {2008}}

@techreport{gibbs-tech-rep-Yu2009,
	Author = {Xiaodong Yu},
	Institution = {University of Maryland, College Park},
	Month = {September},
	Title = {Gibbs Sampling Methods for Dirichlet Process Mixture Model: Technical Details},
	Year = {2009}}
@book{Gilks96, 
        author = {W. R. Gilks and S. Richardson and D. J. Spiegelhalter}, 
        title = {Markov Chain Monte Carlo in Practice}, 
        publisher = {Chapman and Hall}, 
        address = {London}, 
        year = {1996},
   }
   
@article{BasuANDChib2003,
	Author = {Basu, S. and Chib, S.},
	Journal = {Journal of the American Statistical Association},
	Pages = {224-235},
	Title = {{M}arginal {L}ikelihood and {B}ayes {F}actors for {D}irichlet {P}rocess {M}ixture {M}odels},
	Volume = {98},
	Year = {2003}}

@incollection{Raftery1996HypothesisTesting,
	Address = {London, UK},
	Author = {Raftery, Adrian E.},
	Booktitle = {Markov Chain Monte Carlo in Practice},
	Chapter = 10,
	Editor = {Gilks, W. R. and Richardson, S. and Spiegelhalter, D. J.},
	Keywords = {topicinference allpurpose bayes},
	Pages = {163--187},
	Priority = {4},
	Publisher = {Chapman \& Hall},
	Title = {Hypothesis testing and model selection},
	Year = {1996}}

@article{GelfandAndSmith1990,
	Author = {Alan E. Gelfand and Adrian F. M. Smith},
	Journal = {Journal of the American Statistical Association},
	Month = {June},
	Number = {410},
	Pages = {398--409},
	Title = {{S}ampling-{B}ased {A}pproaches to {C}alculating {M}arginal {D}ensities},
	Volume = {85},
	Year = {1990}}

@article{TannerANDWong1987,
	Author = {Martin A. Tanner and Wing Hung Wong},
	Journal = {Journal of the American Statistical Association},
	Number = {398},
	Pages = {528--550},
	Title = {{The Calculation of Posterior Distributions by Data Augmentation}},
	Volume = {82},
	Year = {1987}}

@article{Escobar1994,
	Abstract = {{In this article, the Dirichlet process prior is used to provide a nonparametric Bayesian estimate of a vector of normal means. In the past there have been computational difficulties with this model. This article solves the computational difficulties by developing a "Gibbs sampler" algorithm. The estimator developed in this article is then compared to parametric empirical Bayes estimators (PEB) and nonparametric empirical Bayes estimators (NPEB) in a Monte Carlo study. The Monte Carlo study demonstrates that in some conditions the PEB is better than the NPEB and in other conditions the NPEB is better than the PEB. The Monte Carlo study also shows that the estimator developed in this article produces estimates that are about as good as the PEB when the PEB is better and produces estimates that are as good as the NPEB estimator when that method is better.}},
	Author = {Escobar, Michael D.},
	Journal = {Journal of the American Statistical Association},
	Number = {425},
	Pages = {268--277},
	Priority = {2},
	Publisher = {American Statistical Association},
	Title = {{E}stimating {N}ormal {M}eans with a {D}irichlet {P}rocess {P}rior},
	Volume = {89},
	Year = {1994}}

@article{Diabetes-dataset79,
	Author = {Reaven, G.M. and Miller, R.G.},
	Journal = {Diabetologia},
	Keywords = {Chemical diabetes; multidimensional analysis; insulin resistance; insulin secretion; glucose intolerance; diabetes mellitus},
	Number = {1},
	Pages = {17-24},
	Publisher = {Springer-Verlag},
	Title = {An attempt to define the nature of chemical diabetes using a multidimensional analysis},
	Volume = {16},
	Year = {1979}}

@article{fisher36,
	Author = {Fisher, R. A.},
	Journal = {Annals of Eugenics},
	Keywords = {classic classification linear-classification linear-discriminant-analysis},
	Number = 7,
	Pages = {179-188},
	Title = {{T}he {U}se of {M}ultiple {M}easurements in {T}axonomic {P}roblems},
	Volume = 7,
	Year = 1936}

@article{Azzalini1990,
	Author = {Azzalini, A. and Bowman, A. W.},
	Edition = 39,
	Journal = {Applied Statistics},
	Pages = {357--365},
	Title = {A look at some data on the {O}ld {F}aithful geyser},
	Year = {1990}}

@article{Campbell1974,
	Author = {Campbell, N. A. and Mahon, R. J.},
	Journal = {Australian Journal of Zoology},
	Pages = {417--425},
	Priority = {0},
	Title = {A multivariate study of variation in two species of rock crab of genus {L}eptograpsus},
	Volume = {22},
	Year = {1974}}

@article{BradleyChib1995,
	Abstract = {{Markov chain Monte Carlo (MCMC) integration methods enable the fitting of models of virtually unlimited complexity, and as such have revolutionized the practice of Bayesian data analysis. However, comparison across models may not proceed in a completely analogous fashion, owing to violations of the conditions sufficient to ensure convergence of the Markov chain. In this paper we present a framework for Bayesian model choice, along with an MCMC algorithm that does not suffer from convergence difficulties. Our algorithm applies equally well to problems where only one model is contemplated but its proper size is not known at the outset, such as problems involving integer-valued parameters, multiple changepoints or finite mixture distributions. We illustrate our approach with two published examples.}},
	Author = {Carlin, Bradley P. and Chib, Siddhartha},
	Journal = {Journal of the Royal Statistical Society. Series B},
	Number = {3},
	Pages = {473--484},
	Posted-At = {2010-12-03 15:24:51},
	Priority = {0},
	Title = {{Bayesian Model Choice via Markov Chain Monte Carlo Methods}},
	Volume = {57},
	Year = {1995}}

@inproceedings{Geyer1991,
	Author = {Geyer, C.},
	Booktitle = {Proceedings of the 23rd Symposium on the Interface},
	Pages = {156--163},
	Priority = {2},
	Title = {{M}arkov {C}hain {M}onte {C}arlo maximum likelihood},
	Year = {1991}}



 @article{Chamroukhi-FDA-2018,
 	Journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
 	Author = {Faicel Chamroukhi and Hien D. Nguyen},
 	Note = {DOI: 10.1002/widm.1298.},
 	Volume = {},
 	Title = {Model-Based Clustering and Classification of Functional Data},
 	Year = {2019},
 	Month = {to appear},
 	url =  {https://chamroukhi.com/papers/MBCC-FDA.pdf}
 	}
