for (a in 0:nl) {
for (b in (a + 1 + Lmin) : modelPWR$m)  {
print(a)
# ici et dans ce qui suit a+1 car en matlab les indices commencent de 1
yab <- modelPWR$Y[(a + 1) : b]
X_ab <- phi$XBeta[(a + 1) : b, ]
nk <- b - a
#beta = svd.inverse(crossprod(X_ab, X_ab)) %*% crossprod(X_ab, yab)
beta <- solve(crossprod(X_ab, X_ab), tol = .Machine$double.xmin) %*% t(X_ab) %*% yab
z <- yab - X_ab %*% beta
#sigma2 = crossprod(z, z) / nk
sigma2 <- t(z) %*% z / nk
C1[a + 1, b] <- nk + nk * log(sigma2 + .Machine$double.eps)      # + (z'*z)/sigma2
}
}
library(matlib)
for (a in 0:nl) {
if ((a + 1 + Lmin) <= modelPWR$m) {
############################################################################
# Condition added to handle the cases (a + 1 + Lmin) > n                   #
############################################################################
for (b in (a + 1 + Lmin) : modelPWR$m)  {
print(a)
# ici et dans ce qui suit a+1 car en matlab les indices commencent de 1
yab <- modelPWR$Y[(a + 1) : b]
X_ab <- phi$XBeta[(a + 1) : b, ]
nk <- b - a
#beta = svd.inverse(crossprod(X_ab, X_ab)) %*% crossprod(X_ab, yab)
beta <- inv(crossprod(X_ab, X_ab)) %*% t(X_ab) %*% yab
z <- yab - X_ab %*% beta
#sigma2 = crossprod(z, z) / nk
sigma2 <- t(z) %*% z / nk
C1[a + 1, b] <- nk + nk * log(sigma2 + .Machine$double.eps)      # + (z'*z)/sigma2
}
}
}
for (a in 0:nl) {
if ((a + 1 + Lmin) <= modelPWR$m) {
############################################################################
# Condition added to handle the cases (a + 1 + Lmin) > n                   #
############################################################################
for (b in (a + 1 + Lmin) : modelPWR$m)  {
print(a)
# ici et dans ce qui suit a+1 car en matlab les indices commencent de 1
yab <- modelPWR$Y[(a + 1) : b]
X_ab <- phi$XBeta[(a + 1) : b, ]
nk <- b - a
#beta = svd.inverse(crossprod(X_ab, X_ab)) %*% crossprod(X_ab, yab)
beta <- svd.inverse(crossprod(X_ab, X_ab)) %*% t(X_ab) %*% yab
z <- yab - X_ab %*% beta
#sigma2 = crossprod(z, z) / nk
sigma2 <- t(z) %*% z / nk
C1[a + 1, b] <- nk + nk * log(sigma2 + .Machine$double.eps)      # + (z'*z)/sigma2
}
}
}
library(matrixcalc)
library(R.matlab)
for (a in 0:nl) {
if ((a + 1 + Lmin) <= modelPWR$m) {
############################################################################
# Condition added to handle the cases (a + 1 + Lmin) > n                   #
############################################################################
for (b in (a + 1 + Lmin) : modelPWR$m)  {
print(a)
# ici et dans ce qui suit a+1 car en matlab les indices commencent de 1
yab <- modelPWR$Y[(a + 1) : b]
X_ab <- phi$XBeta[(a + 1) : b, ]
nk <- b - a
#beta = svd.inverse(crossprod(X_ab, X_ab)) %*% crossprod(X_ab, yab)
beta <- svd.inverse(crossprod(X_ab, X_ab)) %*% t(X_ab) %*% yab
z <- yab - X_ab %*% beta
#sigma2 = crossprod(z, z) / nk
sigma2 <- t(z) %*% z / nk
C1[a + 1, b] <- nk + nk * log(sigma2 + .Machine$double.eps)      # + (z'*z)/sigma2
}
}
}
library(matlib)
for (a in 0:nl) {
if ((a + 1 + Lmin) <= modelPWR$m) {
############################################################################
# Condition added to handle the cases (a + 1 + Lmin) > n                   #
############################################################################
for (b in (a + 1 + Lmin) : modelPWR$m)  {
print(a)
# ici et dans ce qui suit a+1 car en matlab les indices commencent de 1
yab <- modelPWR$Y[(a + 1) : b]
X_ab <- phi$XBeta[(a + 1) : b, ]
nk <- b - a
#beta = svd.inverse(crossprod(X_ab, X_ab)) %*% crossprod(X_ab, yab)
beta <- svd.inverse(crossprod(X_ab, X_ab)) %*% t(X_ab) %*% yab
z <- yab - X_ab %*% beta
#sigma2 = crossprod(z, z) / nk
sigma2 <- t(z) %*% z / nk
C1[a + 1, b] <- nk + nk * log(sigma2 + .Machine$double.eps)      # + (z'*z)/sigma2
}
}
}
for (a in 0:nl) {
if ((a + 1 + Lmin) <= modelPWR$m) {
############################################################################
# Condition added to handle the cases (a + 1 + Lmin) > n                   #
############################################################################
for (b in (a + 1 + Lmin) : modelPWR$m)  {
print(a)
# ici et dans ce qui suit a+1 car en matlab les indices commencent de 1
yab <- modelPWR$Y[(a + 1) : b]
X_ab <- phi$XBeta[(a + 1) : b, ]
nk <- b - a
#beta = svd.inverse(crossprod(X_ab, X_ab)) %*% crossprod(X_ab, yab)
beta <- inv(crossprod(X_ab, X_ab)) %*% t(X_ab) %*% yab
z <- yab - X_ab %*% beta
#sigma2 = crossprod(z, z) / nk
sigma2 <- t(z) %*% z / nk
C1[a + 1, b] <- nk + nk * log(sigma2 + .Machine$double.eps)      # + (z'*z)/sigma2
}
}
}
A=matrix( c(5,	0.0149476831091181,	6.70299690991843e-05,	3.33980912302861e-07,
0.0149476831091181,	6.70299690991842e-05,	3.33980912302861e-07,	1.76725325792545e-09,
6.70299690991843e-05,	3.33980912302861e-07,	1.76725325792545e-09,	9.70091643359720e-12,
3.33980912302861e-07,	1.76725325792545e-09,	9.70091643359720e-12,	5.45446491437166e-14), nrow = 4, ncol = 4)
A
solve(t(A) %*% A)
for (a in 0:nl) {
if ((a + 1 + Lmin) <= modelPWR$m) {
############################################################################
# Condition added to handle the cases (a + 1 + Lmin) > n                   #
############################################################################
for (b in (a + 1 + Lmin) : modelPWR$m)  {
print(a)
# ici et dans ce qui suit a+1 car en matlab les indices commencent de 1
yab <- modelPWR$Y[(a + 1) : b]
X_ab <- phi$XBeta[(a + 1) : b, ]
nk <- b - a
#beta = svd.inverse(crossprod(X_ab, X_ab)) %*% crossprod(X_ab, yab)
beta <- ginv(crossprod(X_ab, X_ab)) %*% t(X_ab) %*% yab
z <- yab - X_ab %*% beta
#sigma2 = crossprod(z, z) / nk
sigma2 <- t(z) %*% z / nk
C1[a + 1, b] <- nk + nk * log(sigma2 + .Machine$double.eps)      # + (z'*z)/sigma2
}
}
}
library(MASS)
for (a in 0:nl) {
if ((a + 1 + Lmin) <= modelPWR$m) {
############################################################################
# Condition added to handle the cases (a + 1 + Lmin) > n                   #
############################################################################
for (b in (a + 1 + Lmin) : modelPWR$m)  {
print(a)
# ici et dans ce qui suit a+1 car en matlab les indices commencent de 1
yab <- modelPWR$Y[(a + 1) : b]
X_ab <- phi$XBeta[(a + 1) : b, ]
nk <- b - a
#beta = svd.inverse(crossprod(X_ab, X_ab)) %*% crossprod(X_ab, yab)
beta <- ginv(crossprod(X_ab, X_ab)) %*% t(X_ab) %*% yab
z <- yab - X_ab %*% beta
#sigma2 = crossprod(z, z) / nk
sigma2 <- t(z) %*% z / nk
C1[a + 1, b] <- nk + nk * log(sigma2 + .Machine$double.eps)      # + (z'*z)/sigma2
}
}
}
source("R/costMatrixPPWR.R")
C1 <- costMatrixPPWR(modelPWR, phi, Lmin) #OK
### dynamic programming
solution <- dynamicProg(C1, modelPWR$K)  #oK
source("R/dynamicProg.R")
### dynamic programming
solution <- dynamicProg(C1, modelPWR$K)  #oK
Ck <- solution$J             #OK
t_est <- solution$t_est          #oK
gammak <- c(0, t_est[nrow(t_est), ])  # change points
# estimation of the corresponding regression coefficients
mean_function = matrix(0 , nrow = modelPWR$m , ncol = 1)
betak <- matrix(0 , nrow = modelPWR$p + 1 , ncol = modelPWR$K)
sigma2k <- c(rep(0, modelPWR$K))
if (p == 0){
for (k in 1 : modelPWR$K) {
i <- gammak[k] + 1
j <- gammak[k + 1]
nk <- j - i + 1
yij <- y[i:j]
X_ij <- X[i:j,]
betak[, k] <- solve(t(X_ij) %*% X_ij) %*% t(X_ij) %*% yij
z <- yij - X_ij * betak[, k]
sigma2k[k] <- t(z) * z / nk            #variances
mean_function[i:j,] <- X_ij * betak[,k]
}
}
else {
for (k in 1: modelPWR$K) {
i <- gammak[k] + 1
j <- gammak[k + 1]
nk <- j - i + 1
yij <- y[i:j]
X_ij <- X[i:j,]
betak[,k] <- solve(t(X_ij) %*% X_ij) %*% t(X_ij) %*% yij
z <- yij - X_ij %*% betak[,k]
sigma2k[k] <- t(z) %*% z / nk             #variances
mean_function[i:j,] <- X_ij %*% betak[,k]
}
}
if (p == 0){
for (k in 1 : modelPWR$K) {
i <- gammak[k] + 1
j <- gammak[k + 1]
nk <- j - i + 1
yij <- modelPWR$Y[i:j]
X_ij <- phi$XBeta[i:j,]
betak[, k] <- solve(t(X_ij) %*% X_ij) %*% t(X_ij) %*% yij
z <- yij - X_ij * betak[, k]
sigma2k[k] <- t(z) * z / nk            #variances
mean_function[i:j,] <- X_ij * betak[,k]
}
} else {
for (k in 1: modelPWR$K) {
i <- gammak[k] + 1
j <- gammak[k + 1]
nk <- j - i + 1
yij <- modelPWR$Y[i:j]
X_ij <- phi$XBeta[i:j,]
betak[,k] <- solve(t(X_ij) %*% X_ij) %*% t(X_ij) %*% yij
z <- yij - X_ij %*% betak[,k]
sigma2k[k] <- t(z) %*% z / nk             #variances
mean_function[i:j,] <- X_ij %*% betak[,k]
}
}
# classes estimees:
klas <- rep(0, modelPWR$m)
Zik <- matrix(0 , nrow = modelPWR$m , ncol = modelPWR$K)
for (k in 1:K)  {
i <- gammak[k] + 1
j <- gammak[k + 1]
klas[i:j] <- k
Zik[i:j,k] <- 1
}
PWR = list()
PWR$param = list()
PWR$stats = list()
PWR$param$betak = betak
PWR$param$sigma2k = sigma2k
PWR$param$gammak = gammak[2:(length(gammak) - 1)]    #sans le 0 et le n
PWR$param$parameter_vector = c(as.vector(PWR$param$gammak), as.vector(PWR$param$betak), as.vector(PWR$param$sigma2k))
PWR$stats$klas = klas
PWR$stats$mean_function = mean_function
PWR$stats$regressors = X %*% PWR$param$betak
PWR$stats$Zik = Zik
PWR$stats$objective = Ck[length(Ck)]
PWR$stats$regressors = phi$XBeta %*% PWR$param$betak
PWR$stats$Zik = Zik
PWR$stats$objective = Ck[length(Ck)]
PWR$stats$cputime = Sys.time() - start_time
source('~/Documents/git/PWR_r/PWR/R/main_PWR_ficher.R')
rm(list = ls())
source("R/FData.R")
source("R/ModelPWR.R")
source("R/ModelLearner.R")
# Building matrices for regression
load("data/simulatedTimeSeries.RData")
fData <- FData$new()
fData$setData(X, Y)
# model specification
K <- 5 # number of segments
p <- 3 # polynomial degree
modelPWR <- ModelPWR(fData, K, p)
solution <- fitPWRFisher(modelPWR)
source('~/Documents/git/PWR_r/PWR/R/main_PWR_ficher.R')
modelPWR$K
c(rep(0, modelPWR$K))
beta <- matrix(NA, modelPWR$p + 1, modelPWR$K)
ncol(beta)
objective <- NA
objective
a=c(1,2,3,4)
a[-1]
a[,]
a[1]
rm(list = ls())
source("R/FData.R")
source("R/ModelPWR.R")
source("R/ModelLearner.R")
# Building matrices for regression
load("data/simulatedTimeSeries.RData")
fData <- FData$new()
rm(list = ls())
source("R/FData.R")
source("R/ModelPWR.R")
source("R/ModelLearner.R")
source("R/ModelLearner.R")
rm(list = ls())
source("R/FData.R")
source("R/ModelPWR.R")
source("R/ModelLearner.R")
rm(list = ls())
source("R/FData.R")
source("R/ModelPWR.R")
source("R/ModelLearner.R")
# Building matrices for regression
load("data/simulatedTimeSeries.RData")
fData <- FData$new()
fData$setData(X, Y)
# model specification
K <- 5 # number of segments
p <- 3 # polynomial degree
modelPWR <- ModelPWR(fData, K, p)
solution <- fitPWRFisher(modelPWR)
phi <- designmatrix(modelPWR$X, modelPWR$p)
Lmin <- modelPWR$p + 1
C1 <- costMatrixPPWR(modelPWR, phi, Lmin)
paramPWR <- ParamPWR(modelPWR)
source("R/ParamPWR.R")
paramPWR <- ParamPWR(modelPWR)
paramPWR <- ParamPWR(modelPWR)
source("R/costMatrixPPWR.R")
source("R/ParamPWR.R")
source("R/StatPWR.R")
phi <- designmatrix(modelPWR$X, modelPWR$p)
Lmin <- modelPWR$p + 1
C1 <- costMatrixPPWR(modelPWR, phi, Lmin)
paramPWR <- ParamPWR(modelPWR)
source("R/ParamPWR.R")
paramPWR <- ParamPWR(modelPWR)
Ck <- paramPWR$computeDynamicProgram()
paramPWR <- ParamPWR(modelPWR)
Ck <- paramPWR$computeDynamicProgram()
source("R/ParamPWR.R")
paramPWR <- ParamPWR(modelPWR)
Ck <- paramPWR$computeDynamicProgram()
source("R/ParamPWR.R")
paramPWR <- ParamPWR(modelPWR)
Ck <- paramPWR$computeDynamicProgram()
paramPWR$computeParam()
paramPWR$computeParam(modelPWR, phi)
source("R/ParamPWR.R")
paramPWR <- ParamPWR(modelPWR)
Ck <- paramPWR$computeDynamicProgram()
paramPWR$computeParam(modelPWR, phi)
statPWR <- ParamPWR(modelPWR)
# estimation of the corresponding regression coefficients
statPWR$computeMeanFunction()
Ck
statPWR <- ParamPWR(modelPWR)
# estimation of the corresponding regression coefficients
statPWR$computeMeanFunction()
statPWR
# estimation of the corresponding regression coefficients
statPWR$computeMeanFunction(paramPWR, phi)
statPWR <- StatPWR(modelPWR)
source("R/StatPWR.R")
statPWR <- StatPWR(modelPWR)
# estimation of the corresponding regression coefficients
statPWR$computeMeanFunction(paramPWR, phi)
# classes estimees:
statPWR$klasEstimate()
# classes estimees:
statPWR$klasEstimate(paramPWR)
statPWR$computeRegressors()
statPWR$computeRegressors(phi, paramPWR)
statPWR$objective = Ck[length(Ck)]
statPWR$cputime = Sys.time() - start_time
source('~/Documents/git/PWR_r/PWR/R/main_PWR_ficher.R')
source('~/Documents/git/PWR_r/PWR/R/main_PWR_ficher.R')
source('~/Documents/git/PWR_r/PWR/R/main_PWR_ficher.R')
source('~/Documents/git/PWR_r/PWR/R/main_PWR_ficher.R')
source('~/Documents/git/PWR_r/PWR/R/main_PWR_ficher.R')
solution$statPWR$cpu_time
solution$statPWR$klas
solution$statPWR$klas
solution$statPWR$mean_function
solution$statPWR$z_ik
source('~/Documents/git/PWR_r/PWR/R/main_PWR_ficher.R')
source('~/Documents/git/PWR_r/PWR/R/main_PWR_ficher.R')
solution$statPWR$cpu_time
source('~/Documents/git/PWR_r/PWR/R/main_PWR_ficher.R')
source('~/Documents/git/PWR_r/PWR/R/main_PWR_ficher.R')
source('~/Documents/git/PWR_r/PWR/R/main_PWR_ficher.R')
solution$plot()
source('~/Documents/git/PWR_r/PWR/R/main_PWR.R')
install.packages("RcppArmadillo")
install.packages("RcppArmadillo")
##################################################################
# A polynomial piecewise regression model for the optimal segmentation of a
# time series with regime changes. It uses dynamic programming for the segmentation
# and the LSE for the estimation of the regression parameters.
#
# by Faicel Chamroukhi Decembre 2008.
#
## Please cite the following papers for this code:
#
# @article{chamroukhi_et_al_NN2009,
# 	Address = {Oxford, UK, UK},
# 	Author = {Chamroukhi, F. and Sam\'{e}, A. and Govaert, G. and Aknin, P.},
# 	Date-Added = {2014-10-22 20:08:41 +0000},
# 	Date-Modified = {2014-10-22 20:08:41 +0000},
# 	Journal = {Neural Networks},
# 	Number = {5-6},
# 	Pages = {593--602},
# 	Publisher = {Elsevier Science Ltd.},
# 	Title = {Time series modeling by a regression approach based on a latent process},
# 	Volume = {22},
# 	Year = {2009},
# 	url  = {https://chamroukhi.users.lmno.cnrs.fr/papers/Chamroukhi_Neural_Networks_2009.pdf}
# 	}
#
# @INPROCEEDINGS{Chamroukhi-IJCNN-2009,
#   AUTHOR =       {Chamroukhi, F. and Sam\'e,  A. and Govaert, G. and Aknin, P.},
#   TITLE =        {A regression model with a hidden logistic process for feature extraction from time series},
#   BOOKTITLE =    {International Joint Conference on Neural Networks (IJCNN)},
#   YEAR =         {2009},
#   month = {June},
#   pages = {489--496},
#   Address = {Atlanta, GA},
#  url = {https://chamroukhi.users.lmno.cnrs.fr/papers/chamroukhi_ijcnn2009.pdf}
# }
#
#@article{Chamroukhi-FDA-2018,
#	Journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
#	Author = {Faicel Chamroukhi and Hien D. Nguyen},
#	Note = {DOI: 10.1002/widm.1298.},
#	Volume = {},
#	Title = {Model-Based Clustering and Classification of Functional Data},
#	Year = {2018},
#	Month = {Dec},
#	url =  {https://chamroukhi.com/papers/MBCC-FDA.pdf}
#	}
#
#
##################################################################
rm(list = ls())
source("R/FData.R")
source("R/ModelPWR.R")
source("R/ModelLearner.R")
# Building matrices for regression
load("data/simulatedTimeSeries.RData")
fData <- FData$new()
fData$setData(X, Y)
# model specification
K <- 5 # number of segments
p <- 3 # polynomial degree
modelPWR <- ModelPWR(fData, K, p)
solution <- fitPWRFisher(modelPWR)
solution$plot()
##################################################################
# A polynomial piecewise regression model for the optimal segmentation of a
# time series with regime changes. It uses dynamic programming for the segmentation
# and the LSE for the estimation of the regression parameters.
#
# by Faicel Chamroukhi Decembre 2008.
#
## Please cite the following papers for this code:
#
# @article{chamroukhi_et_al_NN2009,
# 	Address = {Oxford, UK, UK},
# 	Author = {Chamroukhi, F. and Sam\'{e}, A. and Govaert, G. and Aknin, P.},
# 	Date-Added = {2014-10-22 20:08:41 +0000},
# 	Date-Modified = {2014-10-22 20:08:41 +0000},
# 	Journal = {Neural Networks},
# 	Number = {5-6},
# 	Pages = {593--602},
# 	Publisher = {Elsevier Science Ltd.},
# 	Title = {Time series modeling by a regression approach based on a latent process},
# 	Volume = {22},
# 	Year = {2009},
# 	url  = {https://chamroukhi.users.lmno.cnrs.fr/papers/Chamroukhi_Neural_Networks_2009.pdf}
# 	}
#
# @INPROCEEDINGS{Chamroukhi-IJCNN-2009,
#   AUTHOR =       {Chamroukhi, F. and Sam\'e,  A. and Govaert, G. and Aknin, P.},
#   TITLE =        {A regression model with a hidden logistic process for feature extraction from time series},
#   BOOKTITLE =    {International Joint Conference on Neural Networks (IJCNN)},
#   YEAR =         {2009},
#   month = {June},
#   pages = {489--496},
#   Address = {Atlanta, GA},
#  url = {https://chamroukhi.users.lmno.cnrs.fr/papers/chamroukhi_ijcnn2009.pdf}
# }
#
#@article{Chamroukhi-FDA-2018,
#	Journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
#	Author = {Faicel Chamroukhi and Hien D. Nguyen},
#	Note = {DOI: 10.1002/widm.1298.},
#	Volume = {},
#	Title = {Model-Based Clustering and Classification of Functional Data},
#	Year = {2018},
#	Month = {Dec},
#	url =  {https://chamroukhi.com/papers/MBCC-FDA.pdf}
#	}
#
#
##################################################################
rm(list = ls())
source("R/FData.R")
source("R/ModelPWR.R")
source("R/ModelLearner.R")
# Building matrices for regression
load("data/simulatedTimeSeries.RData")
fData <- FData$new()
fData$setData(X, Y)
# model specification
K <- 5 # number of segments
p <- 3 # polynomial degree
modelPWR <- ModelPWR(fData, K, p)
solution <- fitPWRFisher(modelPWR)
solution$plot()
source('~/Documents/git/PWR_r/PWR/R/main_PWR.R')
source('~/Documents/git/PWR_r/PWR/R/main_PWR.R')
source('~/Documents/git/PWR_r/PWR/R/main_PWR.R')
source('~/Documents/git/PWR_r/PWR/R/main_PWR.R')
